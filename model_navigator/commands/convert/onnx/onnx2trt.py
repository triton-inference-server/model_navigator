# Copyright (c) 2021-2023, NVIDIA CORPORATION. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""ConvertONNX2TRT command."""

import pathlib
from typing import Any, Dict, List, Optional

from model_navigator.api.config import (
    TensorRTCompatibilityLevel,
    TensorRTPrecision,
    TensorRTPrecisionMode,
    TensorRTProfile,
)
from model_navigator.commands.base import CommandOutput, CommandStatus
from model_navigator.commands.convert.base import Convert2TensorRTWithMaxBatchSizeSearch
from model_navigator.commands.execution_context import ExecutionContext
from model_navigator.core.logger import LOGGER
from model_navigator.core.workspace import Workspace
from model_navigator.frameworks.onnx.utils import get_onnx_io_names
from model_navigator.frameworks.tensorrt import utils as tensorrt_utils
from model_navigator.utils import devices
from model_navigator.utils.common import parse_kwargs_to_cmd


class ConvertONNX2TRT(Convert2TensorRTWithMaxBatchSizeSearch):
    """Command that converts ONNX checkpoint to TensorRT model plan."""

    def _run(
        self,
        workspace: Workspace,
        path: pathlib.Path,
        parent_path: pathlib.Path,
        precision: TensorRTPrecision,
        precision_mode: TensorRTPrecisionMode,
        dataloader_trt_profile: TensorRTProfile,
        custom_args: Dict[str, Any],
        max_workspace_size: Optional[int] = None,
        batch_dim: Optional[int] = None,
        dataloader_max_batch_size: Optional[int] = None,
        device_max_batch_size: Optional[int] = None,
        optimization_level: Optional[int] = None,
        compatibility_level: Optional[TensorRTCompatibilityLevel] = None,
        optimized_trt_profiles: Optional[List[TensorRTProfile]] = None,
        onnx_parser_flags: Optional[List[int]] = None,
        verbose: bool = False,
    ) -> CommandOutput:
        """Run the ConvertONNX2TRT Command.

        Used for initial conversion (with max batch size search) of ONNX model to TensorRT plan and for optimized conversion with generated TensortRT profiles.
        TensorRT profiles are generated by TensorRTProfileBuilder command and saved in status.yaml file.

        Args:
            workspace: Model Navigator working directory.
            path: ONNX checkpoint path, relative to workspace.
            parent_path: Path of ONNX parent model, relative to workspace.
            input_metadata: Model input metadata.
            output_metadata: Model output metadata.
            precision: TensorRT precision.
            precision_mode: TensorRT precision mode.
            dataloader_trt_profile: Dataloader TensorRT profile.
            max_workspace_size: Maximum TensorRT workspace size, in bytes. Defaults to None.
            batch_dim: Dimension of the batching, None if model does not support batching.
                Defaults to None.
            dataloader_max_batch_size (Optional[int], optional): Maximum batch size in the dataloader. Defaults to None.
            device_max_batch_size: Maximum batch size that fits on the device.
                Defaults to None.
            optimization_level: Optimization level for TensorRT engine
            compatibility_level: Hardware compatibility level for generated engine
            optimized_trt_profiles: List of TensorRT profiles that will be used by Model Navigator for conversion, user provided or optimized by TensorRTProfileBuilder command.
            onnx_parser_flags (Optional[List[trt.OnnxParserFlag]], optional): List of flags to set ONNX parser behavior.
            verbose: enable verbose logging for command
            custom_args (Optional[Dict[str, Any]], optional): Passthrough parameters for Polygraphy convert command
                For available arguments check Polygraphy documentation: https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#polygraphy
                or Polygraphy repository: https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy

        Returns:
            CommandOutput: Status and results of the command.
        """
        LOGGER.info("ONNX to TRT conversion started")

        if not devices.get_available_gpus():
            raise RuntimeError("No GPUs available.")

        input_model_path = workspace.path / parent_path
        converted_model_path = workspace.path / path

        if not input_model_path.exists():
            LOGGER.warning(f"Exported ONNX model not found at {input_model_path}. Skipping conversion.")
            return CommandOutput(status=CommandStatus.SKIPPED)
        converted_model_path.parent.mkdir(parents=True, exist_ok=True)

        onnx_input_names, _ = get_onnx_io_names(onnx_path=input_model_path)

        def get_args(max_batch_size=None):
            if optimized_trt_profiles:
                profiles = optimized_trt_profiles
            else:
                profiles = [
                    self._get_conversion_profiles(
                        trt_profile=dataloader_trt_profile,
                        batch_dim=batch_dim,
                        max_batch_size=max_batch_size,
                    )
                ]

            profiles_dicts = []
            for profile in profiles:
                profile_dict = {name: shapes for name, shapes in profile.to_dict().items() if name in onnx_input_names}
                profiles_dicts.append(profile_dict)

            kwargs = {
                "exported_model_path": input_model_path.relative_to(workspace.path).as_posix(),
                "converted_model_path": converted_model_path.relative_to(workspace.path).as_posix(),
                "profiles": profiles_dicts,
                "max_workspace_size": max_workspace_size,
                "precision": precision.value,
                "precision_mode": precision_mode.value,
                "navigator_workspace": workspace.path.as_posix(),
                "custom_args": custom_args,
            }
            if optimization_level is not None:
                kwargs["optimization_level"] = optimization_level
            if compatibility_level is not None:
                kwargs["compatibility_level"] = compatibility_level.value
            if onnx_parser_flags:
                kwargs["onnx_parser_flags"] = onnx_parser_flags
            args = parse_kwargs_to_cmd(kwargs)
            return args

        with ExecutionContext(
            workspace=workspace,
            script_path=converted_model_path.parent / "reproduce_conversion.py",
            cmd_path=converted_model_path.parent / "reproduce_conversion.sh",
            verbose=verbose,
        ) as context:
            from model_navigator.commands.convert.converters import onnx2trt

            conversion_max_batch_size = self._execute_conversion(
                convert_func=lambda args: context.execute_external_runtime_script(onnx2trt.__file__, args),
                get_args=get_args,
                batch_dim=batch_dim,
                device_max_batch_size=device_max_batch_size,
                dataloader_max_batch_size=dataloader_max_batch_size,
                custom_trt_profile_available=bool(optimized_trt_profiles),
            )

        conversion_profiles = self._get_conversion_profiles(
            trt_profile=optimized_trt_profiles[0] if optimized_trt_profiles else dataloader_trt_profile,
            batch_dim=batch_dim,
            max_batch_size=conversion_max_batch_size,
        )

        LOGGER.info("Converted ONNX to TensorRT.")
        return CommandOutput(
            status=CommandStatus.OK,
            output={"conversion_max_batch_size": conversion_max_batch_size, "conversion_profiles": conversion_profiles},
        )

    @staticmethod
    def _get_conversion_profiles(
        trt_profile: TensorRTProfile,
        batch_dim: Optional[int] = None,
        max_batch_size: Optional[int] = None,
    ):
        if batch_dim is not None and max_batch_size is not None and max_batch_size > 0:
            trt_profile = tensorrt_utils.get_trt_profile_with_new_max_batch_size(
                trt_profile=trt_profile,
                max_batch_size=max_batch_size,
                batch_dim=batch_dim,
            )

        return trt_profile
