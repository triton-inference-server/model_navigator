# Copyright (c) 2021-2025, NVIDIA CORPORATION. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""ConvertONNX2TRT command."""

import pathlib
from typing import Any, Dict, List, Optional

import model_navigator.core.context as ctx
from model_navigator.commands.base import CommandOutput, CommandStatus
from model_navigator.commands.convert.base import Convert2TensorRTWithMaxBatchSizeSearch
from model_navigator.commands.execution_context import ExecutionContext
from model_navigator.configuration import (
    PrecisionType,
    TensorRTCompatibilityLevel,
    TensorRTPrecision,
    TensorRTPrecisionMode,
    TensorRTProfile,
)
from model_navigator.core.logger import LOGGER
from model_navigator.core.workspace import Workspace
from model_navigator.frameworks.onnx.utils import get_onnx_io_names
from model_navigator.frameworks.tensorrt import utils as tensorrt_utils
from model_navigator.frameworks.tensorrt.timing_tactics import trt_cache_inplace_cache_dir
from model_navigator.utils import devices
from model_navigator.utils.common import parse_kwargs_to_cmd


class ConvertONNX2TRT(Convert2TensorRTWithMaxBatchSizeSearch):
    """Command that converts ONNX checkpoint to TensorRT model plan."""

    def _setup_onnx_path(
        self,
        precision: TensorRTPrecision,
        converted_model_path: pathlib.Path,
        workspace: Workspace,
    ) -> Optional[str]:
        """Set up the ONNX path for TensorRT conversion.

        This method only determines the path where a quantized ONNX model should be saved.
        It does not perform the actual quantization.

        Args:
            precision: TensorRT precision
            converted_model_path: Path where the converted model will be saved
            workspace: Model Navigator workspace
        Returns:
            Optional[str]: Path to the quantized ONNX model or None if quantization is not needed.
        """
        # For quantization precisions, define the path where quantized model should be saved
        # All quantization precisions use the same path convention
        if precision in (TensorRTPrecision.FP8, TensorRTPrecision.INT8, TensorRTPrecision.NVFP4):
            relative_path = converted_model_path.relative_to(workspace.path)
            quantized_path = relative_path.parent / "quantized_model.onnx"
            return quantized_path.as_posix()
        else:
            return None

    def _run(
        self,
        workspace: Workspace,
        path: pathlib.Path,
        parent_path: pathlib.Path,
        precision: TensorRTPrecision,
        precision_mode: TensorRTPrecisionMode,
        dataloader_trt_profile: TensorRTProfile,
        custom_args: Dict[str, Any],
        max_workspace_size: Optional[int] = None,
        batch_dim: Optional[int] = None,
        conversion_fallback: bool = False,
        dataloader_max_batch_size: Optional[int] = None,
        device_max_batch_size: Optional[int] = None,
        optimization_level: Optional[int] = None,
        compatibility_level: Optional[TensorRTCompatibilityLevel] = None,
        trt_profiles: Optional[List[TensorRTProfile]] = None,
        onnx_parser_flags: Optional[List[int]] = None,
        timing_cache_dir: Optional[str] = None,
        verbose: bool = False,
        model_precision: Optional[PrecisionType] = None,
    ) -> CommandOutput:
        """Run the ConvertONNX2TRT Command.

        Used for initial conversion (with max batch size search) of ONNX model to TensorRT plan and for optimized conversion with generated TensortRT profiles.
        TensorRT profiles are generated by TensorRTProfileBuilder command and saved in status.yaml file.

        Args:
            workspace: Model Navigator working directory.
            path: ONNX checkpoint path, relative to workspace.
            parent_path: Path of ONNX parent model, relative to workspace.
            precision: TensorRT precision.
            precision_mode: TensorRT precision mode.
            dataloader_trt_profile: Dataloader TensorRT profile.
            max_workspace_size: Maximum TensorRT workspace size, in bytes. Defaults to None.
            batch_dim: Dimension of the batching, None if model does not support batching.
                Defaults to None.
            conversion_fallback: Enable fallback for conversion to try conversion with smaller batch size
            dataloader_max_batch_size (Optional[int], optional): Maximum batch size in the dataloader. Defaults to None.
            device_max_batch_size: Maximum batch size that fits on the device.
                Defaults to None.
            optimization_level: Optimization level for TensorRT engine
            compatibility_level: Hardware compatibility level for generated engine
            trt_profiles: List of TensorRT profiles that will be used by Model Navigator for conversion, user provided.
            onnx_parser_flags (Optional[List[trt.OnnxParserFlag]], optional): List of flags to set ONNX parser behavior.
            timing_cache_dir: (Optional[str]): Directory to save timing cache. Defaults to None which means it will be saved in workspace root.
            verbose: enable verbose logging for command
            custom_args (Optional[Dict[str, Any]], optional): Passthrough parameters for Polygraphy convert command
                For available arguments check Polygraphy documentation: https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#polygraphy
                or Polygraphy repository: https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy
            model_precision: Source model precision.

        Returns:
            CommandOutput: Status and results of the command.
        """
        LOGGER.info("ONNX to TRT conversion started")

        if not devices.get_available_gpus():
            raise RuntimeError("No GPUs available.")

        input_model_path = workspace.path / parent_path
        converted_model_path = workspace.path / path

        if not input_model_path.exists():
            LOGGER.warning(f"Exported ONNX model not found at {input_model_path}. Skipping conversion.")
            return CommandOutput(status=CommandStatus.SKIPPED)
        converted_model_path.parent.mkdir(parents=True, exist_ok=True)

        onnx_input_names, _ = get_onnx_io_names(onnx_path=input_model_path)

        def get_args(max_batch_size=None):
            profiles = [
                self._get_conversion_profiles(
                    trt_profile=dataloader_trt_profile,
                    batch_dim=batch_dim,
                    max_batch_size=max_batch_size,
                )
            ]
            if trt_profiles:
                profiles.extend(trt_profiles)

            profiles_dicts = []
            for profile in profiles:
                profile_dict = {name: shapes for name, shapes in profile.to_dict().items() if name in onnx_input_names}
                profiles_dicts.append(profile_dict)

            module_name = ctx.global_context.get(ctx.INPLACE_OPTIMIZE_MODULE_NAME_CONTEXT_KEY) or workspace.path.stem

            # Get the path where a quantized ONNX model should be saved
            onnx_quant_path = self._setup_onnx_path(
                precision=precision,
                converted_model_path=converted_model_path,
                workspace=workspace,
            )

            # We still pass the original input model path to the converter
            onnx_input_path = input_model_path.relative_to(workspace.path).as_posix()

            kwargs = {
                "exported_model_path": onnx_input_path,
                "converted_model_path": converted_model_path.relative_to(workspace.path).as_posix(),
                "profiles": profiles_dicts,
                "max_workspace_size": max_workspace_size,
                "precision": precision.value,
                "precision_mode": precision_mode.value,
                "navigator_workspace": workspace.path.as_posix(),
                "model_name": module_name,
                "timing_cache_dir": trt_cache_inplace_cache_dir(),
                "custom_args": custom_args,
                "batch_dim": batch_dim,
                "model_precision": model_precision,
                "quantized_onnx_path": onnx_quant_path,
            }
            if optimization_level is not None:
                kwargs["optimization_level"] = optimization_level
            if compatibility_level is not None:
                kwargs["compatibility_level"] = compatibility_level.value
            if onnx_parser_flags:
                kwargs["onnx_parser_flags"] = onnx_parser_flags
            if timing_cache_dir is not None:
                kwargs["timing_cache_dir"] = str(timing_cache_dir)
            args = parse_kwargs_to_cmd(kwargs)
            return args

        with ExecutionContext(
            workspace=workspace,
            script_path=converted_model_path.parent / "reproduce_conversion.py",
            cmd_path=converted_model_path.parent / "reproduce_conversion.sh",
            verbose=verbose,
        ) as context:
            from model_navigator.commands.convert.converters import onnx2trt

            conversion_max_batch_size = self._execute_conversion(
                convert_func=lambda args: context.execute_python_script(
                    onnx2trt.__file__,
                    onnx2trt.convert,
                    args,
                    run_in_isolation=True,
                ),
                get_args=get_args,
                batch_dim=batch_dim,
                device_max_batch_size=device_max_batch_size,
                dataloader_max_batch_size=dataloader_max_batch_size,
                custom_trt_profile_available=bool(trt_profiles),
                conversion_fallback=conversion_fallback,
            )

        profiles = [
            self._get_conversion_profiles(
                trt_profile=dataloader_trt_profile,
                batch_dim=batch_dim,
                max_batch_size=conversion_max_batch_size,
            )
        ]

        LOGGER.info("Converted ONNX to TensorRT.")
        return CommandOutput(
            status=CommandStatus.OK,
            output={"conversion_max_batch_size": conversion_max_batch_size, "conversion_profiles": profiles},
        )

    @staticmethod
    def _get_conversion_profiles(
        trt_profile: TensorRTProfile,
        batch_dim: Optional[int] = None,
        max_batch_size: Optional[int] = None,
    ):
        if batch_dim is not None and max_batch_size is not None and max_batch_size > 0:
            trt_profile = tensorrt_utils.get_trt_profile_with_new_max_batch_size(
                trt_profile=trt_profile,
                max_batch_size=max_batch_size,
                batch_dim=batch_dim,
            )

        return trt_profile
