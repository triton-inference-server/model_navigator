{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#triton-model-navigator","title":"Triton Model Navigator","text":""},{"location":"#overview","title":"Overview","text":"<p>Welcome to the Triton Model Navigator, an inference toolkit designed for optimizing and deploying Deep Learning models with a focus on NVIDIA GPUs. The Triton Model Navigator streamlines the process of moving models and pipelines implemented in PyTorch, TensorFlow, and/or ONNX to TensorRT.</p> <p>The Triton Model Navigator automates several critical steps, including model export, conversion, correctness testing, and profiling. By providing a single entry point for various supported frameworks, users can efficiently search for the best deployment option using the per-framework optimize function. The resulting optimized models are ready for deployment on either PyTriton or Triton Inference Server.</p>"},{"location":"#features-at-glance","title":"Features at Glance","text":"<p>The distinct capabilities of the Triton Model Navigator are summarized in the feature matrix:</p> Feature Description Ease-of-use A single line of code to run all possible optimization paths directly from your source code Wide Framework Support Compatible with various machine learning frameworks including PyTorch, TensorFlow, and ONNX Models Optimization Enhance the performance of models such as ResNET and BERT for efficient inference deployment Pipelines Optimization Streamline Python code pipelines for models such as Stable Diffusion and Whisper using Inplace Optimization, exclusive to PyTorch Model Export and Conversion Automate the process of exporting and converting models between various formats with focus on TensorRT and Torch-TensorRT Correctness Testing Ensures the converted model produces correct outputs validating against the original model Performance Profiling Profiles models to select the optimal format based on performance metrics such as latency and throughput to optimize target hardware utilization Models Deployment Automates models and pipelines deployment on PyTriton and the Triton Inference Server through a dedicated API"},{"location":"#support-matrix-for-frameworks","title":"Support Matrix for Frameworks","text":"<p>The Triton Model Navigator efficiently produces various optimized models ready for deployment. The accompanying table showcases the diverse model formats achievable through the Triton Model Navigator across different frameworks, highlighting its versatility.</p> <p>Table: Supported conversion target formats per supported Python framework or file.</p> PyTorch TensorFlow 2 JAX ONNX Torch Compile SavedModel SavedModel TensorRT TorchScript Trace TensorRT in TensorFlow TensorRT in TensorFlow TorchScript Script ONNX ONNX Torch-TensorRT TensorRT TensorRT ONNX TensorRT <p>Note: The Triton Model Navigator has the capability to support any Python function as input. However, in this particular case, its role is limited to profiling the function without generating any serialized models.</p> <p>The Inplace Optimize feature is dedicated for PyTorch to optimize pipelines patching <code>nn.Modules</code> and optimize them to TensorRT. The table below highlights the possible optimization paths for Inplace Optimize:</p> <p>Table: Supported conversion target formats for Inplace Optimize.</p> PyTorch Torch Compile TorchScript Trace TorchScript Script Torch-TensorRT ONNX TensorRT"},{"location":"#what-next","title":"What next?","text":"<p>Learn more about using the Triton Model Navigator in Quick Start, where you will find more information about optimizing models and serving inference.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":""},{"location":"CHANGELOG/#0140","title":"0.14.0","text":"<ul> <li>new: TensorRT INT8 and FP8 quantization through ModelOpt (ONNX path)</li> <li>new: TensorRT NVFP4 quantization through ModelOpt (Torch path)</li> <li>new: Improved TorchCompile performance for repeated compilations using TORCHINDUCTOR_CACHE_DIR environment variable</li> <li>new: Global context with scoped variables - temporary context variables</li> <li>new: Added new context variables <code>INPLACE_OPTIMIZE_WORKSPACE_CONTEXT_KEY</code> and <code>INPLACE_OPTIMIZE_MODULE_GRAPH_ID_CONTEXT_KEY</code></li> <li>new: nav.bundle.save now has include and exclude patterns for fine grained files selection</li> <li>new: GPU and Host memory usage logging</li> <li>change: Install the TensorRT package for architectures other than x86_64</li> <li>change: Disable conversion fallback for TensorRT paths and expose control option in custom config</li> <li>change: Use torch.export.save for Torch-TRT model serialization</li> <li>change: Added export_engine to OnnxConfig for improved export control</li> <li>fix: Correctness command relative tolerance formula</li> <li> <p>fix: Memory management during export and conversion process for Torch</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.7.0a0+7c8ec84dab</li> <li>TensorFlow 2.17.0</li> <li>TensorRT 10.9.0.34</li> <li>TensorRT ModelOptimizer 0.27.0</li> <li>Torch-TensorRT 2.7.0a0</li> <li>ONNX Runtime 1.20.2</li> <li>Polygraphy 0.49.20</li> <li>GraphSurgeon 0.5.8</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#0131","title":"0.13.1","text":"<ul> <li> <p>fix: Add AutocastType to public API</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.6.0a0+df5bbc0</li> <li>TensorFlow 2.16.1</li> <li>TensorRT 10.6.0.26</li> <li>Torch-TensorRT 2.6.0a0</li> <li>ONNX Runtime 1.19.2</li> <li>Polygraphy: 0.49.13</li> <li>GraphSurgeon: 0.5.2</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#0130","title":"0.13.0","text":"<ul> <li>new: Introducing custom_args in TensorConfig for custom runners to use which   allows dynamic shapes setup for TorchTensorRT compilation</li> <li>new: autocast_dtype added Torch runner configuration to set the dtype for autocast</li> <li>new: New version of Onnx Runtime 1.20 for python version &gt;= 3.10</li> <li>new: Use <code>torch.compile</code> path in heuristic search for max batch size</li> <li>change: Removed TensorFlow dependencies for <code>nav.jax.optimize</code></li> <li>change: Removed PyTorch dependencies from <code>nav.profile</code></li> <li>change: Collect all Python packages in status instead of filtered list</li> <li>change: Use default throughput cutoff threshold for max batch size heuristic when <code>None</code> provided in configuration</li> <li>change: Updated default ONNX opset to 20 for Torch &gt;= 2.5</li> <li>fix: Exception is raised with Python &gt;=3.11 due to wrong dataclass initialization</li> <li>fix: Removed option from ExportOption removed from Torch 2.5</li> <li>fix: Improved preprocessing stage in Torch based runners</li> <li>fix: Warn when using autocast with bfloat16 in Torch</li> <li> <p>fix: Pass runner configuration to runners in nav.profile</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.6.0a0+df5bbc0</li> <li>TensorFlow 2.16.1</li> <li>TensorRT 10.6.0.26</li> <li>Torch-TensorRT 2.6.0a0</li> <li>ONNX Runtime 1.19.2</li> <li>Polygraphy: 0.49.13</li> <li>GraphSurgeon: 0.5.2</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#0120","title":"0.12.0","text":"<ul> <li>new: simple and detailed reporting of the optimization process</li> <li>new: adjusted exporting TensorFlow SavedModel for Keras 3.x</li> <li>new: inform user when wrapped a module which is not called during optimize</li> <li>new: inform user when module uses a custom forward function</li> <li>new: support for dynamic shapes in Torch ExportedProgram</li> <li>new: use ExportedProgram for Torch-TensorRT conversion</li> <li>new: support back-off policy during profiling to avoid reporting local minimum</li> <li>new: automatically scale conversion batch size when modules have different batch sizes in scope of a single pipeline</li> <li>change: TensorRT conversion max batch size search rely on saturating throughput for base formats</li> <li>change: adjusted profiling configuration for throughput cutoff search</li> <li>change: include optimized pipeline to list of examined variants during <code>nav.profile</code></li> <li>change: performance is not executed when correctness failed for format and runtime</li> <li>change: verify command is not executed when verify function is not provided</li> <li>change: do not create a model copy before executing <code>torch.compile</code></li> <li>fix: pipelines sometimes obtain model and tensors on different devices during <code>nav.profile</code></li> <li>fix: extract graph from ExportedProgram for running inference</li> <li> <p>fix: runner configuration not propagated to pre-processing steps</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.4.0a0+3bcc3cddb5</li> <li>TensorFlow 2.16.1</li> <li>TensorRT 10.3.0.26</li> <li>Torch-TensorRT 2.4.0.a0</li> <li>ONNX Runtime 1.18.1</li> <li>Polygraphy: 0.49.12</li> <li>GraphSurgeon: 0.5.2</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#0110","title":"0.11.0","text":"<ul> <li>new: Python 3.12 support</li> <li>new: Improved logging</li> <li>new: optimized in-place module can be stored to Triton model repository</li> <li>new: multi-profile support for TensorRT model build and runtime</li> <li>new: measure duration of each command executed in optimization pipeline</li> <li>new: TensorRT-LLM model store generation for deployment on Triton Inference Server</li> <li>change: filter unsupported runners instead of raising an error when running optimize</li> <li>change: moved JAX to support to experimental module and limited support</li> <li>change: use autocast=True for Torch based runners</li> <li>change: use torch.inference_mode or torch.no_grad context in <code>nav.profile</code> measurements</li> <li>change: use multiple strategies to select optimized runtime, defaults   to [<code>MaxThroughputAndMinLatencyStrategy</code>, <code>MinLatencyStrategy</code>]</li> <li>change: <code>trt_profiles</code> are not set automatically for module when using <code>nav.optimize</code></li> <li> <p>fix: properly revert log level after torch onnx dynamo export</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.4.0a0+07cecf4</li> <li>TensorFlow 2.15.0</li> <li>TensorRT 10.0.1.6</li> <li>Torch-TensorRT 2.4.0.a0</li> <li>ONNX Runtime 1.18.1</li> <li>Polygraphy: 0.49.10</li> <li>GraphSurgeon: 0.5.2</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#0101","title":"0.10.1","text":"<ul> <li> <p>fix: Check if torch 2 is available before doing dynamo cleanup</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.4.0a0+07cecf4</li> <li>TensorFlow 2.15.0</li> <li>TensorRT 10.0.1.6</li> <li>Torch-TensorRT 2.4.0.a0</li> <li>ONNX Runtime 1.18.1</li> <li>Polygraphy: 0.49.10</li> <li>GraphSurgeon: 0.5.2</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#0100","title":"0.10.0","text":"<ul> <li>new: inplace <code>nav.Module</code> accepts <code>batching</code> flag which overrides a config setting and <code>precision</code> which allows   setting appropriate configuration for TensorRT</li> <li>new: Allow to set device when loading optimized modules using <code>nav.load_optimized()</code></li> <li>new: Add support for custom i/o names and dynamic shapes in Torch ONNX Dynamo path</li> <li>new: Added <code>nav.bundle.save</code> and <code>nav.bundle.load</code> to save and load optimized models from cache</li> <li>change: Improved optimize and profile status in inplace mode</li> <li>change: Improved handling defaults for ONNX Dynamo when executing <code>nav.package.optimize</code></li> <li>fix: Maintaining modules device in <code>nav.profile()</code></li> <li>fix: Add support for all precisions for TensorRT in <code>nav.profile()</code></li> <li> <p>fix: Forward method not passed to other inplace modules.</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.4.0a0+07cecf4</li> <li>TensorFlow 2.15.0</li> <li>TensorRT 10.0.1.6</li> <li>Torch-TensorRT 2.4.0.a0</li> <li>ONNX Runtime 1.18.0</li> <li>Polygraphy: 0.49.10</li> <li>GraphSurgeon: 0.5.2</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#090","title":"0.9.0","text":"<ul> <li>new: TensorRT Timing Tactics Cache Management - using timing tactics cache files for optimization performance   improvements</li> <li>new: Added throughput saturation verification in <code>nav.profile()</code> (enabled by default)</li> <li>new: Allow to override Inplace cache dir through <code>MODEL_NAVIGATOR_DEFAULT_CACHE_DIR</code> env variable</li> <li>new: inplace <code>nav.Module</code> can now receive a function name to be used instead of call in modules/submodules, allows   customizing modules with non-standard calls</li> <li>fix: torch dynamo export and torch dynamo onnx export</li> <li>fix: measurement stabilization in <code>nav.profile()</code></li> <li>fix: inplace inference through Torch</li> <li>fix: trt_profiles argument handling in ONNX to TRT conversion</li> <li>fix: optimal shape configuration for batch size in Inplace API</li> <li>change: Disable TensorRT profile builder</li> <li> <p>change: <code>nav.optimize()</code> does not override module configuration</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.3.0a0+6ddf5cf85e</li> <li>TensorFlow 2.15.0</li> <li>TensorRT 8.6.3</li> <li>Torch-TensorRT 2.0.0.dev0</li> <li>ONNX Runtime 1.17.1</li> <li>Polygraphy: 0.49.4</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#081","title":"0.8.1","text":"<ul> <li>fix: Inference with TensorRT when model has input with empty shape</li> <li>fix: Using stabilized runners when model has no batching</li> <li>fix: Invalid dependencies for cuDNN - review known issues</li> <li>fix: Make ONNX Graph Surgeon produce artifacts within protobuf Limit (2G)</li> <li>change: Remove TensorRTCUDAGraph from default runners</li> <li> <p>change: updated ONNX package to 1.16</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.3.0a0+40ec155e58</li> <li>TensorFlow 2.15.0</li> <li>TensorRT 8.6.3</li> <li>Torch-TensorRT 2.0.0.dev0</li> <li>ONNX Runtime 1.17.1</li> <li>Polygraphy: 0.49.4</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#080","title":"0.8.0","text":"<ul> <li>new: Allow to select device for TensorRT runner</li> <li>new: Add device output buffers to TensorRT runner</li> <li>new: nav.profile added for profiling any Python function</li> <li>change: API for Inplace optimization (breaking change)</li> <li>fix: Passing inputs for Torch to ONNX export</li> <li>fix: Parse args to kwargs in torchscript-trace export</li> <li> <p>fix: Lower peak memory usage when loading Torch inplace optimized model</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.3.0a0+ebedce2</li> <li>TensorFlow 2.15.0</li> <li>TensorRT 8.6.3</li> <li>Torch-TensorRT 2.0.0.dev0</li> <li>ONNX Runtime 1.17.1</li> <li>Polygraphy: 0.49.4</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#077","title":"0.7.7","text":"<ul> <li> <p>change: Add input and output specs for Triton model repositories generated from packages</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.2.0a0+81ea7a48</li> <li>TensorFlow 2.14.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.16.2</li> <li>Polygraphy: 0.49.0</li> <li>GraphSurgeon: 0.3.27</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#076","title":"0.7.6","text":"<ul> <li>fix: Passing inputs for Torch to ONNX export</li> <li> <p>fix: Passing input data to OnnxCUDA runner</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.2.0a0+81ea7a48</li> <li>TensorFlow 2.14.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.16.2</li> <li>Polygraphy: 0.49.0</li> <li>GraphSurgeon: 0.3.27</li> <li>tf2onnx v1.16.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#075","title":"0.7.5","text":"<ul> <li>new: FP8 precision support for TensorRT</li> <li>new: Support for autocast and inference mode configuration for Torch runners</li> <li>new: Allow to select device for Torch and ONNX runners</li> <li>new: Add support for <code>default_model_filename</code> in Triton model configuration</li> <li>new: Detailed profiling of inference steps (pre- and postprocessing, memcpy and compute)</li> <li>fix: JAX export and TensorRT conversion fails when custom workspace is used</li> <li>fix: Missing max workspace size passed to TensorRT conversion</li> <li>fix: Execution of TensorRT optimize raise error during handling output metadata</li> <li> <p>fix: Limited Polygraphy version to work correctly with onnxruntime-gpu package</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.2.0a0+6a974be</li> <li>TensorFlow 2.13.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.16.2</li> <li>Polygraphy: 0.49.0</li> <li>GraphSurgeon: 0.3.27</li> <li>tf2onnx v1.15.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#074","title":"0.7.4","text":"<ul> <li>new: decoupled mode configuration in Triton Model Config</li> <li>new: support for PyTorch ExportedProgram and ONNX dynamo export</li> <li>new: added GraphSurgeon ONNX optimization</li> <li>fix: compatibility of generating PyTriton model config through adapter</li> <li>fix: installation of packages that are platform dependent</li> <li>fix: update package config with model loaded from source</li> <li>change: in TensorRT runner, when TensorType.TORCH is the return type lazily convert tensor to Torch</li> <li>change: move from Polygraphy CLI to Polygraphy Python API</li> <li> <p>change: removed Windows from support list</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+32f93b1</li> <li>TensorFlow 2.13.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.16.2</li> <li>Polygraphy: 0.49.0</li> <li>GraphSurgeon: 0.3.27</li> <li>tf2onnx v1.15.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#073","title":"0.7.3","text":"<ul> <li>new: Data dependent dynamic control flow support in nav.Module (multiple computation graphs per module)</li> <li>new: Added find max batch size utility</li> <li>new: Added utilities API documentation</li> <li>new: Add Timer class for measuring execution time of models and Inplace modules.</li> <li>fix: Use wide range of shapes for TensorRT conversion</li> <li>fix: Sorting of samples loaded from workspace</li> <li>change: in Inplace, store one sample by default per module and store shape info for all samples</li> <li> <p>change: always execute export for all supported formats</p> </li> <li> <p>Known issues and limitations:</p> <ul> <li>nav.Module moves original torch.nn.Module to the CPU, in case of weight sharing that might result in unexpected   behaviour</li> <li>For data dependent dynamic control flow (multiple computation graphs) nav.Module might copy the weights for each   separate graph</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+29c30b1</li> <li>TensorFlow 2.13.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.15.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.27</li> <li>tf2onnx v1.15.1</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#072","title":"0.7.2","text":"<ul> <li>fix: Obtaining inputs names from ONNX file for TensorRT conversion</li> <li> <p>change: Raise exception instead of exit with code when required command has failed</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+b5021ba</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.15.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.27</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#071","title":"0.7.1","text":"<ul> <li>fix: gather onnx input names based on model's forward signature</li> <li>fix: do not run TensorRT max batch size search when max batch size is None</li> <li> <p>fix: use pytree metadata to flatten torch complex outputs</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+b5021ba</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.15.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.27</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#070","title":"0.7.0","text":"<ul> <li>new: Inplace Optimize feature - optimize models directly in the Python code</li> <li>new: Non-tensor inputs and outputs support</li> <li>new: Model warmup support in Triton model configuration</li> <li>new: nav.tensorrt.optimize api added for testing and measuring performance of TensorRT models</li> <li>new: Extended custom configs to pass arguments directly to export and conversion operations like <code>torch.onnx.export</code>   or <code>polygraphy convert</code></li> <li>new: Collect GPU clock during model profiling</li> <li>new: Add option to configure minimal trials and stabilization windows for performance verification and profiling</li> <li>change: Navigator package version change to 0.2.3. Custom configurations now use trt_profiles list instead single   value</li> <li> <p>change: Store separate reproduction scripts for runners used during correctness and profiling</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+b5021ba</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.15.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.27</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#063","title":"0.6.3","text":"<ul> <li> <p>fix: Conditional imports of supported frameworks in export commands</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+4136153</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#062","title":"0.6.2","text":"<ul> <li>new: Collect information about TensorRT shapes used during conversion</li> <li>fix: Invalid link in documentation</li> <li> <p>change: Improved rendering documentation</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+4136153</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#061","title":"0.6.1","text":"<ul> <li> <p>fix: Add model from package to Triton model store with custom configs</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+4136153</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#060","title":"0.6.0","text":"<ul> <li>new: Zero-copy runners for Torch, ONNX and TensorRT - omit H2D and D2H memory copy between runners execution</li> <li>new: <code>nav.pacakge.profile</code> API method to profile generated models on provided dataloader</li> <li>change: ProfilerConfig replaced with OptimizationProfile:<ul> <li>new: OptimizationProfile impact the conversion for TensorRT</li> <li>new: <code>batch_sizes</code> and <code>max_batch_size</code> limit the max profile in TensorRT conversion</li> <li>new: Allow to provide separate dataloader for profiling - first sample used only</li> </ul> </li> <li>new: allow to run <code>nav.package.optimize</code> on empty package - status generation only</li> <li>new: use <code>torch.inference_mode</code> for inference runner when PyTorch 2.x is available</li> <li>fix: Missing <code>model</code> in config when passing package generated during <code>nav.{framework}.optimize</code> directly   to <code>nav.package.optimize</code> command</li> <li> <p>Other minor fixes and improvements</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+4136153</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#056","title":"0.5.6","text":"<ul> <li>fix: Load samples as sorted to keep valid order</li> <li>fix: Execute conversion when model already exists in path</li> <li> <p>Other minor fixes and improvements</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+fe05266f</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#055","title":"0.5.5","text":"<ul> <li>new: Public <code>nav.utilities</code> module with UnpackedDataloader wrapper</li> <li>new: Added support for strict flag in Torch custom config</li> <li>new: Extended TensorRT custom config to support builder optimization level and hardware compatibility flags</li> <li> <p>fix: Invalid optimal shape calculation for odd values in max batch size</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+fe05266f</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#054","title":"0.5.4","text":"<ul> <li>new: Custom implementation for ONNX and TensorRT runners</li> <li>new: Use CUDA 12 for JAX in unit tests and functional tests</li> <li>new: Step-by-step examples</li> <li>new: Updated documentation</li> <li>new: TensorRTCUDAGraph runner introduced with support for CUDA graphs</li> <li>fix: Optimal shape not set correctly during adaptive conversion</li> <li>fix: Find max batch size command for JAX</li> <li> <p>fix: Save stdout to logfiles in debug mode</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.1.0a0+fe05266f</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#053","title":"0.5.3","text":"<ul> <li> <p>fix: filter outputs using output_metadata in ONNX runners</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.0.0a0+1767026</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#052","title":"0.5.2","text":"<ul> <li>new: Added Contributor License Agreement (CLA)</li> <li>fix: Added missing --extra-index-url to installation instruction for pypi</li> <li>fix: Updated wheel readme</li> <li>fix: Do not run TorchScript export when only ONNX in target formats and ONNX extended export is disabled</li> <li> <p>fix: Log full traceback for ModelNavigatorUserInputError</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 2.0.0a0+1767026</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#051","title":"0.5.1","text":"<ul> <li>fix: Using relative workspace cause error during Onnx to TensorRT conversion</li> <li>fix: Added external weight in package for ONNX format</li> <li> <p>fix: bugfixes for functional tests</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#050","title":"0.5.0","text":"<ul> <li>new: Support for PyTriton deployment</li> <li>new: Support for Python models with python.optimize API</li> <li>new: PyTorch 2 compile CPU and CUDA runners</li> <li>new: Collect conversion max batch size in status</li> <li>new: PyTorch runners with <code>compile</code> support</li> <li>change: Improved handling CUDA and CPU runners</li> <li>change: Reduced finding device max batch size time by running it once as separate pipeline</li> <li> <p>change: Stored find max batch size result in separate filed in status</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#044","title":"0.4.4","text":"<ul> <li> <p>fix: when exporting single input model to saved model, unwrap one element list with inputs</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#043","title":"0.4.3","text":"<ul> <li> <p>fix: in Keras inference use model.predict(tensor) for single input models</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#042","title":"0.4.2","text":"<ul> <li>fix: loading configuration for trt_profile from package</li> <li>fix: missing reproduction scripts and logs inside package</li> <li>fix: invalid model path in reproduction script for ONNX to TRT conversion</li> <li> <p>fix: collecting metadata from ONNX model in main thread during ONNX to TRT conversion</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#041","title":"0.4.1","text":"<ul> <li> <p>fix: when specified use dynamic axes from custom OnnxConfig</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.2.2</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.43.1</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#040","title":"0.4.0","text":"<ul> <li>new: <code>optimize</code> method that replace <code>export</code> and perform max batch size search and improved profiling during process</li> <li>new: Introduced custom configs in <code>optimize</code> for better parametrization of export/conversion commands</li> <li>new: Support for adding user runners for model correctness and profiling</li> <li>new: Search for max possible batch size per format during conversion and profiling</li> <li>new: API for creating Triton model store from Navigator Package and user provided models</li> <li>change: Improved status structure for Navigator Package</li> <li>deprecated: Optimize for Triton Inference Server support</li> <li>deprecated: HuggingFace contrib module</li> <li> <p>Bug fixes and other improvements</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.2.2</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.43.1</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#038","title":"0.3.8","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 22.11</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#037","title":"0.3.7","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 22.10</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#036","title":"0.3.6","text":"<ul> <li>Updated NVIDIA containers defaults to 22.09</li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: cast int64 input data to int32 in runner for Torch-TensorRT</li> <li>new: cast 64-bit data samples to 32-bit values for TensorRT</li> <li>new: verbose flag for logging export and conversion commands to console</li> <li>new: debug flag to enable debug mode for export and conversion commands</li> <li>change: logs from commands are streamed to console during command run</li> <li>change: package load omit the log files and autogenerated scripts</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#035","title":"0.3.5","text":"<ul> <li>Updated NVIDIA containers defaults to 22.08</li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: TRTExec runner use <code>use_cuda_graph=True</code> by default</li> <li>new: log warning instead of raising error when dataloader dump inputs with <code>nan</code> or <code>inf</code> values</li> <li>new: enabled logging for command input parameters</li> <li>fix: invalid use of Polygraphy TRT profile when trt_dynamic_axes is passed to export function</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.38.0</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.19.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#034","title":"0.3.4","text":"<ul> <li>Updated NVIDIA containers defaults to 22.07</li> <li>Model Navigator OTIS:<ul> <li>deprecated: <code>TF32</code> precision for TensorRT from CLI options - will be removed in future versions</li> <li>fix: Tensorflow module was imported when obtaining model signature during conversion</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Support for building framework containers with Model Navigator installed</li> <li>new: Example for loading Navigator Package for reproducing the results</li> <li>new: Create reproducing script for correctness and performance steps</li> <li>new: TrtexecRunner for correctness and performance tests   with trtexec tool</li> <li>new: Use TF32 support by default for models with FP32 precision</li> <li>new: Reset conversion parameters to defaults when using <code>load</code> for package</li> <li>new: Testing all options for JAX export enable_xla and jit_compile parameters</li> <li>change: Profiling stability improvements</li> <li>change: Rename of <code>onnx_runtimes</code> export function parameters to <code>runtimes</code></li> <li>deprecated: <code>TF32</code> precision for TensorRT from available options - will be removed in future versions</li> <li>fix: Do not save TF-TRT models to the .nav package</li> <li>fix: Do not save TF-TRT models from the .nav package</li> <li>fix: Correctly load .nav packages when <code>_input_names</code> or <code>_output_names</code> specified</li> <li>fix: Adjust TF and TF-TRT model signatures to match <code>input_names</code></li> <li>fix: Save ONNX opset for CLI configuration inside package</li> <li>fix: Reproduction scripts were missing for failing paths</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.38.0</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#033","title":"0.3.3","text":"<ul> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Improved handling inputs and outputs metadata</li> <li>new: Navigator Package version updated to 0.1.3</li> <li>new: Backward compatibility with previous versions of Navigator Package</li> <li>fix: Dynamic shapes for output shapes were read incorrectly</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#032","title":"0.3.2","text":"<ul> <li>Updated NVIDIA containers defaults to 22.06</li> <li>Model Navigator OTIS:<ul> <li>new: Perf Analyzer profiling data use base64 format for content</li> <li>fix: Signature for TensorRT model when has <code>uint64</code> or <code>int64</code> input and/or outputs defined</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Updated navigator package format to 0.1.1</li> <li>new: Added Model Navigator version to status file</li> <li>new: Add atol and rtol configuration to CLI config for model</li> <li>new: Added experimental support for JAX models</li> <li>new: In case of export or conversion failures prepare minimal scripts to reproduce errors</li> <li>fix: Conversion parameters are not stored in Navigator Package for CLI execution</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#031","title":"0.3.1","text":"<ul> <li>Updated NVIDIA containers defaults to 22.05</li> <li>Model Navigator OTIS:<ul> <li>fix: Saving paths inside the Triton package status file</li> <li>fix: Empty list of gpus cause the process run on CPU only</li> <li>fix: Reading content from zipped Navigator Package</li> <li>fix: When no GPU or target device set to CPU <code>optimize</code> avoid running unsupported conversions in CLI</li> <li>new: Converter accept passing target device kind to selected CPU or GPU supported conversions</li> <li>new: Added support for OpenVINO accelerator for ONNXRuntime</li> <li>new: Added option <code>--config-search-early-exit-enable</code> for Model Analyzer early exit support   in manual profiling mode</li> <li>new: Added option <code>--model-config-name</code> to the <code>select</code> command.   It allows to pick a particular model configuration for deployment from the set of all configurations   generated by Triton Model Analyzer, even if it's not the best performing one.</li> <li>removed: The <code>--tensorrt-strict-types</code> option has been removed due to deprecation of the functionality   in upstream libraries.</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Added dynamic shapes support and trt dynamic shapes support for TensorFlow2 export</li> <li>new: Improved per format logging</li> <li>new: PyTorch to Torch-TRT precision selection added</li> <li>new: Advanced profiling (measurement windows, configurable batch sizes)</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.16.0</li> <li>tf2onnx: v1.10.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#030","title":"0.3.0","text":"<ul> <li>Updated NVIDIA containers defaults to 22.04</li> <li>Model Navigator Export API<ul> <li>Support for exporting models from TensorFlow2 and PyTorch source code to supported target formats</li> <li>Support for conversion from ONNX to supported target formats</li> <li>Support for exporting HuggingFace models</li> <li>Conversion, Correctness and performance tests for exported models</li> <li>Definition of package structure for storing all exported models and additional metadata</li> </ul> </li> <li>Model Navigator OTIS:<ul> <li>change: <code>run</code> command has been deprecated and may be removed in a future release</li> <li>new: <code>optimize</code> command replace <code>run</code> and produces an output <code>*.triton.nav</code> package</li> <li>new: <code>select</code> selects the best-performing configuration from <code>*.triton.nav</code> package and create a   Triton Inference Server model repository</li> <li>new: Added support for using shared memory option for Perf Analyzer</li> </ul> </li> <li> <p>Remove wkhtmltopdf package dependency</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.14.0</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#027","title":"0.2.7","text":"<ul> <li>Updated NVIDIA containers defaults to 22.02</li> <li>Removed support for Python 3.7</li> <li>Triton Model configuration related:<ul> <li>Support dynamic batching without setting preferred batch size value</li> </ul> </li> <li> <p>Profiling related:</p> <ul> <li>Deprecated <code>--config-search-max-preferred-batch-size</code> flag as is no longer supported in Triton Model Analyzer</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#026","title":"0.2.6","text":"<ul> <li>Updated NVIDIA containers defaults to 22.01</li> <li>Removed support for Python 3.6 due to EOL</li> <li>Conversion related:<ul> <li>Added support for Torch-TensorRT conversion</li> </ul> </li> <li> <p>Fixes and improvements</p> <ul> <li>Processes inside containers started by Model Navigator now run without root privileges</li> <li>Fix for volume mounts while running Triton Inference Server in container from other container</li> <li>Fix for conversion of models without file extension on input and output paths</li> <li>Fix using <code>--model-format</code> argument when input and output files have no extension</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#025","title":"0.2.5","text":"<ul> <li>Updated NVIDIA containers defaults to 21.12</li> <li>Conversion related:<ul> <li>[Experimental] TF-TRT - fixed default dataset profile generation</li> </ul> </li> <li> <p>Configuration Model on Triton related</p> <ul> <li>Fixed name for onnxruntime backend in Triton model deployment configuration</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.33.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#024","title":"0.2.4","text":"<ul> <li>Updated NVIDIA containers defaults to 21.10</li> <li>Fixed generating profiling data when <code>dtypes</code> are not passed</li> <li>Conversion related:<ul> <li>[Experimental] Added support for TF-TRT conversion</li> </ul> </li> <li>Configuration Model on Triton related<ul> <li>Added possibility to select batching mode - default, dynamic and disabled options supported</li> </ul> </li> <li>Install dependencies from pip packages instead of wheels for Polygraphy and Triton Model Analyzer</li> <li> <p>fixes and improvements</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.33.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#023","title":"0.2.3","text":"<ul> <li>Updated NVIDIA containers defaults to 21.09</li> <li>Improved naming of arguments specific for TensorRT conversion and acceleration with backward compatibility</li> <li>Use pip package for Triton Model Analyzer installation with minimal version 1.8.0</li> <li>Fixed <code>model_repository</code> path to be not relative to <code>&lt;navigator_workspace&gt;</code> dir</li> <li>Handle exit codes correctly from CLI commands</li> <li>Support for use device ids for <code>--gpus</code> argument</li> <li>Conversion related<ul> <li>Added support for precision modes to support multiple precisions during conversion to TensorRT</li> <li>Added <code>--tensorrt-sparse-weights</code> flag for sparse weight optimization for TensorRT</li> <li>Added <code>--tensorrt-strict-types</code> flag forcing it to choose tactics based on the layer precision for TensorRT</li> <li>Added <code>--tensorrt-explicit-precision</code> flag enabling explicit precision mode</li> <li>Fixed nan values appearing in relative tolerance during conversion to TensorRT</li> </ul> </li> <li>Configuration Model on Triton related<ul> <li>Removed default value for <code>engine_count_per_device</code></li> <li>Added possibility to define Triton Custom Backend parameters with <code>triton_backend_parameters</code> command</li> <li>Added possibility to define max workspace size for TensorRT backend accelerator using   argument <code>tensorrt_max_workspace_size</code></li> </ul> </li> <li>Profiling related<ul> <li>Added <code>config_search</code> prefix to all profiling parameters (BREAKING CHANGE)</li> <li>Added <code>config_search_max_preferred_batch_size</code> parameter</li> <li>Added <code>config_search_backend_parameters</code> parameter</li> </ul> </li> <li> <p>fixes and improvements</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Polygraphy: 0.32.0</li> <li>GraphSurgeon: 0.3.13</li> <li>tf2onnx: v1.9.2 (support for ONNX opset 14,   tf 1.15 and 2.6)</li> <li>Triton Model Analyzer 1.8.2</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#022","title":"0.2.2","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 21.08</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 1.7.0</li> <li>Triton Inference Server Client: 2.13.0</li> <li>Polygraphy: 0.31.1</li> <li>GraphSurgeon: 0.3.11</li> <li>tf2onnx: v1.9.1 (support for ONNX opset 14,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#021","title":"0.2.1","text":"<ul> <li>Fixed triton-model-config error when tensorrt_capture_cuda_graph flag is not passed</li> <li>Dump Conversion Comparator inputs and outputs into JSON files</li> <li>Added information in logs on the tolerance parameters values to pass the conversion verification</li> <li>Use <code>count_windows</code> mode as default option for Perf Analyzer</li> <li>Added possibility to define custom docker images</li> <li> <p>Bugfixes</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 1.6.0</li> <li>Triton Inference Server Client: 2.12.0</li> <li>Polygraphy: 0.31.1</li> <li>GraphSurgeon: 0.3.11</li> <li>tf2onnx: v1.9.1 (support for ONNX opset 14,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>TensorRT backend acceleration not supported for ONNX Runtime in Triton Inference Server ver. 21.07</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#020","title":"0.2.0","text":"<ul> <li> <p>comprehensive refactor of command-line API in order to provide more gradual   pipeline steps execution</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 21.05</li> <li>tf2onnx: v1.8.5 (support for ONNX opset 13,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>issues with TorchScript -&gt; ONNX conversion due   to issue in PyTorch 1.8<ul> <li>affected NVIDIA PyTorch containers: 20.12, 21.02, 21.03</li> <li>workaround: use PyTorch containers newer than 21.03</li> </ul> </li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#011","title":"0.1.1","text":"<ul> <li>documentation update</li> </ul>"},{"location":"CHANGELOG/#010","title":"0.1.0","text":"<ul> <li> <p>Release of main components:</p> <ul> <li>Model Converter - converts the model to a set of variants optimized for inference or to be later optimized by   Triton Inference Server backend.</li> <li>Model Repo Builder - setup Triton Inference Server Model Repository, including its configuration.</li> <li>Model Analyzer - select optimal Triton Inference Server configuration based on models compute and memory   requirements,   available computation infrastructure, and model application constraints.</li> <li>Helm Chart Generator - deploy Triton Inference Server and model with optimal configuration to cloud.</li> </ul> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 21.03+616e8a30</li> <li>tf2onnx: v1.8.4 (support for ONNX opset 13, tf 1.15   and 2.4)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   Refer to its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>issues with TorchScript -&gt; ONNX conversion due   to issue in PyTorch 1.8<ul> <li>affected NVIDIA PyTorch containers: 20.12, 21.03</li> <li>workaround: use containers different from above</li> </ul> </li> <li>Triton Inference Server stays in the background when the profile process is interrupted by the user</li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are much appreciated! Every little helps, and we will always give credit.</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/triton-inference-server/model_navigator/issues.</p> <p>If you are reporting a bug, include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever would like to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>The Triton Model Navigator could always use more documentation, whether as part of the official Triton Model Navigator docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/triton-inference-server/model_navigator/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible to make it easier to implement.</li> </ul>"},{"location":"CONTRIBUTING/#sign-your-work","title":"Sign your Work","text":"<p>We require that all contributors \"sign-off\" on their commits. This certifies that the contribution is your original work, or you have the rights to submit it under the same license or a compatible license.</p> <p>Any contribution which contains commits that are not Signed-Off will not be accepted.</p> <p>To sign off on a commit, you simply use the <code>--signoff</code> (or <code>-s</code>) option when committing your changes: <pre><code>$ git commit -s -m \"Add cool feature.\n</code></pre></p> <p>This will append the following to your commit message:</p> <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre> <p>By doing this, you certify the below:</p> <pre><code>Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.\n</code></pre>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up the <code>Triton Model Navigator</code> for local development.</p> <ol> <li>Fork the <code>Triton Model Navigator</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>$ git clone git@github.com:your_name_here/model-navigator.git\n</code></pre> <ol> <li>Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development:</li> </ol> <pre><code>$ mkvirtualenv model_navigator\n$ cd model_navigator/\n$ make install-dev\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass linters and the    tests, including testing other Python versions with tox:</li> </ol> <pre><code>$ make lint  # will run i.a. flake8 and pytype linters\n$ make test  # will run a test with on your current virtualenv\n$ make test-fw  # will run a framework test inside framework container\n</code></pre> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>$ git add .\n$ git commit -s -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, you should update the docs. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> </ol>"},{"location":"CONTRIBUTING/#tips","title":"Tips","text":"<p>To run a subset of tests:</p> <pre><code>$ pytest tests.test_model_navigator\n</code></pre>"},{"location":"CONTRIBUTING/#releasing","title":"Releasing","text":"<p>As a reminder for the maintainers on how to deploy - make sure all your changes are committed (including an entry in CHANGELOG.md) into the master branch. Then run:</p> <pre><code>$ bump2version patch # possible: major / minor / patch\n$ git push\n$ git push --tags\n</code></pre>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Add/update docstrings as defined in the Google Style Guide.</p>"},{"location":"CONTRIBUTING/#contributor-license-agreement-cla","title":"Contributor License Agreement (CLA)","text":"<p>Triton requires that all contributors (or their corporate entity) send a signed copy of the Contributor License Agreement to triton-cla@nvidia.com. NOTE: Contributors with no company affiliation can fill <code>N/A</code> in the <code>Corporation Name</code> and <code>Corporation Address</code> fields.</p>"},{"location":"LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol>"},{"location":"examples/","title":"Tutorials","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>We provide step-by-step examples that demonstrate how to use various features of Model Navigator. For the sake of readability and accessibility, we use a simple <code>torch.nn.Linear</code> model as an example. These examples illustrate how to optimize, test and deploy the model on the PyTriton and Triton Inference Server.</p>"},{"location":"examples/#step-by-step-examples","title":"Step-by-step examples","text":"<ol> <li>Optimize model</li> <li>Optimize model and verify model</li> <li>Optimize model and save package</li> <li>Load and optimize package</li> <li>Optimize and serve model on PyTriton</li> <li>Optimize and serve model on Triton Inference Server</li> <li>Optimize model and use for offline inference</li> <li>Optimize PyTorch QAT model</li> <li>Custom configuration for optimize</li> <li>Inplace Optimize of single model</li> <li>Inplace Optimize of models pipeline</li> </ol>"},{"location":"examples/#example-models","title":"Example models","text":"<p>Inside the example/models directory you can find ready to use example models in various frameworks.</p> <p><code>Python</code>: - Identity Model</p> <p><code>PyTorch</code>:</p> <ul> <li>BART (Inplace Optimize)</li> <li>BERT</li> <li>Linear Model</li> <li>ResNet50</li> <li>ResNet50 (Inplace Optimize)</li> <li>Stable Diffusion (Inplace Optimize)</li> <li>Whisper (Inplace Optimize)</li> </ul> <p><code>TensorFlow</code>:</p> <ul> <li>Linear Model</li> <li>EfficientNet</li> <li>BERT</li> </ul> <p><code>JAX</code>:</p> <ul> <li>Linear Model</li> <li>GPT-2</li> </ul> <p><code>ONNX</code>:</p> <ul> <li>Identity Model</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the installation of the Triton Model Navigator, ensure your system meets the following criteria:</p> <ul> <li>Operating System: Linux (Ubuntu 20.04+ recommended)</li> <li>Python: Version <code>3.9</code> or newer</li> <li>NVIDIA GPU</li> </ul> <p>You can use NGC Containers for PyTorch and TensorFlow which contain all necessary dependencies:</p> <ul> <li>PyTorch</li> <li>TensorFlow</li> </ul> <p>The library can be installed in:</p> <ul> <li>system environment</li> <li>virtualenv</li> <li>Docker</li> </ul> <p>The NVIDIA optimized Docker images for Python frameworks could be obtained from NVIDIA NGC Catalog.</p> <p>For using NVIDIA optimized Docker images, we recommend installing NVIDIA Container Toolkit to run model inference on NVIDIA GPU.</p>"},{"location":"installation/#install","title":"Install","text":"<p>The Triton Model Navigator can be installed from <code>pypi.org</code>.</p>"},{"location":"installation/#installing-with-pytorch-extras","title":"Installing with PyTorch extras","text":"<p>For installing with PyTorch dependencies, use:</p> <pre><code>pip install -U --extra-index-url https://pypi.ngc.nvidia.com triton-model-navigator[torch]\n</code></pre> <p>or with nvidia-pyindex:</p> <pre><code>pip install nvidia-pyindex\npip install -U triton-model-navigator[torch]\n</code></pre>"},{"location":"installation/#installing-with-tensorflow-extras","title":"Installing with TensorFlow extras","text":"<p>For installing with TensorFlow dependencies, use:</p> <pre><code>pip install -U --extra-index-url https://pypi.ngc.nvidia.com triton-model-navigator[tensorflow]\n</code></pre> <p>or with nvidia-pyindex:</p> <pre><code>pip install nvidia-pyindex\npip install -U triton-model-navigator[tensorflow]\n</code></pre>"},{"location":"installation/#installing-with-jax-extras-experimental","title":"Installing with JAX extras (experimental)","text":"<p>For installing with JAX dependencies, use:</p> <pre><code>pip install -U --extra-index-url https://pypi.ngc.nvidia.com triton-model-navigator[jax]\n</code></pre> <p>or with nvidia-pyindex:</p> <pre><code>pip install nvidia-pyindex\npip install -U triton-model-navigator[jax]\n</code></pre>"},{"location":"installation/#installing-with-onnxruntime-gpu-for-cuda-11","title":"Installing with onnxruntime-gpu for CUDA 11","text":"<p>The default CUDA version for ONNXRuntime since 1.19.0 is CUDA 12. To install with CUDA 11 support use following extra index url: <pre><code>.. --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-11/pypi/simple/ ..\n</code></pre></p>"},{"location":"installation/#building-the-wheel","title":"Building the wheel","text":"<p>The Triton Model Navigator can be built as a wheel. We have prepared all necessary steps under <code>Makefile</code> command.</p> <p>Firstly, install the Triton Model Navigator with development packages: <pre><code>make install-dev\n</code></pre></p> <p>Next, simply run:</p> <pre><code>make dist\n</code></pre> <p>The wheel will be generated in the <code>dist</code> catalog.</p>"},{"location":"known_issues/","title":"Known Issues","text":""},{"location":"known_issues/#known-issues-and-limitations","title":"Known Issues and Limitations","text":"<ul> <li>nav.Module moves original torch.nn.Module to the CPU, in case of weight sharing that might result in unexpected behavior</li> <li>For data dependent dynamic control flow (multiple computation graphs) nav.Module might copy the weights for each separate graph</li> <li>Source model running in Python can cause OOM issue when GPU memory is larger than CPU RAM memory</li> <li>Verify command could potentially experience CUDA OOM errors while trying to run inference on two models at the same time.</li> <li>Dependencies between modules in optimized pipelines may lead to unexpected behavior and failure in Inplace Optimize</li> <li>TensorRT might require manual installation of correct version of <code>nvidia-cudnn-cu12</code> package</li> <li>ONNXRuntime 1.17.x does not support ONNX IR 10 (onnx ver 1.16.0)</li> <li>ONNXRuntime 1.17.x requires cuDNN 8.x</li> <li>DistillBERT ONNX dynamo export does not support dynamic shapes</li> </ul>"},{"location":"quick_start/","title":"Quick start","text":""},{"location":"quick_start/#quick-start","title":"Quick Start","text":"<p>These sections provide an overview of optimizing a model, optimizing a pipeline, deploying a model for serving inference on PyTriton or the Triton Inference Server as well as using the Navigator Package. In each section, you will find links to learn more about the Triton Model Navigator features.</p>"},{"location":"quick_start/#optimize-pipeline","title":"Optimize Pipeline","text":"<p>The Inplace Optimize feature of the Triton Model Navigator offers a PyTorch-specific solution that seamlessly substitutes <code>nn.Module</code> objects in your code with their enhanced models.</p> <p>This process is centered around the <code>nav.Module</code> wrapper, which is used to decorate your pipeline models. It initiates the optimization across the entire pipeline when paired with the appropriate dataloader.</p> <p>The Triton Model Navigator diligently audits the decorated modules, gathering essential metadata about the inputs and outputs. It then commences the optimization process, akin to that used for individual model optimization. Ultimately, it replaces the original model with its optimized version directly within the codebase.</p> <p>The concept is built around the pipeline and dataloader:</p> <ul> <li><code>pipeline</code> - a Python object or callable with 1 or more wrapped models to optimize.</li> <li><code>dataloader</code> - a method or class generating input data. The data is utilized to perform export and conversion, as well    as determine the maximum and minimum shapes of the model inputs and create output samples that are used during    the optimization process.</li> </ul> <p>The below code presents Stable Diffusion pipeline optimization. But first, before you run the example install the required packages:</p> <pre><code>pip install transformers diffusers torch\n</code></pre> <p>Then, initialize the pipeline and wrap the model components with <code>nav.Module</code>:</p> <pre><code>import model_navigator as nav\nfrom transformers.modeling_outputs import BaseModelOutputWithPooling\nfrom diffusers import DPMSolverMultistepScheduler, StableDiffusionPipeline\n\n\ndef get_pipeline():\n    # Initialize Stable Diffusion pipeline and wrap modules for optimization\n    pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\")\n    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe = pipe.to(\"cuda\")\n\n    pipe.text_encoder = nav.Module(\n        pipe.text_encoder,\n        name=\"clip\",\n        output_mapping=lambda output: BaseModelOutputWithPooling(**output), # Mapping to convert output data to HuggingFace class\n    )\n    pipe.unet = nav.Module(\n        pipe.unet,\n        name=\"unet\",\n    )\n    pipe.vae.decoder = nav.Module(\n        pipe.vae.decoder,\n        name=\"vae\",\n    )\n\n    return pipe\n</code></pre> <p>Prepare a simple dataloader:</p> <pre><code>def get_dataloader():\n    # Please mind, the first element in tuple need to be a batch size\n    return [(1, \"a photo of an astronaut riding a horse on mars\")]\n</code></pre> <p>Execute model optimization:</p> <pre><code>pipe = get_pipeline()\ndataloader = get_dataloader()\n\nnav.optimize(pipe, dataloader)\n</code></pre> <p>Once the pipeline has been optimized, you can load explicit the most performant version of the modules executing:</p> <pre><code>nav.load_optimized()\n</code></pre> <p>At this point, you can simply use the original pipeline to generate prediction with optimized models directly in Python: <pre><code>pipe.to(\"cuda\")\n\nimages = pipe([\"a photo of an astronaut riding a horse on mars\"])\nimage = images[0][0]\n\nimage.save(\"an_astronaut_riding_a_horse.png\")\n</code></pre></p> <p>An example of how to serve a Stable Diffusion pipeline through PyTriton can be found here.</p>"},{"location":"quick_start/#optimize-model","title":"Optimize Model","text":"<p>Optimizing models using the Triton Model Navigator is as simple as calling the <code>optimize</code> function. The optimization process requires at least:</p> <ul> <li><code>model</code> - a Python object, callable, or file path with a model to optimize.</li> <li><code>dataloader</code> - a method or class generating input data. The data is utilized to perform export and conversion, as well    as determine the maximum and minimum shapes of the model inputs and create output samples that are used during    the optimization process.</li> </ul> <p>Besides the model optimization, the Triton Model Navigator collects information about model shapes, their min and max ranges, validates the correctness of optimized formats, and improves hardware utilization through searching for maximal throughput on current hardware.</p> <p>Here is an example of running <code>optimize</code> on the Torch Hub ResNet50 model:</p> <pre><code>import torch\nimport model_navigator as nav\n\n# run optimization in the parent process only\npackage = nav.torch.optimize(\n    model=torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True).eval(),\n    dataloader=[torch.randn(1, 3, 256, 256) for _ in range(10)],\n)\n</code></pre> <p>Once the model has been optimized, the created artifacts are stored in <code>navigator_workspace</code> and a Package object is returned from the function. Read more about optimize in the documentation.</p>"},{"location":"quick_start/#deploy-the-model-in-pytriton","title":"Deploy the model in PyTriton","text":"<p>The PyTriton can be used to serve inference of any optimized format. The Triton Model Navigator provides a dedicated <code>PyTritonAdapter</code> to retrieve the <code>runner</code> and other information required to bind a model for serving inference. The <code>runner</code> is an abstraction that connects the model checkpoint with its runtime, making the inference process more accessible and straightforward.</p> <p>Following that, you can initialize the PyTriton server using the adapter information:</p> <pre><code>pytriton_adapter = nav.pytriton.PyTritonAdapter(package=package, strategy=nav.MaxThroughputStrategy())\nrunner = pytriton_adapter.runner\n\nrunner.activate()\n\n\n@batch\ndef infer_func(**inputs):\n    return runner.infer(inputs)\n\n\nwith Triton() as triton:\n    triton.bind(\n        model_name=\"resnet50\",\n        infer_func=infer_func,\n        inputs=pytriton_adapter.inputs,\n        outputs=pytriton_adapter.outputs,\n        config=pytriton_adapter.config,\n    )\n    triton.serve()\n</code></pre> <p>Read more about deploying the model on PyTriton in the documentation.</p>"},{"location":"quick_start/#deploy-model-in-triton-inference-server","title":"Deploy model in Triton Inference Server","text":"<p>The optimized model can also be used for serving inference on the Triton Inference Server when the serialized format has been created. The Triton Model Navigator provides functionality to generate a model deployment configuration directly inside Triton <code>model_repository</code>. The following command will select the model format with the highest throughput and create the Triton deployment in the defined path to the model repository:</p> <pre><code>nav.triton.model_repository.add_model_from_package(\n    model_repository_path=pathlib.Path(\"model_repository\"),\n    model_name=\"resnet50\",\n    package=package,\n    strategy=nav.MaxThroughputStrategy(),\n)\n</code></pre> <p>Once the entry is created, you can simply start Triton Inference Server, mounting the defined <code>model_repository_path</code>.</p> <p>Read more about deploying the model on the Triton Inference Server in the documentation.</p>"},{"location":"quick_start/#using-the-navigator-package","title":"Using the Navigator Package","text":"<p>The <code>Navigator Package</code> is an artifact that can be produced at the end of the optimization process. The package is a simple zip file that contains the optimization details, model metadata. Serialized formats and can be saved using:</p> <pre><code>nav.package.save(\n    package=package,\n    path=\"/path/to/package.nav\"\n)\n</code></pre> <p>The package can be easily loaded on other machines and used to re-run the optimization process or profile the model. Read more about using the package in the documentation.</p>"},{"location":"quick_start/#profile-any-model-or-callable-in-python","title":"Profile any model or callable in Python","text":"<p>The Triton Model Navigator enhances models and pipelines and provides a uniform method for profiling any Python function, callable, or model. At present, our support is limited strictly to static batch profiling scenarios.</p> <p>As an example, we will use a simple function that simply sleeps for 50 ms:</p> <pre><code>import time\n\n\ndef custom_fn(input_):\n    # wait 50ms\n    time.sleep(0.05)\n    return input_\n</code></pre> <p>Let's provide a dataloader we will use for profiling:</p> <pre><code># Tuple of batch size and data sample\ndataloader = [(1, [\"This is example input\"])]\n</code></pre> <p>Finally, run the profiling of the function with the prepared dataloader:</p> <pre><code>nav.profile(custom_fn, dataloader)\n</code></pre>"},{"location":"support_matrix/","title":"Support Matrix","text":""},{"location":"support_matrix/#support-matrix","title":"Support Matrix","text":"<p>Please find below information about tested models, used environment and libraries.</p>"},{"location":"support_matrix/#verified-models","title":"Verified Models","text":"<p>We have verified that the NVIDIA Triton Model Navigator Optimize works correctly for the following models.</p> Source Model NVIDIA DeepLearningExamples ResNet50 PyT NVIDIA DeepLearningExamples EfficientNet PyT NVIDIA DeepLearningExamples EfficientNet TF2 NVIDIA DeepLearningExamples BERT TF2 HuggingFace GPT2 Jax HuggingFace GPT2 PyT HuggingFace GPT2 TF2 HuggingFace DistilBERT PyT HuggingFace DistilGPT2 TF2 HuggingFace Stable Diffusion HuggingFace Automatic Speech Recognition"},{"location":"support_matrix/#libraries-and-packages","title":"Libraries and Packages","text":"<p>A set of component versions are imposed by the used NGC container. During testing, we have used <code>24.02</code> a container version that contains:</p> <ul> <li>PyTorch 2.3.0a0+ebedce2</li> <li>TensorFlow 2.15.0</li> <li>TensorRT 8.6.3</li> <li>Torch-TensorRT 2.0.0.dev0</li> <li>ONNX Runtime 1.17.1</li> <li>Polygraphy: 0.49.4</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.16.1</li> <li>Other components versions depend on the used framework containers versions.   See its support matrix   for a detailed summary.</li> </ul>"},{"location":"inference_deployment/pytriton/deployment/","title":"Deployment on Pytriton","text":""},{"location":"inference_deployment/pytriton/deployment/#deployment-on-pytriton","title":"Deployment on PyTriton","text":"<p>The PyTriton is a Flask/FastAPI-like interface that simplifies Triton's deployment in Python environments. In general, using PyTriton can serve any Python function. The Triton Model Navigator provides a <code>runner</code> - an abstraction that connects the model checkpoint with its runtime, making the inference process more accessible and straightforward. The <code>runner</code> is a Python API through which an optimized model can serve inference.</p>"},{"location":"inference_deployment/pytriton/deployment/#obtaining-runner-from-package","title":"Obtaining runner from Package","text":"<p>The Navigator Package provides an API for obtaining the model for serving inference. One of the options is to obtain the <code>runner</code>:</p> <pre><code>runner = package.get_runner()\n</code></pre> <p>The default behavior is to select the model and runner which during profiling obtained the smallest latency and the largest throughput. This runner is considered as most optimal for serving inference queries. Learn more about the <code>get_runner</code> method in Navigator Package API.</p> <p>To use the runner in PyTriton additional information for the serving model is required. For that purpose, we provide a <code>PyTritonAdapter</code> that contains all the minimal information required to prepare for successful deployment of a model using PyTriton.</p>"},{"location":"inference_deployment/pytriton/deployment/#using-pytritonadapter","title":"Using PyTritonAdapter","text":"<p>The Triton Model Navigator provides a dedicated <code>PyTritonAdapter</code> to retrieve the <code>runner</code> and other information required to bind a model for serving inference. Following that, you can initialize the PyTriton server using the adapter information:</p> <pre><code>pytriton_adapter = nav.pytriton.PyTritonAdapter(package=package, strategy=nav.MaxThroughputStrategy())\nrunner = pytriton_adapter.runner\n\nrunner.activate()\n\n\n@batch\ndef infer_func(**inputs):\n    return runner.infer(inputs)\n\n\nwith Triton() as triton:\n    triton.bind(\n        model_name=\"resnet50\",\n        infer_func=infer_func,\n        inputs=pytriton_adapter.inputs,\n        outputs=pytriton_adapter.outputs,\n        config=pytriton_adapter.config,\n    )\n    triton.serve()\n</code></pre> <p>Once the python script is executed, the model inference is served through HTTP/gRPC endpoints.</p> <p>Read more about the adapter API and deployment configuration.</p>"},{"location":"inference_deployment/pytriton/api/adapter/","title":"Adapter","text":""},{"location":"inference_deployment/pytriton/api/adapter/#model_navigator.pytriton.PyTritonAdapter","title":"model_navigator.pytriton.PyTritonAdapter","text":"<pre><code>PyTritonAdapter(package, strategies=None, runner_return_type=NUMPY)\n</code></pre> <p>Provides model and configuration for PyTrtion deployment.</p> <p>Initialize PyTritonAdapter.</p> <p>Parameters:</p> <ul> <li> <code>package</code>               (<code>Package</code>)           \u2013            <p>A package object to be searched for best possible model.</p> </li> <li> <code>strategies</code>               (<code>Optional[List[RuntimeSearchStrategy]]</code>, default:                   <code>None</code> )           \u2013            <p>List of strategies for finding the best model. Strategies are selected in provided order. When         first fails, next strategy from the list is used. When no strategies have been provided it         defaults to [<code>MaxThroughputAndMinLatencyStrategy</code>, <code>MinLatencyStrategy</code>]</p> </li> <li> <code>runner_return_type</code>               (<code>TensorType</code>, default:                   <code>NUMPY</code> )           \u2013            <p>The type of the output tensor. Defaults to <code>TensorType.NUMPY</code>. If the return_type supports CUDA tensors (e.g. TensorType.TORCH) and the input tensors are on CUDA, there will be no additional data transfer between CPU and GPU.</p> </li> </ul> Source code in <code>model_navigator/pytriton/__init__.py</code> <pre><code>def __init__(\n    self,\n    package: Package,\n    strategies: Optional[List[RuntimeSearchStrategy]] = None,\n    runner_return_type: TensorType = TensorType.NUMPY,\n):\n    \"\"\"Initialize PyTritonAdapter.\n\n    Args:\n        package: A package object to be searched for best possible model.\n        strategies: List of strategies for finding the best model. Strategies are selected in provided order. When\n                    first fails, next strategy from the list is used. When no strategies have been provided it\n                    defaults to [`MaxThroughputAndMinLatencyStrategy`, `MinLatencyStrategy`]\n        runner_return_type: The type of the output tensor. Defaults to `TensorType.NUMPY`.\n            If the return_type supports CUDA tensors (e.g. TensorType.TORCH) and the input tensors are on CUDA,\n            there will be no additional data transfer between CPU and GPU.\n    \"\"\"\n    self._package = package\n    self._strategies = strategies or DEFAULT_RUNTIME_STRATEGIES\n    self._runner = self._package.get_runner(strategies=strategies, return_type=runner_return_type)\n    self._batching = self._package.status.config.get(\"batch_dim\", None) == 0\n</code></pre>"},{"location":"inference_deployment/pytriton/api/adapter/#model_navigator.pytriton.PyTritonAdapter.batching","title":"batching  <code>property</code>","text":"<pre><code>batching\n</code></pre> <p>Returns status of batching support by the runner.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if runner supports batching, False otherwise.</p> </li> </ul>"},{"location":"inference_deployment/pytriton/api/adapter/#model_navigator.pytriton.PyTritonAdapter.config","title":"config  <code>property</code>","text":"<pre><code>config\n</code></pre> <p>Returns config for pytriton.</p> <p>Returns:</p> <ul> <li> <code>ModelConfig</code>           \u2013            <p>ModelConfig with configuration for PyTrtion bind method.</p> </li> </ul>"},{"location":"inference_deployment/pytriton/api/adapter/#model_navigator.pytriton.PyTritonAdapter.inputs","title":"inputs  <code>property</code>","text":"<pre><code>inputs\n</code></pre> <p>Returns inputs configuration.</p> <p>Returns:</p> <ul> <li> <code>List[Tensor]</code>           \u2013            <p>List with Tensor objects describing inputs configuration of runner</p> </li> </ul>"},{"location":"inference_deployment/pytriton/api/adapter/#model_navigator.pytriton.PyTritonAdapter.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs\n</code></pre> <p>Returns outputs configuration.</p> <p>Returns:</p> <ul> <li> <code>List[Tensor]</code>           \u2013            <p>List with Tensor objects describing outputs configuration of runner</p> </li> </ul>"},{"location":"inference_deployment/pytriton/api/adapter/#model_navigator.pytriton.PyTritonAdapter.runner","title":"runner  <code>property</code>","text":"<pre><code>runner\n</code></pre> <p>Returns runner.</p> <pre><code>Runner must be activated before use with activate() method.\n</code></pre> <p>Returns:</p> <ul> <li> <code>NavigatorRunner</code>           \u2013            <p>Model Navigator runner.</p> </li> </ul>"},{"location":"inference_deployment/pytriton/api/config/","title":"Deployment Config","text":""},{"location":"inference_deployment/pytriton/api/config/#model_navigator.pytriton.ModelConfig","title":"model_navigator.pytriton.ModelConfig  <code>dataclass</code>","text":"<pre><code>ModelConfig(batching=True, max_batch_size=4, batcher=DynamicBatcher(), response_cache=False, decoupled=False)\n</code></pre> <p>Additional model configuration for running model through Triton Inference Server.</p> <p>Parameters:</p> <ul> <li> <code>batching</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag to enable/disable batching for model.</p> </li> <li> <code>max_batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The maximal batch size that would be handled by model.</p> </li> <li> <code>batcher</code>               (<code>DynamicBatcher</code>, default:                   <code>DynamicBatcher()</code> )           \u2013            <p>Configuration of Dynamic Batching for the model.</p> </li> <li> <code>response_cache</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Flag to enable/disable response cache for the model</p> </li> <li> <code>decoupled</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Flag to enable/disable decoupled transaction policy</p> </li> </ul>"},{"location":"inference_deployment/pytriton/api/config/#model_navigator.pytriton.DynamicBatcher","title":"model_navigator.pytriton.DynamicBatcher  <code>dataclass</code>","text":"<pre><code>DynamicBatcher(\n    max_queue_delay_microseconds=0,\n    preferred_batch_size=None,\n    preserve_ordering=False,\n    priority_levels=0,\n    default_priority_level=0,\n    default_queue_policy=None,\n    priority_queue_policy=None,\n)\n</code></pre> <p>Dynamic batcher configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> <ul> <li> <code>max_queue_delay_microseconds</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum time, in microseconds, a request will be delayed in                           the scheduling queue to wait for additional requests for batching.</p> </li> <li> <code>preferred_batch_size</code>               (<code>Optional[list]</code>, default:                   <code>None</code> )           \u2013            <p>Preferred batch sizes for dynamic batching.</p> </li> <li> <code>preserve_ordering</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Should the dynamic batcher preserve the ordering of responses to                 match the order of requests received by the scheduler.</p> </li> <li> <code>priority_levels</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The number of priority levels to be enabled for the model.</p> </li> <li> <code>default_priority_level</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The priority level used for requests that don't specify their priority.</p> </li> <li> <code>default_queue_policy</code>               (<code>Optional[QueuePolicy]</code>, default:                   <code>None</code> )           \u2013            <p>The default queue policy used for requests.</p> </li> <li> <code>priority_queue_policy</code>               (<code>Optional[Dict[int, QueuePolicy]]</code>, default:                   <code>None</code> )           \u2013            <p>Specify the queue policy for the priority level.</p> </li> </ul>"},{"location":"inference_deployment/pytriton/api/config/#model_navigator.pytriton.Tensor","title":"model_navigator.pytriton.Tensor  <code>dataclass</code>","text":"<pre><code>Tensor(shape, dtype, name=None, optional=False)\n</code></pre> <p>Model input and output definition for Triton deployment.</p> <p>Parameters:</p> <ul> <li> <code>shape</code>               (<code>tuple</code>)           \u2013            <p>Shape of the input/output tensor.</p> </li> <li> <code>dtype</code>               (<code>Union[dtype, Type[dtype], Type[object]]</code>)           \u2013            <p>Data type of the input/output tensor.</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name of the input/output of model.</p> </li> <li> <code>optional</code>               (<code>Optional[bool]</code>, default:                   <code>False</code> )           \u2013            <p>Flag to mark if input is optional.</p> </li> </ul>"},{"location":"inference_deployment/pytriton/api/config/#model_navigator.pytriton.QueuePolicy","title":"model_navigator.pytriton.QueuePolicy  <code>dataclass</code>","text":"<pre><code>QueuePolicy(timeout_action=REJECT, default_timeout_microseconds=0, allow_timeout_override=False, max_queue_size=0)\n</code></pre> <p>Model queue policy configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> <ul> <li> <code>timeout_action</code>               (<code>TimeoutAction</code>, default:                   <code>REJECT</code> )           \u2013            <p>The action applied to timed-out request.</p> </li> <li> <code>default_timeout_microseconds</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The default timeout for every request, in microseconds.</p> </li> <li> <code>allow_timeout_override</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether individual request can override the default timeout value.</p> </li> <li> <code>max_queue_size</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum queue size for holding requests.</p> </li> </ul>"},{"location":"inference_deployment/pytriton/api/config/#model_navigator.pytriton.TimeoutAction","title":"model_navigator.pytriton.TimeoutAction","text":"<p>               Bases: <code>Enum</code></p> <p>Timeout action definition for timeout_action QueuePolicy field.</p> <p>Parameters:</p> <ul> <li> <code>REJECT</code>           \u2013            <p>Reject the request and return error message accordingly.</p> </li> <li> <code>DELAY</code>           \u2013            <p>Delay the request until all other requests at the same (or higher) priority levels that have not reached their timeouts are processed.</p> </li> </ul>"},{"location":"inference_deployment/triton/deployment/","title":"Deployment on Triton Inference Server","text":""},{"location":"inference_deployment/triton/deployment/#deployment-on-triton-inference-server","title":"Deployment on Triton Inference Server","text":"<p>The Triton Model Navigator provides an API for working with the Triton model repository. Currently, we support adding your model or a pre-selected model from a Navigator Package.</p> <p>The API only provides possible functionality for the given model's type and only provides offline validation of the provided configuration. In the end, the model with the configuration is created inside the provided model repository path.</p>"},{"location":"inference_deployment/triton/deployment/#adding-your-model-to-the-triton-model-repository","title":"Adding your model to the Triton model repository","text":"<p>When you work with an already exported model, you can provide a path to where one's model is located. Then you can use one of the specialized APIs that guides you through what options are possible for deployment of the selected model type.</p> <p>Example of deploying a TensorRT model:</p> <pre><code>import model_navigator as nav\n\nnav.triton.model_repository.add_model(\n    model_repository_path=\"/path/to/triton/model/repository\",\n    model_path=\"/path/to/model/plan/file\",\n    model_name=\"NameOfModel\",\n    config=nav.triton.TensorRTModelConfig(\n        max_batch_size=256,\n        optimization=nav.triton.CUDAGraphOptimization(),\n        response_cache=True,\n    )\n)\n</code></pre> <p>The model catalog with the model file and configuration will be created inside <code>model_repository_path</code>. More about the function you can find in the adding model section.</p>"},{"location":"inference_deployment/triton/deployment/#adding-model-from-package-to-the-triton-model-repository","title":"Adding model from package to the Triton model repository","text":"<p>When you want to deploy a model from a package created during the <code>optimize</code> process, you can use:</p> <pre><code>import model_navigator as nav\n\nnav.triton.model_repository.add_model_from_package(\n    model_repository_path=\"/path/to/triton/model/repository\",\n    model_name=\"NameOfModel\",\n    package=package,\n)\n</code></pre> <p>The model is automatically selected based on profiling results. The default selection options can be adjusted by changing the <code>strategy</code> argument. More about the function you can find in adding model section.</p>"},{"location":"inference_deployment/triton/deployment/#using-triton-model-analyzer","title":"Using Triton Model Analyzer","text":"<p>A model added to the Triton Inference Server can be further optimized in the target environment using the Triton Model Analyzer.</p> <p>Please follow the documentation to learn more about how to use the Triton Model Analyzer.</p>"},{"location":"inference_deployment/triton/api/accelerators/","title":"Accelerators","text":""},{"location":"inference_deployment/triton/api/accelerators/#accelerators","title":"Accelerators","text":""},{"location":"inference_deployment/triton/api/accelerators/#model_navigator.triton.AutoMixedPrecisionAccelerator","title":"model_navigator.triton.AutoMixedPrecisionAccelerator  <code>dataclass</code>","text":"<pre><code>AutoMixedPrecisionAccelerator()\n</code></pre> <p>Auto-mixed-precision accelerator for TensorFlow. Enable automatic FP16 precision.</p> <p>Currently empty - no arguments required.</p>"},{"location":"inference_deployment/triton/api/accelerators/#model_navigator.triton.GPUIOAccelerator","title":"model_navigator.triton.GPUIOAccelerator  <code>dataclass</code>","text":"<pre><code>GPUIOAccelerator()\n</code></pre> <p>GPU IO accelerator for TensorFlow.</p> <p>Currently empty - no arguments required.</p>"},{"location":"inference_deployment/triton/api/accelerators/#model_navigator.triton.OpenVINOAccelerator","title":"model_navigator.triton.OpenVINOAccelerator  <code>dataclass</code>","text":"<pre><code>OpenVINOAccelerator()\n</code></pre> <p>OpenVINO optimization.</p> <p>Currently empty - no arguments required.</p>"},{"location":"inference_deployment/triton/api/accelerators/#model_navigator.triton.OpenVINOAccelerator","title":"model_navigator.triton.OpenVINOAccelerator  <code>dataclass</code>","text":"<pre><code>OpenVINOAccelerator()\n</code></pre> <p>OpenVINO optimization.</p> <p>Currently empty - no arguments required.</p>"},{"location":"inference_deployment/triton/api/accelerators/#model_navigator.triton.TensorRTAccelerator","title":"model_navigator.triton.TensorRTAccelerator  <code>dataclass</code>","text":"<pre><code>TensorRTAccelerator(precision=FP32, max_workspace_size=None, max_cached_engines=None, minimum_segment_size=None)\n</code></pre> <p>TensorRT accelerator configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>precision</code>               (<code>TensorRTOptPrecision</code>, default:                   <code>FP32</code> )           \u2013            <p>The precision used for optimization</p> </li> <li> <code>max_workspace_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum GPU memory the model can use temporarily during execution</p> </li> <li> <code>max_cached_engines</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of cached TensorRT engines in dynamic TensorRT ops</p> </li> <li> <code>minimum_segment_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The smallest model subgraph that will be considered for optimization by TensorRT</p> </li> </ul>"},{"location":"inference_deployment/triton/api/accelerators/#model_navigator.triton.TensorRTOptPrecision","title":"model_navigator.triton.TensorRTOptPrecision","text":"<p>               Bases: <code>Enum</code></p> <p>TensorRT optimization allowed precision.</p> <p>Parameters:</p> <ul> <li> <code>FP16</code>           \u2013            <p>fp16 precision</p> </li> <li> <code>FP32</code>           \u2013            <p>fp32 precision</p> </li> </ul>"},{"location":"inference_deployment/triton/api/adding_model/","title":"Adding Model","text":""},{"location":"inference_deployment/triton/api/adding_model/#model-store-api","title":"Model Store API","text":""},{"location":"inference_deployment/triton/api/adding_model/#model_navigator.triton.model_repository.add_model","title":"model_navigator.triton.model_repository.add_model","text":"<pre><code>add_model(model_repository_path, model_name, model_path, config, model_version=1)\n</code></pre> <p>Generate model deployment inside provided model store path.</p> <p>The config requires specialized configuration to be passed for backend on which model is executed. Example:</p> <ul> <li>ONNX model requires ONNXModelConfig</li> <li>TensorRT model requires TensorRTModelConfig</li> <li>TorchScript or Torch-TensorRT models requires PyTorchModelConfig</li> <li>TensorFlow SavedModel or TensorFlow-TensorRT models requires TensorFlowModelConfig</li> <li>Python model requires PythonModelConfig</li> <li>TensorRT-LLM model requires TensorRTLLMModelConfig</li> </ul> <p>Parameters:</p> <ul> <li> <code>model_repository_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path where deployment should be created</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Name under which model is deployed in Triton Inference Server</p> </li> <li> <code>model_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to model</p> </li> <li> <code>config</code>               (<code>Union[ONNXModelConfig, TensorRTModelConfig, PyTorchModelConfig, PythonModelConfig, TensorFlowModelConfig, TensorRTLLMModelConfig]</code>)           \u2013            <p>Specialized configuration of model for backend on which model is executed</p> </li> <li> <code>model_version</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Version of model that is deployed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>           \u2013            <p>Path to created model store</p> </li> </ul> Source code in <code>model_navigator/triton/model_repository.py</code> <pre><code>def add_model(\n    model_repository_path: Union[str, pathlib.Path],\n    model_name: str,\n    model_path: Union[str, pathlib.Path],\n    config: Union[\n        ONNXModelConfig,\n        TensorRTModelConfig,\n        PyTorchModelConfig,\n        PythonModelConfig,\n        TensorFlowModelConfig,\n        TensorRTLLMModelConfig,\n    ],\n    model_version: int = 1,\n) -&gt; pathlib.Path:\n    \"\"\"Generate model deployment inside provided model store path.\n\n    The config requires specialized configuration to be passed for backend on which model is executed. Example:\n\n    - ONNX model requires ONNXModelConfig\n    - TensorRT model requires TensorRTModelConfig\n    - TorchScript or Torch-TensorRT models requires PyTorchModelConfig\n    - TensorFlow SavedModel or TensorFlow-TensorRT models requires TensorFlowModelConfig\n    - Python model requires PythonModelConfig\n    - TensorRT-LLM model requires TensorRTLLMModelConfig\n\n    Args:\n        model_repository_path: Path where deployment should be created\n        model_name: Name under which model is deployed in Triton Inference Server\n        model_path: Path to model\n        config: Specialized configuration of model for backend on which model is executed\n        model_version: Version of model that is deployed\n\n    Returns:\n         Path to created model store\n    \"\"\"\n    if isinstance(config, ONNXModelConfig):\n        model_config = ModelConfigBuilder.from_onnx_config(\n            model_name=model_name,\n            model_version=model_version,\n            onnx_config=config,\n        )\n    elif isinstance(config, TensorFlowModelConfig):\n        model_config = ModelConfigBuilder.from_tensorflow_config(\n            model_name=model_name,\n            model_version=model_version,\n            tensorflow_config=config,\n        )\n    elif isinstance(config, PythonModelConfig):\n        model_config = ModelConfigBuilder.from_python_config(\n            model_name=model_name,\n            model_version=model_version,\n            python_config=config,\n        )\n    elif isinstance(config, PyTorchModelConfig):\n        model_config = ModelConfigBuilder.from_pytorch_config(\n            model_name=model_name,\n            model_version=model_version,\n            pytorch_config=config,\n        )\n    elif isinstance(config, TensorRTModelConfig):\n        model_config = ModelConfigBuilder.from_tensorrt_config(\n            model_name=model_name,\n            model_version=model_version,\n            tensorrt_config=config,\n        )\n    elif isinstance(config, TensorRTLLMModelConfig):\n        model_config = ModelConfigBuilder.from_tensorrt_llm_config(\n            model_name=model_name,\n            model_version=model_version,\n            tensorrt_llm_config=config,\n        )\n    else:\n        raise ModelNavigatorWrongParameterError(f\"Unsupported model config provided: {config.__class__}\")\n\n    model_repository_path = pathlib.Path(model_repository_path)\n\n    # Collect model filename if default not provided\n    backend = model_config.backend or model_config.platform\n    model_filename = model_config.default_model_filename or _get_default_filename(backend=backend)\n\n    # Path to model version catalog\n    model_version_path = _get_version_path(\n        model_repository_path=model_repository_path,\n        model_name=model_config.model_name,\n        version=model_config.model_version,\n    )\n\n    if isinstance(config, TensorRTLLMModelConfig):\n        config.engine_dir = model_version_path / model_filename\n\n    initial_state_files = _collect_initial_state_files(model_config=model_config)\n    warmup_files = _collect_warmup_files(model_config=model_config)\n\n    triton_model_repository = _TritonModelRepository(\n        model_repository_path=model_repository_path,\n        model_name=model_name,\n        model_version=model_version,\n        model_filename=model_filename,\n    )\n\n    return triton_model_repository.deploy_model(\n        model_path=pathlib.Path(model_path),\n        model_config=model_config,\n        warmup_files=warmup_files,\n        initial_state_files=initial_state_files,\n    )\n</code></pre>"},{"location":"inference_deployment/triton/api/adding_model/#model_navigator.triton.model_repository.add_model_from_package","title":"model_navigator.triton.model_repository.add_model_from_package","text":"<pre><code>add_model_from_package(\n    model_repository_path, model_name, package, model_version=1, strategies=None, response_cache=False, warmup=False\n)\n</code></pre> <p>Create the Triton Model Store with optimized model and save it to <code>model_repository_path</code>.</p> <p>Parameters:</p> <ul> <li> <code>model_repository_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path where the model store is located</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Name under which model is deployed in Triton Inference Server</p> </li> <li> <code>package</code>               (<code>Package</code>)           \u2013            <p>Package for which model store is created</p> </li> <li> <code>model_version</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Version of model that is deployed</p> </li> <li> <code>strategies</code>               (<code>Optional[List[RuntimeSearchStrategy]]</code>, default:                   <code>None</code> )           \u2013            <p>List of strategies for finding the best model. Strategies are selected in provided order. When         first fails, next strategy from the list is used. When no strategies have been provided it         defaults to [<code>MaxThroughputAndMinLatencyStrategy</code>, <code>MinLatencyStrategy</code>]</p> </li> <li> <code>response_cache</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable response cache for model</p> </li> <li> <code>warmup</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable warmup for min and max batch size</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Path to created model store</p> </li> </ul> Source code in <code>model_navigator/triton/model_repository.py</code> <pre><code>def add_model_from_package(\n    model_repository_path: Union[str, pathlib.Path],\n    model_name: str,\n    package: Package,\n    model_version: int = 1,\n    strategies: Optional[List[RuntimeSearchStrategy]] = None,\n    response_cache: bool = False,\n    warmup: bool = False,\n):\n    \"\"\"Create the Triton Model Store with optimized model and save it to `model_repository_path`.\n\n    Args:\n        model_repository_path: Path where the model store is located\n        model_name: Name under which model is deployed in Triton Inference Server\n        package: Package for which model store is created\n        model_version: Version of model that is deployed\n        strategies: List of strategies for finding the best model. Strategies are selected in provided order. When\n                    first fails, next strategy from the list is used. When no strategies have been provided it\n                    defaults to [`MaxThroughputAndMinLatencyStrategy`, `MinLatencyStrategy`]\n        response_cache: Enable response cache for model\n        warmup: Enable warmup for min and max batch size\n\n    Returns:\n        Path to created model store\n    \"\"\"\n    if package.is_empty():\n        raise ModelNavigatorEmptyPackageError(\"No models available in the package. Triton deployment is not possible.\")\n\n    if package.config.batch_dim not in [0, None]:\n        raise ModelNavigatorWrongParameterError(\n            \"Only models without batching or batch dimension on first place in shape are supported for Triton.\"\n        )\n\n    if strategies is None:\n        strategies = DEFAULT_RUNTIME_STRATEGIES\n\n    batching = package.config.batch_dim == 0\n\n    runtime_result = None\n    for strategy in strategies:\n        try:\n            runtime_result = RuntimeAnalyzer.get_runtime(\n                models_status=package.status.models_status,\n                strategy=strategy,\n                formats=[fmt.value for fmt in TRITON_FORMATS],\n                runners=[runner.name() for runner in TRITON_RUNNERS],\n            )\n            break\n        except ModelNavigatorRuntimeAnalyzerError:\n            LOGGER.debug(f\"No model found with strategy: {strategy}\")\n\n    if not runtime_result:\n        raise ModelNavigatorError(\"No optimized model found in package.\")\n\n    max_batch_size = max(\n        profiling_results.batch_size if profiling_results.batch_size is not None else 0\n        for profiling_results in runtime_result.runner_status.result[Performance.name][\"profiling_results\"]\n    )\n\n    model_warmup = {}\n    if warmup:\n        model_warmup = _prepare_model_warmup(\n            batching=batching,\n            max_batch_size=max_batch_size,\n            package=package,\n        )\n\n    if runtime_result.model_status.model_config.format == Format.TENSORRT:\n        input_metadata, output_metadata = (\n            _prepare_tensorrt_metadata(package.status.input_metadata),\n            _prepare_tensorrt_metadata(package.status.output_metadata),\n        )\n    else:\n        input_metadata, output_metadata = package.status.input_metadata, package.status.output_metadata\n\n    inputs = _input_tensor_from_metadata(\n        input_metadata,\n        batching=batching,\n    )\n    outputs = _output_tensor_from_metadata(\n        output_metadata,\n        batching=batching,\n    )\n\n    if runtime_result.model_status.model_config.format == Format.ONNX:\n        config = _onnx_config_from_runtime_result(\n            batching=batching,\n            max_batch_size=max_batch_size,\n            inputs=inputs,\n            outputs=outputs,\n            response_cache=response_cache,\n            runtime_result=runtime_result,\n            warmup=model_warmup,\n        )\n\n    elif runtime_result.model_status.model_config.format in [Format.TF_SAVEDMODEL, Format.TF_TRT]:\n        config = _tensorflow_config_from_runtime_result(\n            batching=batching,\n            max_batch_size=max_batch_size,\n            inputs=inputs,\n            outputs=outputs,\n            response_cache=response_cache,\n            runtime_result=runtime_result,\n            warmup=model_warmup,\n        )\n    elif runtime_result.model_status.model_config.format in [Format.TORCHSCRIPT, Format.TORCH_TRT]:\n        config = _pytorch_config_from_runtime_result(\n            batching=batching,\n            max_batch_size=max_batch_size,\n            inputs=inputs,\n            outputs=outputs,\n            response_cache=response_cache,\n            runtime_result=runtime_result,\n            warmup=model_warmup,\n        )\n    elif runtime_result.model_status.model_config.format == Format.TENSORRT:\n        config = _tensorrt_config_from_runtime_result(\n            batching=batching,\n            max_batch_size=max_batch_size,\n            inputs=inputs,\n            outputs=outputs,\n            response_cache=response_cache,\n            warmup=model_warmup,\n        )\n    else:\n        raise ModelNavigatorError(\n            f\"Unsupported model format selected: {runtime_result.model_status.model_config.format}\"\n        )\n\n    return add_model(\n        model_repository_path=model_repository_path,\n        model_name=model_name,\n        model_version=model_version,\n        model_path=package.workspace.path / runtime_result.model_status.model_config.path,\n        config=config,\n    )\n</code></pre>"},{"location":"inference_deployment/triton/api/dynamic_batcher/","title":"Dynamic Batcher","text":""},{"location":"inference_deployment/triton/api/dynamic_batcher/#dynamic-batcher","title":"Dynamic Batcher","text":""},{"location":"inference_deployment/triton/api/dynamic_batcher/#model_navigator.triton.DynamicBatcher","title":"model_navigator.triton.DynamicBatcher  <code>dataclass</code>","text":"<pre><code>DynamicBatcher(\n    max_queue_delay_microseconds=0,\n    preferred_batch_size=None,\n    preserve_ordering=False,\n    priority_levels=0,\n    default_priority_level=0,\n    default_queue_policy=None,\n    priority_queue_policy=None,\n)\n</code></pre> <p>Dynamic batching configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>max_queue_delay_microseconds</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum time, in microseconds, a request will be delayed in                           the scheduling queue to wait for additional requests for batching.</p> </li> <li> <code>preferred_batch_size</code>               (<code>Optional[list]</code>, default:                   <code>None</code> )           \u2013            <p>Preferred batch sizes for dynamic batching.</p> </li> <li> <code>preserve_ordering</code>           \u2013            <p>Should the dynamic batcher preserve the ordering of responses to                 match the order of requests received by the scheduler.</p> </li> <li> <code>priority_levels</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The number of priority levels to be enabled for the model.</p> </li> <li> <code>default_priority_level</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The priority level used for requests that don't specify their priority.</p> </li> <li> <code>default_queue_policy</code>               (<code>Optional[QueuePolicy]</code>, default:                   <code>None</code> )           \u2013            <p>The default queue policy used for requests.</p> </li> <li> <code>priority_queue_policy</code>               (<code>Optional[Dict[int, QueuePolicy]]</code>, default:                   <code>None</code> )           \u2013            <p>Specify the queue policy for the priority level.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/dynamic_batcher/#model_navigator.triton.DynamicBatcher.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.default_priority_level &gt; self.priority_levels:\n        raise ModelNavigatorWrongParameterError(\n            f\"The `default_priority_level` must be between 1 and {self.priority_levels}.\"\n        )\n\n    if self.priority_queue_policy:\n        if not self.priority_levels:\n            raise ModelNavigatorWrongParameterError(\n                \"Provide the `priority_levels` if you want to define `priority_queue_policy` for Dynamic Batching.\"\n            )\n\n        for priority in self.priority_queue_policy.keys():\n            if priority &lt; 0 or priority &gt; self.priority_levels:\n                raise ModelNavigatorWrongParameterError(\n                    f\"Invalid `priority`={priority} provided. The value must be between \"\n                    f\"1 and {self.priority_levels}.\"\n                )\n</code></pre>"},{"location":"inference_deployment/triton/api/dynamic_batcher/#model_navigator.triton.QueuePolicy","title":"model_navigator.triton.QueuePolicy  <code>dataclass</code>","text":"<pre><code>QueuePolicy(timeout_action=REJECT, default_timeout_microseconds=0, allow_timeout_override=False, max_queue_size=0)\n</code></pre> <p>Model queue policy configuration.</p> <p>Used for <code>default_queue_policy</code> and <code>priority_queue_policy</code> fields in DynamicBatcher configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>timeout_action</code>               (<code>TimeoutAction</code>, default:                   <code>REJECT</code> )           \u2013            <p>The action applied to timed-out request.</p> </li> <li> <code>default_timeout_microseconds</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The default timeout for every request, in microseconds.</p> </li> <li> <code>allow_timeout_override</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether individual request can override the default timeout value.</p> </li> <li> <code>max_queue_size</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum queue size for holding requests.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/dynamic_batcher/#model_navigator.triton.TimeoutAction","title":"model_navigator.triton.TimeoutAction","text":"<p>               Bases: <code>Enum</code></p> <p>Timeout action definition for timeout_action QueuePolicy field.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>REJECT</code>           \u2013            <p>\"REJECT\"</p> </li> <li> <code>DELAY</code>           \u2013            <p>\"DELAY\"</p> </li> </ul>"},{"location":"inference_deployment/triton/api/inputs_and_outputs/","title":"Inputs and Outputs","text":""},{"location":"inference_deployment/triton/api/inputs_and_outputs/#model-inputs-and-outputs","title":"Model Inputs and Outputs","text":""},{"location":"inference_deployment/triton/api/inputs_and_outputs/#model_navigator.triton.InputTensorSpec","title":"model_navigator.triton.InputTensorSpec  <code>dataclass</code>","text":"<pre><code>InputTensorSpec(\n    name,\n    shape,\n    dtype=None,\n    reshape=lambda: ()(),\n    is_shape_tensor=False,\n    optional=False,\n    format=None,\n    allow_ragged_batch=False,\n)\n</code></pre> <p>               Bases: <code>BaseTensorSpec</code></p> <p>Stores specification of single input tensor.</p> <p>This includes name, shape, dtype and more parameters available for input tensor in Triton Inference Server:</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>optional</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Flag marking the input is optional for the model execution</p> </li> <li> <code>format</code>               (<code>Optional[InputTensorFormat]</code>, default:                   <code>None</code> )           \u2013            <p>The format of the input.</p> </li> <li> <code>allow_ragged_batch</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Flag marking the input is allowed to be \"ragged\" in a dynamically created batch.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/inputs_and_outputs/#model_navigator.triton.InputTensorSpec.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.dtype:\n        self.dtype = cast_dtype(dtype=self.dtype)\n\n    expect_type(\"name\", self.name, str)\n    expect_type(\"shape\", self.shape, tuple)\n    expect_type(\"reshape\", self.shape, tuple, optional=True)\n    expect_type(\"dtype\", self.dtype, np.dtype, optional=True)\n    expect_type(\"is_shape_tensor\", self.is_shape_tensor, bool, optional=True)\n    is_shape_correct(\"shape\", self.shape)\n    is_shape_correct(\"reshape\", self.reshape, optional=True)\n</code></pre>"},{"location":"inference_deployment/triton/api/inputs_and_outputs/#model_navigator.triton.InputTensorFormat","title":"model_navigator.triton.InputTensorFormat","text":"<p>               Bases: <code>Enum</code></p> <p>Format for input tensor.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>FORMAT_NONE</code>           \u2013            <p>0</p> </li> <li> <code>FORMAT_NHWC</code>           \u2013            <p>1</p> </li> <li> <code>FORMAT_NCHW</code>           \u2013            <p>2</p> </li> </ul>"},{"location":"inference_deployment/triton/api/inputs_and_outputs/#model_navigator.triton.OutputTensorSpec","title":"model_navigator.triton.OutputTensorSpec  <code>dataclass</code>","text":"<pre><code>OutputTensorSpec(name, shape, dtype=None, reshape=lambda: ()(), is_shape_tensor=False, label_filename=None)\n</code></pre> <p>               Bases: <code>BaseTensorSpec</code></p> <p>Stores specification of single output tensor.</p> <p>This includes name, shape, dtype and more parameters available for output tensor in Triton Inference Server:</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>label_filename</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The label file associated with this output.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/inputs_and_outputs/#model_navigator.triton.OutputTensorSpec.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.dtype:\n        self.dtype = cast_dtype(dtype=self.dtype)\n\n    expect_type(\"name\", self.name, str)\n    expect_type(\"shape\", self.shape, tuple)\n    expect_type(\"reshape\", self.shape, tuple, optional=True)\n    expect_type(\"dtype\", self.dtype, np.dtype, optional=True)\n    expect_type(\"is_shape_tensor\", self.is_shape_tensor, bool, optional=True)\n    is_shape_correct(\"shape\", self.shape)\n    is_shape_correct(\"reshape\", self.reshape, optional=True)\n</code></pre>"},{"location":"inference_deployment/triton/api/instance_groups/","title":"Instance Group","text":""},{"location":"inference_deployment/triton/api/instance_groups/#model-instance-group","title":"Model Instance Group","text":""},{"location":"inference_deployment/triton/api/instance_groups/#model_navigator.triton.InstanceGroup","title":"model_navigator.triton.InstanceGroup  <code>dataclass</code>","text":"<pre><code>InstanceGroup(\n    kind=None, count=None, name=None, gpus=lambda: [](), passive=False, host_policy=None, profile=lambda: []()\n)\n</code></pre> <p>Configuration for model instance group.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>kind</code>               (<code>Optional[DeviceKind]</code>, default:                   <code>None</code> )           \u2013            <p>Kind of this instance group.</p> </li> <li> <code>count</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>For a group assigned to GPU, the number of instances created for    each GPU listed in 'gpus'. For a group assigned to CPU the number    of instances created.</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name of this group of instances.</p> </li> <li> <code>gpus</code>               (<code>List[int]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>GPU(s) where instances should be available.</p> </li> <li> <code>passive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether the instances within this instance group will be accepting      inference requests from the scheduler.</p> </li> <li> <code>host_policy</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The host policy name that the instance to be associated with.</p> </li> <li> <code>profile</code>               (<code>List[str]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>For TensorRT models containing multiple optimization profile, this      parameter specifies a set of optimization profiles available to this      instance group.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/instance_groups/#model_navigator.triton.InstanceGroup.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.count is not None and self.count &lt; 1:\n        raise ModelNavigatorWrongParameterError(\"The `count` must be greater or equal 1.\")\n\n    if self.kind not in [None, DeviceKind.KIND_GPU, DeviceKind.KIND_AUTO] and len(self.gpus) &gt; 0:\n        raise ModelNavigatorWrongParameterError(\n            f\"`gpus` cannot be set when device is not {DeviceKind.KIND_GPU} or {DeviceKind.KIND_AUTO}\"\n        )\n</code></pre>"},{"location":"inference_deployment/triton/api/instance_groups/#model_navigator.triton.DeviceKind","title":"model_navigator.triton.DeviceKind","text":"<p>               Bases: <code>Enum</code></p> <p>Device kind for model deployment.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>KIND_AUTO</code>           \u2013            <p>\"KIND_AUTO\"</p> </li> <li> <code>KIND_CPU</code>           \u2013            <p>\"KIND_CPU\"</p> </li> <li> <code>KIND_GPU</code>           \u2013            <p>\"KIND_GPU\"</p> </li> </ul>"},{"location":"inference_deployment/triton/api/sequence_batcher/","title":"Sequence Batcher","text":""},{"location":"inference_deployment/triton/api/sequence_batcher/#sequence-batcher","title":"Sequence Batcher","text":""},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcher","title":"model_navigator.triton.SequenceBatcher  <code>dataclass</code>","text":"<pre><code>SequenceBatcher(strategy=None, max_sequence_idle_microseconds=None, control_inputs=lambda: [](), states=lambda: []())\n</code></pre> <p>Sequence batching configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>strategy</code>               (<code>Optional[Union[SequenceBatcherStrategyDirect, SequenceBatcherStrategyOldest]]</code>, default:                   <code>None</code> )           \u2013            <p>The strategy used by the sequence batcher.</p> </li> <li> <code>max_sequence_idle_microseconds</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time, in microseconds, that a sequence is allowed to                             be idle before it is aborted.</p> </li> <li> <code>control_inputs</code>               (<code>List[SequenceBatcherControlInput]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>The model input(s) that the server should use to communicate             sequence start, stop, ready and similar control values to the model.</p> </li> <li> <code>states</code>               (<code>List[SequenceBatcherState]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>The optional state that can be stored in Triton for performing     inference requests on a sequence.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcher.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.strategy and (\n        not isinstance(self.strategy, SequenceBatcherStrategyDirect)\n        and not isinstance(self.strategy, SequenceBatcherStrategyOldest)\n    ):\n        raise ModelNavigatorWrongParameterError(\"Unsupported strategy type provided.\")\n</code></pre>"},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcherControl","title":"model_navigator.triton.SequenceBatcherControl  <code>dataclass</code>","text":"<pre><code>SequenceBatcherControl(\n    kind, dtype=None, int32_false_true=lambda: [](), fp32_false_true=lambda: [](), bool_false_true=lambda: []()\n)\n</code></pre> <p>Sequence Batching control configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>kind</code>               (<code>SequenceBatcherControlKind</code>)           \u2013            <p>The kind of this control.</p> </li> <li> <code>dtype</code>               (<code>Optional[Union[dtype, Type[dtype]]]</code>, default:                   <code>None</code> )           \u2013            <p>The control's datatype.</p> </li> <li> <code>int32_false_true</code>               (<code>List[int]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>The control's true and false setting is indicated by setting               a value in an int32 tensor.</p> </li> <li> <code>fp32_false_true</code>               (<code>List[float]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>The control's true and false setting is indicated by setting              a value in a fp32 tensor.</p> </li> <li> <code>bool_false_true</code>               (<code>List[bool]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>The control's true and false setting is indicated by setting              a value in a bool tensor.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcherControl.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.kind == SequenceBatcherControlKind.CONTROL_SEQUENCE_CORRID and self.dtype is None:\n        raise ModelNavigatorWrongParameterError(f\"The {self.kind} control type requires `dtype` to be specified.\")\n\n    if self.kind == SequenceBatcherControlKind.CONTROL_SEQUENCE_CORRID and any([\n        self.int32_false_true,\n        self.fp32_false_true,\n        self.bool_false_true,\n    ]):\n        raise ModelNavigatorWrongParameterError(\n            f\"The {self.kind} control type requires `dtype` to be specified only.\"\n        )\n\n    controls = [\n        SequenceBatcherControlKind.CONTROL_SEQUENCE_START,\n        SequenceBatcherControlKind.CONTROL_SEQUENCE_END,\n        SequenceBatcherControlKind.CONTROL_SEQUENCE_READY,\n    ]\n\n    if self.kind in controls and self.dtype:\n        raise ModelNavigatorWrongParameterError(f\"The {self.kind} control does not support `dtype` parameter.\")\n\n    if self.kind in controls and not (self.int32_false_true or self.fp32_false_true or self.bool_false_true):\n        raise ModelNavigatorWrongParameterError(\n            f\"The {self.kind} control type requires one of: \"\n            \"`int32_false_true`, `fp32_false_true`, `bool_false_true` to be specified.\"\n        )\n\n    if self.int32_false_true and len(self.int32_false_true) != 2:\n        raise ModelNavigatorWrongParameterError(\n            \"The `int32_false_true` field should be two element list with false and true values. Example: [0 , 1]\"\n        )\n\n    if self.fp32_false_true and len(self.fp32_false_true) != 2:\n        raise ModelNavigatorWrongParameterError(\n            \"The `fp32_false_true` field should be two element list with false and true values. Example: [0 , 1]\"\n        )\n\n    if self.bool_false_true and len(self.bool_false_true) != 2:\n        raise ModelNavigatorWrongParameterError(\n            \"The `bool_false_true` field should be two element list with false and true values. \"\n            \"Example: [False, True]\"\n        )\n\n    if self.dtype:\n        self.dtype = cast_dtype(dtype=self.dtype)\n\n    expect_type(\"dtype\", self.dtype, np.dtype, optional=True)\n</code></pre>"},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcherControlInput","title":"model_navigator.triton.SequenceBatcherControlInput  <code>dataclass</code>","text":"<pre><code>SequenceBatcherControlInput(input_name, controls)\n</code></pre> <p>Sequence Batching control input configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>input_name</code>               (<code>str</code>)           \u2013            <p>The name of the model input.</p> </li> <li> <code>controls</code>               (<code>List[SequenceBatcherControl]</code>)           \u2013            <p>List of  control value(s) that should be communicated to the       model using this model input.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcherControlKind","title":"model_navigator.triton.SequenceBatcherControlKind","text":"<p>               Bases: <code>Enum</code></p> <p>Sequence Batching control options.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>CONTROL_SEQUENCE_START</code>           \u2013            <p>\"CONTROL_SEQUENCE_START\"</p> </li> <li> <code>CONTROL_SEQUENCE_READY</code>           \u2013            <p>\"CONTROL_SEQUENCE_READY\"</p> </li> <li> <code>CONTROL_SEQUENCE_END</code>           \u2013            <p>\"CONTROL_SEQUENCE_END\"</p> </li> <li> <code>CONTROL_SEQUENCE_CORRID</code>           \u2013            <p>\"CONTROL_SEQUENCE_CORRID\"</p> </li> </ul>"},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcherInitialState","title":"model_navigator.triton.SequenceBatcherInitialState  <code>dataclass</code>","text":"<pre><code>SequenceBatcherInitialState(name, shape, dtype=None, zero_data=None, data_file=None)\n</code></pre> <p>Sequence Batching initial state configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            </li> <li> <code>shape</code>               (<code>Tuple[int, ...]</code>)           \u2013            <p>The shape of the state tensor, not including the batch dimension.</p> </li> <li> <code>dtype</code>               (<code>Optional[Union[dtype, Type[dtype]]]</code>, default:                   <code>None</code> )           \u2013            <p>The data-type of the state.</p> </li> <li> <code>zero_data</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>The identifier for using zeros as initial state data.</p> </li> <li> <code>data_file</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>The file whose content will be used as the initial data for        the state in row-major order.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcherInitialState.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if not self.zero_data and not self.data_file:\n        raise ModelNavigatorWrongParameterError(\"zero_data or data_file has to be defined. None was provided.\")\n\n    if self.zero_data and self.data_file:\n        raise ModelNavigatorWrongParameterError(\"zero_data or data_file has to be defined. Both were provided.\")\n\n    if self.dtype:\n        self.dtype = cast_dtype(dtype=self.dtype)\n\n    expect_type(\"name\", self.name, str)\n    expect_type(\"shape\", self.shape, tuple)\n    expect_type(\"dtype\", self.dtype, np.dtype, optional=True)\n    is_shape_correct(\"shape\", self.shape)\n</code></pre>"},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcherState","title":"model_navigator.triton.SequenceBatcherState  <code>dataclass</code>","text":"<pre><code>SequenceBatcherState(input_name, output_name, dtype, shape, initial_states=lambda: []())\n</code></pre> <p>Sequence Batching state configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>input_name</code>               (<code>str</code>)           \u2013            <p>The name of the model state input.</p> </li> <li> <code>output_name</code>               (<code>str</code>)           \u2013            <p>The name of the model state output.</p> </li> <li> <code>dtype</code>               (<code>Union[dtype, Type[dtype]]</code>)           \u2013            <p>The data-type of the state.</p> </li> <li> <code>shape</code>               (<code>Tuple[int, ...]</code>)           \u2013            <p>The shape of the state tensor.</p> </li> <li> <code>initial_states</code>               (<code>List[SequenceBatcherInitialState]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>The optional field to specify the list of initial states for the model.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcherState.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    self.dtype = cast_dtype(dtype=self.dtype)\n\n    expect_type(\"shape\", self.shape, tuple)\n    expect_type(\"dtype\", self.dtype, np.dtype, optional=True)\n    is_shape_correct(\"shape\", self.shape)\n</code></pre>"},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcherStrategyDirect","title":"model_navigator.triton.SequenceBatcherStrategyDirect  <code>dataclass</code>","text":"<pre><code>SequenceBatcherStrategyDirect(max_queue_delay_microseconds=0, minimum_slot_utilization=0.0)\n</code></pre> <p>Sequence Batching strategy direct configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>max_queue_delay_microseconds</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum time, in microseconds, a candidate request                           will be delayed in the sequence batch scheduling queue to                           wait for additional requests for batching.</p> </li> <li> <code>minimum_slot_utilization</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The minimum slot utilization that must be satisfied to                       execute the batch before 'max_queue_delay_microseconds' expires.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/sequence_batcher/#model_navigator.triton.SequenceBatcherStrategyOldest","title":"model_navigator.triton.SequenceBatcherStrategyOldest  <code>dataclass</code>","text":"<pre><code>SequenceBatcherStrategyOldest(\n    max_candidate_sequences, preferred_batch_size=lambda: [](), max_queue_delay_microseconds=0\n)\n</code></pre> <p>Sequence Batching strategy oldest configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>max_candidate_sequences</code>               (<code>int</code>)           \u2013            <p>Maximum number of candidate sequences that the batcher maintains.</p> </li> <li> <code>preferred_batch_size</code>               (<code>List[int]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>Preferred batch sizes for dynamic batching of candidate sequences.</p> </li> <li> <code>max_queue_delay_microseconds</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum time, in microseconds, a candidate request                           will be delayed in the dynamic batch scheduling queue to                           wait for additional requests for batching.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/","title":"Specialized Configs","text":""},{"location":"inference_deployment/triton/api/specialized_configs/#specialized-configs-for-triton-backends","title":"Specialized Configs for Triton Backends","text":"<p>The Python API provides specialized configuration classes that help provide only available options for the given type of model.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.BaseSpecializedModelConfig","title":"model_navigator.triton.BaseSpecializedModelConfig  <code>dataclass</code>","text":"<pre><code>BaseSpecializedModelConfig(\n    max_batch_size=4,\n    batching=True,\n    default_model_filename=None,\n    batcher=DynamicBatcher(),\n    instance_groups=lambda: [](),\n    parameters=lambda: {}(),\n    response_cache=False,\n    warmup=lambda: {}(),\n    inputs=lambda: [](),\n    outputs=lambda: [](),\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Common fields for specialized model configs.</p> <p>Read more in Triton Inference server documentation</p> <p>Parameters:</p> <ul> <li> <code>max_batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The maximal batch size that would be handled by model.</p> </li> <li> <code>batching</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag to enable/disable batching for model.</p> </li> <li> <code>default_model_filename</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional filename of the model file to use.</p> </li> <li> <code>batcher</code>               (<code>Union[DynamicBatcher, SequenceBatcher]</code>, default:                   <code>DynamicBatcher()</code> )           \u2013            <p>Configuration of Dynamic Batching for the model.</p> </li> <li> <code>instance_groups</code>               (<code>List[InstanceGroup]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>Instance groups configuration for multiple instances of the model</p> </li> <li> <code>parameters</code>               (<code>Dict[str, str]</code>, default:                   <code>lambda: {}()</code> )           \u2013            <p>Custom parameters for model or backend</p> </li> <li> <code>response_cache</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Flag to enable/disable response cache for the model</p> </li> <li> <code>warmup</code>               (<code>Dict[str, ModelWarmup]</code>, default:                   <code>lambda: {}()</code> )           \u2013            <p>Warmup configuration for model</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.BaseSpecializedModelConfig.backend","title":"backend  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>backend\n</code></pre> <p>Backend property that has to be overridden by specialized configs.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.BaseSpecializedModelConfig.custom_fields","title":"custom_fields  <code>property</code>","text":"<pre><code>custom_fields\n</code></pre> <p>Custom fields that are configured as parameters.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.BaseSpecializedModelConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/base_model_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.batching and self.max_batch_size &lt;= 0:\n        raise ModelNavigatorWrongParameterError(\"The `max_batch_size` must be greater or equal to 1.\")\n\n    if type(self.batcher) not in [DynamicBatcher, SequenceBatcher]:\n        raise ModelNavigatorWrongParameterError(\"Unsupported batcher type provided.\")\n\n    if self.backend != Backend.TensorRT and any(group.profile for group in self.instance_groups):\n        raise ModelNavigatorWrongParameterError(\n            \"Invalid `profile` option. The value can be set only for `backend=Backend.TensorRT`\"\n        )\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.ONNXModelConfig","title":"model_navigator.triton.ONNXModelConfig  <code>dataclass</code>","text":"<pre><code>ONNXModelConfig(\n    max_batch_size=4,\n    batching=True,\n    default_model_filename=None,\n    batcher=DynamicBatcher(),\n    instance_groups=lambda: [](),\n    parameters=lambda: {}(),\n    response_cache=False,\n    warmup=lambda: {}(),\n    inputs=lambda: [](),\n    outputs=lambda: [](),\n    platform=None,\n    optimization=None,\n)\n</code></pre> <p>               Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for ONNX backend supported model.</p> <p>Parameters:</p> <ul> <li> <code>platform</code>               (<code>Optional[Platform]</code>, default:                   <code>None</code> )           \u2013            <p>Override backend parameter with platform.       Possible options: Platform.ONNXRuntimeONNX</p> </li> <li> <code>optimization</code>               (<code>Optional[ONNXOptimization]</code>, default:                   <code>None</code> )           \u2013            <p>Possible optimization for ONNX models</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.ONNXModelConfig.backend","title":"backend  <code>property</code>","text":"<pre><code>backend\n</code></pre> <p>Define backend value for config.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.ONNXModelConfig.custom_fields","title":"custom_fields  <code>property</code>","text":"<pre><code>custom_fields\n</code></pre> <p>Custom fields that are configured as parameters.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.ONNXModelConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/onnx_model_config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    super().__post_init__()\n    if self.optimization and not isinstance(self.optimization, ONNXOptimization):\n        raise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\n\n    if self.platform and self.platform != Platform.ONNXRuntimeONNX:\n        raise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use: {Platform.ONNXRuntimeONNX}.\")\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.ONNXOptimization","title":"model_navigator.triton.ONNXOptimization  <code>dataclass</code>","text":"<pre><code>ONNXOptimization(accelerator)\n</code></pre> <p>ONNX possible optimizations.</p> <p>Parameters:</p> <ul> <li> <code>accelerator</code>               (<code>Union[OpenVINOAccelerator, TensorRTAccelerator]</code>)           \u2013            <p>Execution accelerator for model</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.ONNXOptimization.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/onnx_model_config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.accelerator and type(self.accelerator) not in [OpenVINOAccelerator, TensorRTAccelerator]:\n        raise ModelNavigatorWrongParameterError(\"Unsupported accelerator type provided.\")\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.PythonModelConfig","title":"model_navigator.triton.PythonModelConfig  <code>dataclass</code>","text":"<pre><code>PythonModelConfig(\n    max_batch_size=4,\n    batching=True,\n    default_model_filename=None,\n    batcher=DynamicBatcher(),\n    instance_groups=lambda: [](),\n    parameters=lambda: {}(),\n    response_cache=False,\n    warmup=lambda: {}(),\n    inputs=lambda: [](),\n    outputs=lambda: [](),\n)\n</code></pre> <p>               Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for Python backend supported model.</p> <p>Parameters:</p> <ul> <li> <code>inputs</code>               (<code>Sequence[InputTensorSpec]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>Required definition of model inputs</p> </li> <li> <code>outputs</code>               (<code>Sequence[OutputTensorSpec]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>Required definition of model outputs</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.PythonModelConfig.backend","title":"backend  <code>property</code>","text":"<pre><code>backend\n</code></pre> <p>Define backend value for config.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.PythonModelConfig.custom_fields","title":"custom_fields  <code>property</code>","text":"<pre><code>custom_fields\n</code></pre> <p>Custom fields that are configured as parameters.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.PythonModelConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/python_model_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    super().__post_init__()\n    assert len(self.inputs) &gt; 0, \"Model inputs definition is required for Python backend.\"\n    assert len(self.outputs) &gt; 0, \"Model outputs definition is required for Python backend.\"\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.PyTorchModelConfig","title":"model_navigator.triton.PyTorchModelConfig  <code>dataclass</code>","text":"<pre><code>PyTorchModelConfig(\n    max_batch_size=4,\n    batching=True,\n    default_model_filename=None,\n    batcher=DynamicBatcher(),\n    instance_groups=lambda: [](),\n    parameters=lambda: {}(),\n    response_cache=False,\n    warmup=lambda: {}(),\n    inputs=lambda: [](),\n    outputs=lambda: [](),\n    platform=None,\n)\n</code></pre> <p>               Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for PyTorch backend supported model.</p> <p>Parameters:</p> <ul> <li> <code>platform</code>               (<code>Optional[Platform]</code>, default:                   <code>None</code> )           \u2013            <p>Override backend parameter with platform.       Possible options: Platform.PyTorchLibtorch</p> </li> <li> <code>inputs</code>               (<code>Sequence[InputTensorSpec]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>Required definition of model inputs</p> </li> <li> <code>outputs</code>               (<code>Sequence[OutputTensorSpec]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>Required definition of model outputs</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.PyTorchModelConfig.backend","title":"backend  <code>property</code>","text":"<pre><code>backend\n</code></pre> <p>Define backend value for config.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.PyTorchModelConfig.custom_fields","title":"custom_fields  <code>property</code>","text":"<pre><code>custom_fields\n</code></pre> <p>Custom fields that are configured as parameters.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.PyTorchModelConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/pytorch_model_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    super().__post_init__()\n    assert len(self.inputs) &gt; 0, \"Model inputs definition is required for PyTorch backend.\"\n    assert len(self.outputs) &gt; 0, \"Model outputs definition is required for PyTorch backend.\"\n\n    if self.platform and self.platform != Platform.PyTorchLibtorch:\n        raise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use: {Platform.PyTorchLibtorch}.\")\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorFlowModelConfig","title":"model_navigator.triton.TensorFlowModelConfig  <code>dataclass</code>","text":"<pre><code>TensorFlowModelConfig(\n    max_batch_size=4,\n    batching=True,\n    default_model_filename=None,\n    batcher=DynamicBatcher(),\n    instance_groups=lambda: [](),\n    parameters=lambda: {}(),\n    response_cache=False,\n    warmup=lambda: {}(),\n    inputs=lambda: [](),\n    outputs=lambda: [](),\n    platform=None,\n    optimization=None,\n)\n</code></pre> <p>               Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for TensorFlow backend supported model.</p> <p>Parameters:</p> <ul> <li> <code>platform</code>               (<code>Optional[Platform]</code>, default:                   <code>None</code> )           \u2013            <p>Override backend parameter with platform.       Possible options: Platform.TensorFlowSavedModel, Platform.TensorFlowGraphDef</p> </li> <li> <code>optimization</code>               (<code>Optional[TensorFlowOptimization]</code>, default:                   <code>None</code> )           \u2013            <p>Possible optimization for TensorFlow models</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorFlowModelConfig.backend","title":"backend  <code>property</code>","text":"<pre><code>backend\n</code></pre> <p>Define backend value for config.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorFlowModelConfig.custom_fields","title":"custom_fields  <code>property</code>","text":"<pre><code>custom_fields\n</code></pre> <p>Custom fields that are configured as parameters.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorFlowModelConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorflow_model_config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    super().__post_init__()\n    if self.optimization and not isinstance(self.optimization, TensorFlowOptimization):\n        raise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\n\n    platforms = [Platform.TensorFlowSavedModel, Platform.TensorFlowGraphDef]\n    if self.platform and self.platform not in platforms:\n        raise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use one of: {platforms}\")\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorFlowOptimization","title":"model_navigator.triton.TensorFlowOptimization  <code>dataclass</code>","text":"<pre><code>TensorFlowOptimization(accelerator)\n</code></pre> <p>TensorFlow possible optimizations.</p> <p>Parameters:</p> <ul> <li> <code>accelerator</code>               (<code>Union[AutoMixedPrecisionAccelerator, GPUIOAccelerator, TensorRTAccelerator]</code>)           \u2013            <p>Execution accelerator for model</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorFlowOptimization.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorflow_model_config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.accelerator and type(self.accelerator) not in [\n        AutoMixedPrecisionAccelerator,\n        GPUIOAccelerator,\n        TensorRTAccelerator,\n    ]:\n        raise ModelNavigatorWrongParameterError(\"Unsupported accelerator type provided.\")\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorRTModelConfig","title":"model_navigator.triton.TensorRTModelConfig  <code>dataclass</code>","text":"<pre><code>TensorRTModelConfig(\n    max_batch_size=4,\n    batching=True,\n    default_model_filename=None,\n    batcher=DynamicBatcher(),\n    instance_groups=lambda: [](),\n    parameters=lambda: {}(),\n    response_cache=False,\n    warmup=lambda: {}(),\n    inputs=lambda: [](),\n    outputs=lambda: [](),\n    platform=None,\n    optimization=None,\n)\n</code></pre> <p>               Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for TensorRT platform supported model.</p> <p>Parameters:</p> <ul> <li> <code>platform</code>               (<code>Optional[Platform]</code>, default:                   <code>None</code> )           \u2013            <p>Override backend parameter with platform.       Possible options: Platform.TensorRTPlan</p> </li> <li> <code>optimization</code>               (<code>Optional[TensorRTOptimization]</code>, default:                   <code>None</code> )           \u2013            <p>Possible optimization for TensorRT models</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorRTModelConfig.backend","title":"backend  <code>property</code>","text":"<pre><code>backend\n</code></pre> <p>Define backend value for config.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorRTModelConfig.custom_fields","title":"custom_fields  <code>property</code>","text":"<pre><code>custom_fields\n</code></pre> <p>Custom fields that are configured as parameters.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorRTModelConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorrt_model_config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    super().__post_init__()\n    if self.optimization and not isinstance(self.optimization, TensorRTOptimization):\n        raise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\n\n    if self.platform and self.platform != Platform.TensorRTPlan:\n        raise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use: {Platform.TensorRTPlan}.\")\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorRTOptimization","title":"model_navigator.triton.TensorRTOptimization  <code>dataclass</code>","text":"<pre><code>TensorRTOptimization(cuda_graphs=False, gather_kernel_buffer_threshold=None, eager_batching=False)\n</code></pre> <p>TensorRT possible optimizations.</p> <p>Parameters:</p> <ul> <li> <code>cuda_graphs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Use CUDA graphs API to capture model operations and execute them more efficiently.</p> </li> <li> <code>gather_kernel_buffer_threshold</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The backend may use a gather kernel to gather input data if the                             device has direct access to the source buffer and the destination                             buffer.</p> </li> <li> <code>eager_batching</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Start preparing the next batch before the model instance is ready for the next inference.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorRTOptimization.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorrt_model_config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if not self.cuda_graphs and not self.gather_kernel_buffer_threshold and not self.eager_batching:\n        raise ModelNavigatorWrongParameterError(\"At least one of the optimization options should be enabled.\")\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorRTLLMModelConfig","title":"model_navigator.triton.TensorRTLLMModelConfig  <code>dataclass</code>","text":"<pre><code>TensorRTLLMModelConfig(\n    max_batch_size=4,\n    batching=True,\n    default_model_filename=None,\n    batcher=DynamicBatcher(),\n    instance_groups=lambda: [](),\n    parameters=lambda: {}(),\n    response_cache=False,\n    warmup=lambda: {}(),\n    inputs=lambda: [](),\n    outputs=lambda: [](),\n    encoder_dir=None,\n    max_beam_width=None,\n    batching_strategy=INFLIGHT,\n    batch_scheduler_policy=MAX_UTILIZATION,\n    decoding_mode=None,\n    gpu_device_ids=lambda: [](),\n    gpu_weights_percent=None,\n    kv_cache_config=None,\n    peft_cache_config=None,\n    enable_chunked_context=None,\n    normalize_log_probs=None,\n    cancellation_check_period_ms=None,\n    stats_check_period_ms=None,\n    request_stats_max_iterations=None,\n    iter_stats_max_iterations=None,\n    exclude_input_in_output=None,\n    medusa_choices=None,\n    _engine_dir=None,\n)\n</code></pre> <p>               Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for TensorRT-LLM platform supported model.</p> <p>Adapted from TensorRT-LLM config: https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/all_models/inflight_batcher_llm/tensorrt_llm/config.pbtxts</p> <p>Relevant TensorRT-LLM classes: - ExecutorConfig: https://nvidia.github.io/TensorRT-LLM/_cpp_gen/executor.html#_CPPv4N12tensorrt_llm8executor14ExecutorConfigE - KVCacheConfig: https://nvidia.github.io/TensorRT-LLM/_cpp_gen/executor.html#_CPPv4N12tensorrt_llm8executor13KvCacheConfigE - PeftCacheConfig: https://nvidia.github.io/TensorRT-LLM/_cpp_gen/executor.html#_CPPv4N12tensorrt_llm8executor15PeftCacheConfigE</p> <p>Parameters:</p> <ul> <li> <code>engine_dir</code>           \u2013            <p>Path to the TensorRT engine directory.</p> </li> <li> <code>encoder_dir</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Path to the encoder model directory.</p> </li> <li> <code>max_beam_width</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Maximal size of each beam in beam search.</p> </li> <li> <code>batching_strategy</code>               (<code>BatchingStrategy</code>, default:                   <code>INFLIGHT</code> )           \u2013            <p>Batching strategy for model.</p> </li> <li> <code>batch_scheduler_policy</code>               (<code>BatchSchedulerPolicy</code>, default:                   <code>MAX_UTILIZATION</code> )           \u2013            <p>Batching scheduler policy for model.</p> </li> <li> <code>decoding_mode</code>               (<code>Optional[DecodingMode]</code>, default:                   <code>None</code> )           \u2013            <p>Decoding mode for model.</p> </li> <li> <code>gpu_device_ids</code>               (<code>List[int]</code>, default:                   <code>lambda: []()</code> )           \u2013            <p>List of GPU devices on which model is running.</p> </li> <li> <code>gpu_weights_percent</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The percentage of GPU memory fraction that should be allocated for weights.</p> </li> <li> <code>kv_cache_config</code>               (<code>Optional[KVCacheConfig]</code>, default:                   <code>None</code> )           \u2013            <p>KV cache config for model.</p> </li> <li> <code>peft_cache_config</code>               (<code>Optional[PeftCacheConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Peft cache config for model.</p> </li> <li> <code>enable_chunked_context</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Enable chunked context for model</p> </li> <li> <code>normalize_log_probs</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Controls if log probabilities should be normalized or not.</p> </li> <li> <code>cancellation_check_period_ms</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The request cancellation period check in ms.</p> </li> <li> <code>stats_check_period_ms</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The statistics checking period in ms.</p> </li> <li> <code>request_stats_max_iterations</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Controls the maximum number of iterations for which to keep per-request statistics.</p> </li> <li> <code>iter_stats_max_iterations</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Controls the maximum number of iterations for which to keep statistics.</p> </li> <li> <code>exclude_input_in_output</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Controls if output tokens in Result should include the input tokens. Default is false.</p> </li> <li> <code>medusa_choices</code>               (<code>Optional[Union[List[int], List[List[int]], List[Tuple[int]]]]</code>, default:                   <code>None</code> )           \u2013            <p>Medusa choices as in https://github.com/FasterDecoding/Medusa/blob/main/medusa/model/medusa_choices.py</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorRTLLMModelConfig.backend","title":"backend  <code>property</code>","text":"<pre><code>backend\n</code></pre> <p>Define backend value for config.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorRTLLMModelConfig.custom_fields","title":"custom_fields  <code>property</code>","text":"<pre><code>custom_fields\n</code></pre> <p>Custom fields that are configured as parameters.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorRTLLMModelConfig.engine_dir","title":"engine_dir  <code>property</code> <code>writable</code>","text":"<pre><code>engine_dir\n</code></pre> <p>Engined directory path.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.TensorRTLLMModelConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorrt_llm_model_config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    super().__post_init__()\n    self._validate_config()\n    self._initialize_instance_groups()\n    self._initialize_inputs()\n    self._initialize_outputs()\n    self._initialize_parameters()\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.BatchingStrategy","title":"model_navigator.triton.BatchingStrategy","text":"<p>               Bases: <code>Enum</code></p> <p>Define the supported batch strategies.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.BatchSchedulerPolicy","title":"model_navigator.triton.BatchSchedulerPolicy","text":"<p>               Bases: <code>Enum</code></p> <p>Define the supported batch scheduler policies.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.DecodingMode","title":"model_navigator.triton.DecodingMode","text":"<p>               Bases: <code>Enum</code></p> <p>Define the supported decoding modes.</p>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.KVCacheConfig","title":"model_navigator.triton.KVCacheConfig  <code>dataclass</code>","text":"<pre><code>KVCacheConfig(\n    enable_block_reuse=None,\n    max_tokens=None,\n    sink_token_length=None,\n    max_attention_window=None,\n    free_gpu_memory_fraction=None,\n    host_cache_size=None,\n    onboard_blocks=None,\n)\n</code></pre> <p>Configuration of KV cache in TRT-LLM.</p> <p>More: https://nvidia.github.io/TensorRT-LLM/_cpp_gen/executor.html#_CPPv4N12tensorrt_llm8executor13KvCacheConfigE</p> <p>Parameters:</p> <ul> <li> <code>enable_block_reuse</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Controls if KV cache blocks can be reused for different requests.</p> </li> <li> <code>max_tokens</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of tokens that should be stored in the KV cache If both max_tokens and         free_gpu_memory_fraction are specified, memory corresponding to the minimum will be allocated.</p> </li> <li> <code>sink_token_length</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of sink tokens (tokens to always keep in attention window)</p> </li> <li> <code>max_attention_window</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Size of the attention window for each sequence. Only the last max_attention_window tokens                   of each sequence will be stored in the KV cache.</p> </li> <li> <code>free_gpu_memory_fraction</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The fraction of GPU memory fraction that should be allocated for the KV cache.                       Default is 90%. If both max_tokens and free_gpu_memory_fraction are specified,                       memory corresponding to the minimum will be allocated.</p> </li> <li> <code>host_cache_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Size of secondary memory pool in bytes. Default is 0. Having a secondary memory pool increases              KV cache block reuse potential.</p> </li> <li> <code>onboard_blocks</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Controls whether offloaded blocks should be onboarded back into primary memory before             being reused.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.KVCacheConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorrt_llm_model_config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.max_tokens is not None and self.max_tokens &lt;= 0:\n        raise ModelNavigatorWrongParameterError(\"`max_tokens` must be greater than 0.\")\n\n    if self.sink_token_length is not None and self.sink_token_length &lt;= 0:\n        raise ModelNavigatorWrongParameterError(\"`sink_token_length` must be greater than 0.\")\n\n    if self.max_attention_window is not None and self.max_attention_window &lt;= 0:\n        raise ModelNavigatorWrongParameterError(\"`max_attention_window` must be greater than 0.\")\n\n    if self.free_gpu_memory_fraction is not None and (\n        self.free_gpu_memory_fraction &lt; 0.0 or self.free_gpu_memory_fraction &gt; 1.0\n    ):\n        raise ModelNavigatorWrongParameterError(\"`free_gpu_memory_fraction` must be between 0.0 and 1.0.\")\n\n    if self.host_cache_size is not None and self.host_cache_size &lt;= 0:\n        raise ModelNavigatorWrongParameterError(\"`host_cache_size` must be greater than 0.\")\n\n    if self.onboard_blocks is not None and self.onboard_blocks &lt;= 0:\n        raise ModelNavigatorWrongParameterError(\"`onboard_blocks` must be greater than 0.\")\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.KVCacheConfig.as_parameters","title":"as_parameters","text":"<pre><code>as_parameters()\n</code></pre> <p>Convert dataclass to configuration flags passed to backend as parameters.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorrt_llm_model_config.py</code> <pre><code>def as_parameters(self):\n    \"\"\"Convert dataclass to configuration flags passed to backend as parameters.\"\"\"\n    data = {}\n    for k, v in self.__dict__.items():\n        if v is None:\n            continue\n\n        mapped_key = self._MAPPING.get(k, k)\n        data[mapped_key] = v\n\n    return data\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.PeftCacheConfig","title":"model_navigator.triton.PeftCacheConfig  <code>dataclass</code>","text":"<pre><code>PeftCacheConfig(optimal_adapter_size=None, max_adapter_size=None, gpu_memory_fraction=None, host_memory_bytes=None)\n</code></pre> <p>Configuration of Peft Cache.</p> <p>More: https://nvidia.github.io/TensorRT-LLM/_cpp_gen/executor.html#_CPPv4N12tensorrt_llm8executor15PeftCacheConfigE</p> <p>Parameters:</p> <ul> <li> <code>optimal_adapter_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            </li> <li> <code>max_adapter_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            </li> <li> <code>gpu_memory_fraction</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            </li> <li> <code>host_memory_bytes</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            </li> </ul>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.PeftCacheConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorrt_llm_model_config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.optimal_adapter_size is not None and self.optimal_adapter_size &lt;= 0:\n        raise ModelNavigatorWrongParameterError(\"`optimal_adapter_size` must be greater than 0.\")\n\n    if self.max_adapter_size is not None and self.max_adapter_size &lt;= 0:\n        raise ModelNavigatorWrongParameterError(\"`max_adapter_size` must be greater than 0.\")\n\n    if self.max_adapter_size and self.optimal_adapter_size and self.max_adapter_size &lt; self.optimal_adapter_size:\n        raise ModelNavigatorWrongParameterError(\n            \"`max_adapter_size` must be greater than or equal to `optimal_adapter_size`.\"\n        )\n\n    if self.gpu_memory_fraction is not None and (self.gpu_memory_fraction &lt; 0.0 or self.gpu_memory_fraction &gt; 1.0):\n        raise ModelNavigatorWrongParameterError(\"`gpu_memory_fraction` must be between 0.0 and 1.0.\")\n\n    if self.host_memory_bytes is not None and self.host_memory_bytes &lt;= 0:\n        raise ModelNavigatorWrongParameterError(\"`host_memory_bytes` must be greater than 0.\")\n</code></pre>"},{"location":"inference_deployment/triton/api/specialized_configs/#model_navigator.triton.PeftCacheConfig.as_parameters","title":"as_parameters","text":"<pre><code>as_parameters()\n</code></pre> <p>Convert dataclass to configuration flags passed to backend as parameters.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorrt_llm_model_config.py</code> <pre><code>def as_parameters(self):\n    \"\"\"Convert dataclass to configuration flags passed to backend as parameters.\"\"\"\n    data = {}\n    for k, v in self.__dict__.items():\n        if v is None:\n            continue\n\n        data[f\"lora_cache_{k}\"] = v\n\n    return data\n</code></pre>"},{"location":"inference_deployment/triton/api/warmup/","title":"Model Warmup","text":""},{"location":"inference_deployment/triton/api/warmup/#model-warmup","title":"Model Warmup","text":""},{"location":"inference_deployment/triton/api/warmup/#model_navigator.triton.ModelWarmup","title":"model_navigator.triton.ModelWarmup  <code>dataclass</code>","text":"<pre><code>ModelWarmup(inputs, batch_size=1, iterations=0)\n</code></pre> <p>Model warmup configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The batch size of the inference request. This must be &gt;= 1. For models that don't support batching,         batch_size must be 1.</p> </li> <li> <code>inputs</code>               (<code>Dict[str, ModelWarmupInput]</code>)           \u2013            <p>The warmup meta data associated with every model input, including control tensors.</p> </li> <li> <code>iterations</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The number of iterations that this warmup sample will be executed. For example, if this field is         set to 2, 2 model executions using this sample will be scheduled for warmup. Default value is 0 which         indicates that this sample will be used only once.</p> </li> </ul>"},{"location":"inference_deployment/triton/api/warmup/#model_navigator.triton.ModelWarmup.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.batch_size &lt; 1:\n        raise ModelNavigatorWrongParameterError(\"`batch_size` must be greater or equal 1.\")\n\n    if self.iterations &lt; 0:\n        raise ModelNavigatorWrongParameterError(\"`iterations` must be greater or equal 0.\")\n</code></pre>"},{"location":"inference_deployment/triton/api/warmup/#model_navigator.triton.ModelWarmupInput","title":"model_navigator.triton.ModelWarmupInput  <code>dataclass</code>","text":"<pre><code>ModelWarmupInput(shape, dtype, input_data_type, input_data_file=None)\n</code></pre> <p>Model warmup input configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>shape</code>               (<code>Tuple[int, ...]</code>)           \u2013            <p>Shape of the model input/output</p> </li> <li> <code>dtype</code>               (<code>Optional[Union[dtype, Type[dtype]]]</code>)           \u2013            <p>Data type</p> </li> <li> <code>input_data_type</code>               (<code>ModelWarmupInputDataType</code>)           \u2013            <p>Type of input data used for warmup</p> </li> <li> <code>input_data_file</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Path to file with input data. Provide the path where the file is located.              Required only when input_data_type is <code>ModelWarmupInputDataType.DATA_FILE</code></p> </li> </ul>"},{"location":"inference_deployment/triton/api/warmup/#model_navigator.triton.ModelWarmupInput.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate the configuration for early error handling.\"\"\"\n    if self.input_data_type == ModelWarmupInputDataType.FILE and self.input_data_file is None:\n        raise ModelNavigatorWrongParameterError(\"`input_data_file` is required. Set the file path.\")\n\n    if self.input_data_type != ModelWarmupInputDataType.FILE and self.input_data_file is not None:\n        raise ModelNavigatorWrongParameterError(\"`input_data_file` is not required. Remove the parameter.\")\n</code></pre>"},{"location":"inference_deployment/triton/api/warmup/#model_navigator.triton.ModelWarmupInputDataType","title":"model_navigator.triton.ModelWarmupInputDataType","text":"<p>               Bases: <code>Enum</code></p> <p>Model warmup input data type.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> <code>ZERO</code>           \u2013            <p>\"ZERO\"</p> </li> <li> <code>RANDOM</code>           \u2013            <p>\"RANDOM\"</p> </li> <li> <code>FILE</code>           \u2013            <p>\"FILE\"</p> </li> </ul>"},{"location":"models_optimize/optimize/optimize/","title":"Optimize Models","text":""},{"location":"models_optimize/optimize/optimize/#optimize-models","title":"Optimize Models","text":"<p>Model optimization plays a crucial role in unlocking the maximum performance capabilities of the underlying hardware. These sections describe in details how the Triton Model Navigator performs the optimization to improve the inference performance and reduce the cost.</p>"},{"location":"models_optimize/optimize/optimize/#overview-of-model-optimize","title":"Overview of Model Optimize","text":"<p>The Triton Model Navigator optimize process encompasses several crucial steps aimed at improving the performance of deep learning models and converting them into the most optimal formats. The Triton Model Navigator supports various frameworks, including TensorFlow 2, PyTorch, ONNX, and JAX.</p> <p>To initiate the multistep conversion and optimization process in the <code>Triton Model Navigator</code>, users only need to provide the <code>model</code> and <code>dataloader</code>. However, for further customization, additional parameters and <code>custom_configs</code> can be used to tailor the optimization process to specific requirements.</p> <p>The optimization process consists of the following steps:</p> <ol> <li> <p>Model export: The source deep learning model, created using one of the supported frameworks, is exported to one of    the intermediaries formats: TorchScript, SavedModel, ONNX.</p> </li> <li> <p>Model conversion: The exported model is then converted into a target representation with the goal of achieving the best    possible performance. It includes: TorchTensorRT, TensorFlowTensorRT, ONNX, and TensorRT.</p> </li> <li> <p>Correctness test: To ensure the correctness of the produced models, the Triton Model Navigator performs a series of    correctness    tests. These tests calculate absolute and relative tolerance values for source and converted models.</p> </li> <li> <p>Model profiling: the Triton Model Navigator conducts performance profiling of the converted models. This process    uses <code>Navigator Runners</code> to perform inference and measure its time. The profiler aims to find the maximum throughput    for each model and calculates its latency. This information can then be used to retrieve the best runners and provide    you with performance details for the optimal configuration. In that stage, a single data sample is used to perform    profiling.</p> </li> <li> <p>Verification: Once the profiling is complete, the Triton Model Navigator performs verification tests to validate the metrics    provided by the user in <code>verify_func</code> against all converted models.</p> </li> </ol>"},{"location":"models_optimize/optimize/optimize/#optimize-resnet50-from-torch-hub","title":"Optimize ResNet50 from Torch Hub","text":"<p>The Triton Model Navigator provides a simple path to optimize a model implemented in PyTorch, TensorFlow or ONNX to NVIDIA TensorRT. You can work with your own model implementation or use solutions provided by public hubs like TorchHub or HuggingFace.</p> <p>Here is an example of running <code>optimize</code> on the TorchHub ResNet50 model:</p> <pre><code>import torch\nimport model_navigator as nav\n\npackage = nav.torch.optimize(\n    model=torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True).eval(),\n    dataloader=[torch.randn(1, 3, 256, 256) for _ in range(10)],\n)\n</code></pre> <p>Once the model has been optimized, the created artifacts are stored in a dedicated workspace and a Package object is returned from the function.</p>"},{"location":"models_optimize/optimize/optimize/#optimize-the-qat-model","title":"Optimize the QAT model","text":"<p>By going through the Optimize process with the Triton Model Navigator, deep learning models can be optimized and converted into the most suitable formats for deployment, with NVIDIA TensorRT often providing the optimal solution to achieve the best performance.</p> <p>NVIDIA TensorRT can be used for applications deployed to the data center, as well as embedded and automotive environments. It powers key NVIDIA solutions such as NVIDIA TAO, NVIDIA DRIVE\u2122, NVIDIA Clara\u2122, and NVIDIA Jetpack\u2122. TensorRT is also integrated with application-specific SDKs, such as NVIDIA DeepStream, NVIDIA Riva, NVIDIA Merlin\u2122, NVIDIA Maxine\u2122, NVIDIA Morpheus, and NVIDIA Broadcast Engine to provide developers with a unified path to deploy intelligent video analytics, speech AI, recommender systems, video conference, AI-based cybersecurity, and streaming apps in production.</p> <p>You can use those default TensorRT compute plans for your deployment to get excellent performance for NVIDIA hardware.</p> <p>You can also apply quantization to some selected models to get better performance, like in the HiFiGAN example. This model uses quantization-aware training, so accuracy is perfect, but many other models can use post-training quantization by just enabling the INT8 flag in the optimize function. It can reduce accuracy, so you must validate the quantized model in such cases.</p> <p>The Triton Model Navigator can build your quantized model, when the flag <code>INT8</code> is used:</p> <pre><code>package = nav.torch.optimize(\n    model=model,\n    dataloader=dataloader,\n    custom_configs=[\n        nav.TensorRTConfig(precision=nav.TensorRTPrecision.INT8),\n    ],\n)\n</code></pre> <p>At the end, the summary of the execution is presented, and artifacts are stored in the Navigator workspace, which by default is in the <code>navigator_workspace</code> folder.</p>"},{"location":"models_optimize/optimize/api/config/","title":"Config","text":"<p>Classes, enums and types used to configure the Triton Model Navigator.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration","title":"model_navigator.configuration","text":"<p>Definition of enums and classes representing configuration for Model Navigator.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.AutocastType","title":"AutocastType","text":"<p>               Bases: <code>Enum</code></p> <p>Torch runner autocast options.</p> <p>Parameters:</p> <ul> <li> <code>DEVICE</code>           \u2013            <p>Use device default dtype.</p> </li> <li> <code>FP16</code>               (<code>str</code>)           \u2013            <p>Use float16 autocast during runtime.</p> </li> <li> <code>BF16</code>               (<code>str</code>)           \u2013            <p>Use bfloat16 autocast during runtime.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfig","title":"CustomConfig","text":"<p>               Bases: <code>ABC</code></p> <p>Base class used for custom configs. Input for Model Navigator <code>optimize</code> method.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def defaults(self) -&gt; None:\n    \"\"\"Update parameters to defaults.\"\"\"\n    return None\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n    \"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\n    return cls(**config_dict)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfig.name","title":"name  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the CustomConfig.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\n@abc.abstractmethod\ndef name(cls) -&gt; str:\n    \"\"\"Name of the CustomConfig.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForFormat","title":"CustomConfigForFormat  <code>dataclass</code>","text":"<pre><code>CustomConfigForFormat(custom_args=dict(), device=None)\n</code></pre> <p>               Bases: <code>DataObject</code>, <code>CustomConfig</code></p> <p>Abstract base class used for custom configs representing particular format.</p> <p>Parameters:</p> <ul> <li> <code>custom_args</code>               (<code>Dict[str, Any]</code>, default:                   <code>dict()</code> )           \u2013            <p>Custom arguments passed to conversion function.</p> </li> <li> <code>device</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>torch-like string used for selecting device e.q. \"cuda:0\".</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForFormat.format","title":"format  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>format\n</code></pre> <p>Format represented by CustomConfig.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForFormat.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def defaults(self) -&gt; None:\n    \"\"\"Update parameters to defaults.\"\"\"\n    return None\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForFormat.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForFormat.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n    \"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\n    return cls(**config_dict)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForFormat.name","title":"name  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the CustomConfig.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\n@abc.abstractmethod\ndef name(cls) -&gt; str:\n    \"\"\"Name of the CustomConfig.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForFormat.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForFormat.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForFormat.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForTensorRT","title":"CustomConfigForTensorRT  <code>dataclass</code>","text":"<pre><code>CustomConfigForTensorRT(\n    custom_args=dict(),\n    device=None,\n    trt_profiles=None,\n    trt_profile=None,\n    precision=DEFAULT_TENSORRT_PRECISION,\n    precision_mode=DEFAULT_TENSORRT_PRECISION_MODE,\n    max_workspace_size=DEFAULT_MAX_WORKSPACE_SIZE,\n    conversion_fallback=False,\n)\n</code></pre> <p>               Bases: <code>CustomConfigForFormat</code></p> <p>Abstract base class used for custom configs representing particular TensorRT format.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForTensorRT.format","title":"format  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>format\n</code></pre> <p>Format represented by CustomConfig.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForTensorRT.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize common TensorRT parameters and validate configuration.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize common TensorRT parameters and validate configuration.\"\"\"\n    precision = (self.precision,) if not isinstance(self.precision, (list, tuple)) else self.precision\n    self.precision = tuple(TensorRTPrecision(p) for p in precision)\n    self.precision_mode = TensorRTPrecisionMode(self.precision_mode)\n\n    # TODO: Remove before 1.0.0 release\n    if self.trt_profile is not None and self.trt_profiles is not None:\n        raise ModelNavigatorConfigurationError(\"Only one of trt_profile and trt_profiles can be set.\")\n    elif self.trt_profile:\n        warnings.warn(\n            \"trt_profile will be deprecated in future releases. Use trt_profiles instead.\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        self.trt_profiles = [self.trt_profile]\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForTensorRT.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def defaults(self) -&gt; None:\n    \"\"\"Update parameters to defaults.\"\"\"\n    self.precision = tuple(TensorRTPrecision(p) for p in DEFAULT_TENSORRT_PRECISION)\n    self.precision_mode = TensorRTPrecisionMode(DEFAULT_TENSORRT_PRECISION_MODE)\n    self.max_workspace_size = DEFAULT_MAX_WORKSPACE_SIZE\n    self.trt_profiles = None\n    self.trt_profile = None\n    self.conversion_fallback = False\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForTensorRT.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForTensorRT.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n    \"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\n    return cls(**config_dict)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForTensorRT.name","title":"name  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the CustomConfig.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\n@abc.abstractmethod\ndef name(cls) -&gt; str:\n    \"\"\"Name of the CustomConfig.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForTensorRT.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForTensorRT.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.CustomConfigForTensorRT.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.DeviceKind","title":"DeviceKind","text":"<p>               Bases: <code>Enum</code></p> <p>Supported types of devices.</p> <p>Parameters:</p> <ul> <li> <code>CPU</code>               (<code>str</code>)           \u2013            <p>Select CPU device.</p> </li> <li> <code>GPU</code>               (<code>str</code>)           \u2013            <p>Select GPU with CUDA support.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.Format","title":"Format","text":"<p>               Bases: <code>Enum</code></p> <p>All model formats supported by Model Navigator 'optimize' function.</p> <p>Parameters:</p> <ul> <li> <code>PYTHON</code>               (<code>str</code>)           \u2013            <p>Format indicating any model defined in Python.</p> </li> <li> <code>TORCH</code>               (<code>str</code>)           \u2013            <p>Format indicating PyTorch model.</p> </li> <li> <code>TENSORFLOW</code>               (<code>str</code>)           \u2013            <p>Format indicating TensorFlow model.</p> </li> <li> <code>JAX</code>               (<code>str</code>)           \u2013            <p>Format indicating JAX model.</p> </li> <li> <code>TORCHSCRIPT</code>               (<code>str</code>)           \u2013            <p>Format indicating TorchScript model.</p> </li> <li> <code>TF_SAVEDMODEL</code>               (<code>str</code>)           \u2013            <p>Format indicating TensorFlow SavedModel.</p> </li> <li> <code>TF_TRT</code>               (<code>str</code>)           \u2013            <p>Format indicating TensorFlow TensorRT model.</p> </li> <li> <code>TORCH_TRT</code>               (<code>str</code>)           \u2013            <p>Format indicating PyTorch TensorRT model.</p> </li> <li> <code>ONNX</code>               (<code>str</code>)           \u2013            <p>Format indicating ONNX model.</p> </li> <li> <code>TENSORRT</code>               (<code>str</code>)           \u2013            <p>Format indicating TensorRT model.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.JitType","title":"JitType","text":"<p>               Bases: <code>Enum</code></p> <p>TorchScript export parameter.</p> <p>Used for selecting the type of TorchScript export.</p> <p>Parameters:</p> <ul> <li> <code>TRACE</code>               (<code>str</code>)           \u2013            <p>Use tracing during export.</p> </li> <li> <code>SCRIPT</code>               (<code>str</code>)           \u2013            <p>Use scripting during export.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.MaxThroughputAndMinLatencyStrategy","title":"MaxThroughputAndMinLatencyStrategy","text":"<p>               Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the highest throughput and the lowest latency.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.MaxThroughputAndMinLatencyStrategy.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return name of strategy.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __str__(self):\n    \"\"\"Return name of strategy.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.MaxThroughputStrategy","title":"MaxThroughputStrategy","text":"<p>               Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the highest throughput.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.MaxThroughputStrategy.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return name of strategy.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __str__(self):\n    \"\"\"Return name of strategy.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.MaxThroughputWithLatencyBudgetStrategy","title":"MaxThroughputWithLatencyBudgetStrategy","text":"<pre><code>MaxThroughputWithLatencyBudgetStrategy(latency_budget)\n</code></pre> <p>               Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the hightest throughput within the latency budget.</p> <p>Initialize the class.</p> <p>Parameters:</p> <ul> <li> <code>latency_budget</code>               (<code>float</code>)           \u2013            <p>Latency budget in milliseconds.</p> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __init__(self, latency_budget: float) -&gt; None:\n    \"\"\"Initialize the class.\n\n    Args:\n        latency_budget: Latency budget in milliseconds.\n    \"\"\"\n    super().__init__()\n    self.latency_budget = latency_budget\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.MaxThroughputWithLatencyBudgetStrategy.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return name of strategy.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __str__(self):\n    \"\"\"Return name of strategy.\"\"\"\n    return f\"{self.__class__.__name__}({self.latency_budget}[ms])\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.MinLatencyStrategy","title":"MinLatencyStrategy","text":"<p>               Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the lowest latency.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.MinLatencyStrategy.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return name of strategy.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __str__(self):\n    \"\"\"Return name of strategy.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxConfig","title":"OnnxConfig  <code>dataclass</code>","text":"<pre><code>OnnxConfig(\n    custom_args=dict(),\n    device=None,\n    opset=DEFAULT_ONNX_OPSET,\n    onnx_extended_conversion=False,\n    graph_surgeon_optimization=True,\n    export_device=None,\n    dynamic_axes=None,\n    model_path=None,\n    export_engine=lambda: [OnnxTraceExportConfig()](),\n)\n</code></pre> <p>               Bases: <code>CustomConfigForFormat</code></p> <p>ONNX custom config used for ONNX export and conversion.</p> <p>Parameters:</p> <ul> <li> <code>opset</code>               (<code>Optional[int]</code>, default:                   <code>DEFAULT_ONNX_OPSET</code> )           \u2013            <p>ONNX opset used for conversion.</p> </li> <li> <code>onnx_extended_conversion</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enables additional conversions from TorchScript to ONNX.</p> </li> <li> <code>graph_surgeon_optimization</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enables polygraphy graph surgeon optimization: fold_constants, infer_shapes, toposort, cleanup.</p> </li> <li> <code>export_device</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Device used for ONNX export.</p> </li> <li> <code>dynamic_axes</code>               (<code>Optional[Dict[str, Union[Dict[int, str], List[int]]]]</code>, default:                   <code>None</code> )           \u2013            <p>Dynamic axes for ONNX conversion.</p> </li> <li> <code>model_path</code>               (<code>Optional[Union[str, Path]]</code>, default:                   <code>None</code> )           \u2013            <p>optional path to onnx model file, if provided the model will be loaded from the file instead of exporting to onnx</p> </li> <li> <code>export_engine</code>               (<code>List[OnnxExportEngineType]</code>, default:                   <code>lambda: [OnnxTraceExportConfig()]()</code> )           \u2013            <p>List of export engines to use. Expects only one engine of a type. First of each type will be used. Currently, only Torch Dynamo exports engine is supported in addtion to default Torch Trace export.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxConfig.format","title":"format  <code>property</code>","text":"<pre><code>format\n</code></pre> <p>Returns Format.ONNX.</p> <p>Returns:</p> <ul> <li> <code>Format</code>           \u2013            <p>Format.ONNX</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> <p>Only configuration related to ONNX export and conversion parameters are updated. We leave the dynamo and extended conversion flags as are set during config initialization.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def defaults(self) -&gt; None:\n    \"\"\"Update parameters to defaults.\n\n    Only configuration related to ONNX export and conversion parameters are updated. We leave the dynamo and\n    extended conversion flags as are set during config initialization.\n    \"\"\"\n    super().defaults()\n    self.opset = DEFAULT_ONNX_OPSET\n    self.graph_surgeon_optimization = True\n    self.export_device = None\n    self.dynamic_axes = None\n    self.model_path = None\n    self.export_engine = [OnnxTraceExportConfig()]\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n    \"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\n    return cls(**config_dict)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxConfig.get_export_engine","title":"get_export_engine","text":"<pre><code>get_export_engine(engine_type)\n</code></pre> <p>Find given export engine in export_engine list.</p> <p>Parameters:</p> <ul> <li> <code>engine_type</code>               (<code>Type[_EngineTypeT]</code>)           \u2013            <p>Type of export engine to find</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[_EngineTypeT]</code>           \u2013            <p>Export engine if found, otherwise None</p> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def get_export_engine(self, engine_type: Type[_EngineTypeT]) -&gt; Optional[_EngineTypeT]:\n    \"\"\"Find given export engine in export_engine list.\n\n    Args:\n        engine_type: Type of export engine to find\n\n    Returns:\n        Export engine if found, otherwise None\n    \"\"\"\n    for engine in self.export_engine:\n        if isinstance(engine, engine_type):\n            return engine\n    return None\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Name of the config.\"\"\"\n    return \"Onnx\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxDynamoExportConfig","title":"OnnxDynamoExportConfig  <code>dataclass</code>","text":"<pre><code>OnnxDynamoExportConfig(dynamo_dynamic_shapes=None)\n</code></pre> <p>               Bases: <code>DataObject</code></p> <p>ONNX export config used for ONNX Torch Dynamo export.</p> <p>Parameters:</p> <ul> <li> <code>dynamo_dynamic_shapes</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Enable dynamic shapes for dynamo export. By default dynamic shapes are enabled if dynamic_axes are set or batching is enabled.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxDynamoExportConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxDynamoExportConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxDynamoExportConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxDynamoExportConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxTraceExportConfig","title":"OnnxTraceExportConfig  <code>dataclass</code>","text":"<pre><code>OnnxTraceExportConfig()\n</code></pre> <p>               Bases: <code>DataObject</code></p> <p>ONNX export config used for ONNX Torch Trace export.</p> <p>Torch Trace export is performed by default, but when OnnxDynamoExportConfig is used in export_engins list OnnxTraceExportConfig must be explicitly provided to be performed.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxTraceExportConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxTraceExportConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxTraceExportConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OnnxTraceExportConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OptimizationProfile","title":"OptimizationProfile  <code>dataclass</code>","text":"<pre><code>OptimizationProfile(\n    max_batch_size=None,\n    batch_sizes=None,\n    window_size=DEFAULT_WINDOW_SIZE,\n    stability_percentage=DEFAULT_STABILITY_PERCENTAGE,\n    stabilization_windows=DEFAULT_STABILIZATION_WINDOWS,\n    min_trials=DEFAULT_MIN_TRIALS,\n    max_trials=DEFAULT_MAX_TRIALS,\n    throughput_cutoff_threshold=DEFAULT_THROUGHPUT_CUTOFF_THRESHOLD,\n    throughput_backoff_limit=DEFAULT_THROUGHPUT_BACKOFF_LIMIT,\n    dataloader=None,\n)\n</code></pre> <p>               Bases: <code>DataObject</code></p> <p>Optimization profile configuration.</p> <p>For each batch size profiler will run measurements in windows of fixed number of queries. Batch sizes are profiled in the ascending order.</p> <p>Profiler will run multiple trials and will stop when the measurements are stable (within <code>stability_percentage</code> from the mean) within three consecutive windows. If the measurements are not stable after <code>max_trials</code> trials, the profiler will stop with an error. Profiler will also stop profiling when the throughput does not increase at least by <code>throughput_cutoff_threshold</code>.</p> <p>Parameters:</p> <ul> <li> <code>max_batch_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Maximal batch size used during conversion and profiling. None mean automatic search is enabled.</p> </li> <li> <code>batch_sizes</code>           \u2013            <p>List of batch sizes to profile. None mean automatic search is enabled.</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>DEFAULT_WINDOW_SIZE</code> )           \u2013            <p>Number of requests to measure in each window.</p> </li> <li> <code>stability_percentage</code>               (<code>float</code>, default:                   <code>DEFAULT_STABILITY_PERCENTAGE</code> )           \u2013            <p>Allowed percentage of variation from the mean in consecutive windows.</p> </li> <li> <code>stabilization_windows</code>               (<code>int</code>, default:                   <code>DEFAULT_STABILIZATION_WINDOWS</code> )           \u2013            <p>Number consecutive windows selected for stabilization.</p> </li> <li> <code>min_trials</code>               (<code>int</code>, default:                   <code>DEFAULT_MIN_TRIALS</code> )           \u2013            <p>Minimal number of window trials.</p> </li> <li> <code>max_trials</code>               (<code>int</code>, default:                   <code>DEFAULT_MAX_TRIALS</code> )           \u2013            <p>Maximum number of window trials.</p> </li> <li> <code>throughput_cutoff_threshold</code>               (<code>Optional[float]</code>, default:                   <code>DEFAULT_THROUGHPUT_CUTOFF_THRESHOLD</code> )           \u2013            <p>Minimum throughput increase to continue profiling.</p> </li> <li> <code>throughput_backoff_limit</code>               (<code>int</code>, default:                   <code>DEFAULT_THROUGHPUT_BACKOFF_LIMIT</code> )           \u2013            <p>Back-off limit to run multiple more profiling steps to avoid stop at local minimum                       when throughput saturate based on <code>throughput_cutoff_threshold</code>.</p> </li> <li> <code>dataloader</code>               (<code>Optional[SizedDataLoader]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dataloader for profiling. Use only 1 sample.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OptimizationProfile.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate OptimizationProfile definition to avoid unsupported configurations.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate OptimizationProfile definition to avoid unsupported configurations.\"\"\"\n    if self.stability_percentage &lt;= 0:\n        raise ModelNavigatorConfigurationError(\"`stability_percentage` must be greater than 0.0.\")\n\n    if self.throughput_backoff_limit &lt; 0:\n        raise ModelNavigatorConfigurationError(\"`throughput_backoff_limit` must be greater then or equal to 0.\")\n\n    greater_or_equal_1 = [\n        \"window_size\",\n        \"stability_percentage\",\n        \"stabilization_windows\",\n        \"min_trials\",\n        \"max_trials\",\n    ]\n    for member in greater_or_equal_1:\n        value = getattr(self, member)\n        if value &lt; 1:\n            raise ModelNavigatorConfigurationError(f\"`{member}` must be greater or equal 1.\")\n\n    if self.min_trials &lt; self.stabilization_windows:\n        raise ModelNavigatorConfigurationError(\n            \"`min_trials` must be greater or equal than `stabilization_windows`.\"\n        )\n\n    if self.min_trials &gt; self.max_trials:\n        raise ModelNavigatorConfigurationError(\"`max_trials` must be greater or equal `min_trials`.\")\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OptimizationProfile.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Clone the current OptimizationProfile using deepcopy.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def clone(self) -&gt; \"OptimizationProfile\":\n    \"\"\"Clone the current OptimizationProfile using deepcopy.\"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OptimizationProfile.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OptimizationProfile.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(optimization_profile_dict)\n</code></pre> <p>Instantiate OptimizationProfile class from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>optimization_profile_dict</code>               (<code>Mapping</code>)           \u2013            <p>Data dictionary.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>OptimizationProfile</code>           \u2013            <p>OptimizationProfile</p> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, optimization_profile_dict: Mapping) -&gt; \"OptimizationProfile\":\n    \"\"\"Instantiate OptimizationProfile class from a dictionary.\n\n    Args:\n        optimization_profile_dict (Mapping): Data dictionary.\n\n    Returns:\n        OptimizationProfile\n    \"\"\"\n    return cls(\n        max_batch_size=optimization_profile_dict.get(\"max_batch_size\"),\n        batch_sizes=optimization_profile_dict.get(\"batch_sizes\"),\n        window_size=optimization_profile_dict.get(\"window_size\", DEFAULT_WINDOW_SIZE),\n        stability_percentage=optimization_profile_dict.get(\"stability_percentage\", DEFAULT_STABILITY_PERCENTAGE),\n        stabilization_windows=optimization_profile_dict.get(\"stabilization_windows\", DEFAULT_STABILIZATION_WINDOWS),\n        min_trials=optimization_profile_dict.get(\"min_trials\", DEFAULT_MIN_TRIALS),\n        max_trials=optimization_profile_dict.get(\"max_trials\", DEFAULT_MAX_TRIALS),\n        throughput_cutoff_threshold=optimization_profile_dict.get(\n            \"throughput_cutoff_threshold\", DEFAULT_THROUGHPUT_CUTOFF_THRESHOLD\n        ),\n        throughput_backoff_limit=optimization_profile_dict.get(\n            \"throughput_backoff_limit\", DEFAULT_THROUGHPUT_BACKOFF_LIMIT\n        ),\n    )\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OptimizationProfile.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OptimizationProfile.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.OptimizationProfile.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Append <code>dataloader</code> field to filtered fields during dump.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Append `dataloader` field to filtered fields during dump.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if not filter_fields:\n        filter_fields = []\n\n    filter_fields += [\"dataloader\"]\n    return super().to_dict(filter_fields=filter_fields, parse=parse)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.RuntimeSearchStrategy","title":"RuntimeSearchStrategy","text":"<p>Base class for runtime search strategies.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.RuntimeSearchStrategy.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return name of strategy.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __str__(self):\n    \"\"\"Return name of strategy.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.SelectedRuntimeStrategy","title":"SelectedRuntimeStrategy","text":"<pre><code>SelectedRuntimeStrategy(model_key, runner_name)\n</code></pre> <p>               Bases: <code>RuntimeSearchStrategy</code></p> <p>Get a selected runtime.</p> <p>Initialize the class.</p> <p>Parameters:</p> <ul> <li> <code>model_key</code>               (<code>str</code>)           \u2013            <p>Unique key of the model.</p> </li> <li> <code>runner_name</code>               (<code>str</code>)           \u2013            <p>Name of the runner.</p> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __init__(self, model_key: str, runner_name: str) -&gt; None:\n    \"\"\"Initialize the class.\n\n    Args:\n        model_key (str): Unique key of the model.\n        runner_name (str): Name of the runner.\n    \"\"\"\n    super().__init__()\n    self.model_key = model_key\n    self.runner_name = runner_name\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.SelectedRuntimeStrategy.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return name of strategy.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __str__(self):\n    \"\"\"Return name of strategy.\"\"\"\n    return f\"{self.__class__.__name__}({self.model_key}:{self.runner_name})\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.ShapeTuple","title":"ShapeTuple  <code>dataclass</code>","text":"<pre><code>ShapeTuple(min, opt, max)\n</code></pre> <p>               Bases: <code>DataObject</code></p> <p>Represents a set of shapes for a single binding in a profile.</p> <p>Each element of the tuple represents a shape for a single dimension of the binding.</p> <p>Parameters:</p> <ul> <li> <code>min</code>               (<code>Tuple[int]</code>)           \u2013            <p>The minimum shape that the profile will support.</p> </li> <li> <code>opt</code>               (<code>Tuple[int]</code>)           \u2013            <p>The shape for which TensorRT will optimize the engine.</p> </li> <li> <code>max</code>               (<code>Tuple[int]</code>)           \u2013            <p>The maximum shape that the profile will support.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.ShapeTuple.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Iterate over shapes.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over shapes.\"\"\"\n    yield from [self.min, self.opt, self.max]\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.ShapeTuple.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Representation.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __repr__(self):\n    \"\"\"Representation.\"\"\"\n    return type(self).__name__ + self.__str__()\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.ShapeTuple.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>String representation.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __str__(self):\n    \"\"\"String representation.\"\"\"\n    return f\"(min={self.min}, opt={self.opt}, max={self.max})\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.ShapeTuple.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.ShapeTuple.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.ShapeTuple.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.ShapeTuple.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.SizedIterable","title":"SizedIterable","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol representing sized iterable. Used by dataloader.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.SizedIterable.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Magic method iter.</p> <p>Returns:</p> <ul> <li> <code>Iterator</code>           \u2013            <p>Iterator to next item.</p> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Magic method __iter__.\n\n    Returns:\n        Iterator to next item.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.SizedIterable.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Magic method len.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Length of size iterable.</p> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Magic method __len__.\n\n    Returns:\n        Length of size iterable.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowConfig","title":"TensorFlowConfig  <code>dataclass</code>","text":"<pre><code>TensorFlowConfig(custom_args=dict(), device=None, jit_compile=(None,), enable_xla=(None,))\n</code></pre> <p>               Bases: <code>CustomConfigForFormat</code></p> <p>TensorFlow custom config used for SavedModel export.</p> <p>Parameters:</p> <ul> <li> <code>jit_compile</code>               (<code>Tuple[Optional[bool], ...]</code>, default:                   <code>(None,)</code> )           \u2013            <p>Enable or Disable jit_compile flag for tf.function wrapper for Jax infer function.</p> </li> <li> <code>enable_xla</code>               (<code>Tuple[Optional[bool], ...]</code>, default:                   <code>(None,)</code> )           \u2013            <p>Enable or Disable enable_xla flag for jax2tf converter.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowConfig.format","title":"format  <code>property</code>","text":"<pre><code>format\n</code></pre> <p>Returns Format.TF_SAVEDMODEL.</p> <p>Returns:</p> <ul> <li> <code>Format</code>           \u2013            <p>Format.TF_SAVEDMODEL</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def defaults(self) -&gt; None:\n    \"\"\"Update parameters to defaults.\"\"\"\n    self.jit_compile = (None,)\n    self.enable_xla = (None,)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n    \"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\n    return cls(**config_dict)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Name of the config.\"\"\"\n    return \"TensorFlow\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowTensorRTConfig","title":"TensorFlowTensorRTConfig  <code>dataclass</code>","text":"<pre><code>TensorFlowTensorRTConfig(\n    custom_args=dict(),\n    device=None,\n    trt_profiles=None,\n    trt_profile=None,\n    precision=DEFAULT_TENSORRT_PRECISION,\n    precision_mode=DEFAULT_TENSORRT_PRECISION_MODE,\n    max_workspace_size=DEFAULT_MAX_WORKSPACE_SIZE_TFTRT,\n    conversion_fallback=False,\n    minimum_segment_size=DEFAULT_MIN_SEGMENT_SIZE,\n)\n</code></pre> <p>               Bases: <code>CustomConfigForTensorRT</code></p> <p>TensorFlow TensorRT custom config used for TensorRT SavedModel export.</p> <p>Parameters:</p> <ul> <li> <code>minimum_segment_size</code>               (<code>int</code>, default:                   <code>DEFAULT_MIN_SEGMENT_SIZE</code> )           \u2013            <p>Min size of subgraph.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowTensorRTConfig.format","title":"format  <code>property</code>","text":"<pre><code>format\n</code></pre> <p>Returns Format.TF_TRT.</p> <p>Returns:</p> <ul> <li> <code>Format</code>           \u2013            <p>Format.TF_TRT</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowTensorRTConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize common TensorRT parameters and validate configuration.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize common TensorRT parameters and validate configuration.\"\"\"\n    precision = (self.precision,) if not isinstance(self.precision, (list, tuple)) else self.precision\n    self.precision = tuple(TensorRTPrecision(p) for p in precision)\n    self.precision_mode = TensorRTPrecisionMode(self.precision_mode)\n\n    # TODO: Remove before 1.0.0 release\n    if self.trt_profile is not None and self.trt_profiles is not None:\n        raise ModelNavigatorConfigurationError(\"Only one of trt_profile and trt_profiles can be set.\")\n    elif self.trt_profile:\n        warnings.warn(\n            \"trt_profile will be deprecated in future releases. Use trt_profiles instead.\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        self.trt_profiles = [self.trt_profile]\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowTensorRTConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def defaults(self) -&gt; None:\n    \"\"\"Update parameters to defaults.\"\"\"\n    super().defaults()\n    self.minimum_segment_size = DEFAULT_MIN_SEGMENT_SIZE\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowTensorRTConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowTensorRTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate TensorFlowTensorRTConfig from a dictionary.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TensorFlowTensorRTConfig\":\n    \"\"\"Instantiate TensorFlowTensorRTConfig from a dictionary.\"\"\"\n    if config_dict.get(\"trt_profiles\") is not None:\n        # if config_dict.get(\"trt_profiles\") is not None and not isinstance(config_dict[\"trt_profile\"], TensorRTProfile):\n        parsed_trt_profiles = []\n        for trt_profile in config_dict.get(\"trt_profiles\"):\n            if not isinstance(trt_profile, TensorRTProfile):\n                trt_profile = TensorRTProfile.from_dict(trt_profile)\n            parsed_trt_profiles.append(trt_profile)\n        config_dict[\"trt_profiles\"] = parsed_trt_profiles\n    return cls(**config_dict)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowTensorRTConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Name of the config.\"\"\"\n    return \"TensorFlowTensorRT\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowTensorRTConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowTensorRTConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorFlowTensorRTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTCompatibilityLevel","title":"TensorRTCompatibilityLevel","text":"<p>               Bases: <code>Enum</code></p> <p>Compatibility level for TensorRT.</p> <p>Parameters:</p> <ul> <li> <code>AMPERE_PLUS</code>               (<code>str</code>)           \u2013            <p>Support AMPERE plus architecture</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTConfig","title":"TensorRTConfig  <code>dataclass</code>","text":"<pre><code>TensorRTConfig(\n    custom_args=dict(),\n    device=None,\n    trt_profiles=None,\n    trt_profile=None,\n    precision=DEFAULT_TENSORRT_PRECISION,\n    precision_mode=DEFAULT_TENSORRT_PRECISION_MODE,\n    max_workspace_size=DEFAULT_MAX_WORKSPACE_SIZE,\n    conversion_fallback=False,\n    optimization_level=None,\n    compatibility_level=None,\n    onnx_parser_flags=None,\n    timing_cache_dir=None,\n    model_path=None,\n)\n</code></pre> <p>               Bases: <code>CustomConfigForTensorRT</code></p> <p>TensorRT custom config used for TensorRT conversion.</p> <p>Parameters:</p> <ul> <li> <code>optimization_level</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optimization level for TensorRT conversion. Allowed values are fom 0 to 5. Where default is                 3 based on TensorRT API documentation.</p> </li> <li> <code>compatibility_level</code>               (<code>Optional[TensorRTCompatibilityLevel]</code>, default:                   <code>None</code> )           \u2013            <p>Compatibility level for TensorRT conversion.</p> </li> <li> <code>onnx_parser_flags</code>               (<code>Optional[List[int]]</code>, default:                   <code>None</code> )           \u2013            <p>List of TensorRT OnnxParserFlags used for conversion.</p> </li> <li> <code>timing_cache_dir</code>               (<code>Optional[Union[str, Path]]</code>, default:                   <code>None</code> )           \u2013            <p>Storage directory for TRT tactic timing info collected from builder</p> </li> <li> <code>model_path</code>               (<code>Optional[Union[str, Path]]</code>, default:                   <code>None</code> )           \u2013            <p>optional path to trt model file, if provided the model will be loaded from the file instead of converting onnx to trt</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTConfig.format","title":"format  <code>property</code>","text":"<pre><code>format\n</code></pre> <p>Returns Format.TENSORRT.</p> <p>Returns:</p> <ul> <li> <code>Format</code>           \u2013            <p>Format.TENSORRT</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Parse dataclass enums.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Parse dataclass enums.\"\"\"\n    super().__post_init__()\n    if self.optimization_level is not None and (self.optimization_level &lt; 0 or self.optimization_level &gt; 5):\n        raise ModelNavigatorConfigurationError(\n            f\"TensorRT `optimization_level` must be between 0 and 5. Provided value: {self.optimization_level}.\"\n        )\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def defaults(self) -&gt; None:\n    \"\"\"Update parameters to defaults.\"\"\"\n    super().defaults()\n    self.optimization_level = None\n    self.compatibility_level = None\n    self.onnx_parser_flags = None\n    self.timing_cache_dir = None\n    self.model_path = None\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate TensorRTConfig from a dictionary.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TensorRTConfig\":\n    \"\"\"Instantiate TensorRTConfig from a dictionary.\"\"\"\n    if config_dict.get(\"trt_profiles\") is not None:\n        parsed_trt_profiles = []\n        for trt_profile in config_dict.get(\"trt_profiles\"):\n            if not isinstance(trt_profile, TensorRTProfile):\n                trt_profile = TensorRTProfile.from_dict(trt_profile)\n            parsed_trt_profiles.append(trt_profile)\n        config_dict[\"trt_profiles\"] = parsed_trt_profiles\n    return cls(**config_dict)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Name of the config.\"\"\"\n    return \"TensorRT\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTPrecision","title":"TensorRTPrecision","text":"<p>               Bases: <code>Enum</code></p> <p>Precisions supported during TensorRT conversions.</p> <p>Parameters:</p> <ul> <li> <code>INT8</code>               (<code>str</code>)           \u2013            <p>8-bit integer precision.</p> </li> <li> <code>FP8</code>               (<code>str</code>)           \u2013            <p>8-bit floating point precision.</p> </li> <li> <code>FP16</code>               (<code>str</code>)           \u2013            <p>16-bit floating point precision.</p> </li> <li> <code>BF16</code>               (<code>str</code>)           \u2013            <p>16-bit brain floating point precision.</p> </li> <li> <code>FP32</code>               (<code>str</code>)           \u2013            <p>32-bit floating point precision.</p> </li> <li> <code>NVFP4</code>               (<code>str</code>)           \u2013            <p>4-bit floating point precision.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTPrecisionMode","title":"TensorRTPrecisionMode","text":"<p>               Bases: <code>Enum</code></p> <p>Precision modes for TensorRT conversions.</p> <p>Parameters:</p> <ul> <li> <code>HIERARCHY</code>               (<code>str</code>)           \u2013            <p>Use TensorRT precision hierarchy starting from highest to lowest.</p> </li> <li> <code>SINGLE</code>               (<code>str</code>)           \u2013            <p>Use single precision.</p> </li> <li> <code>MIXED</code>               (<code>str</code>)           \u2013            <p>Use mixed precision.</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTProfile","title":"TensorRTProfile","text":"<p>               Bases: <code>Dict[str, ShapeTuple]</code></p> <p>Single optimization profile that can be used to build an engine.</p> <p>More specifically, it is an <code>Dict[str, ShapeTuple]</code> which maps binding names to a set of min/opt/max shapes.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTProfile.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> <p>Retrieves the shapes registered for a given input name.</p> <p>Returns:</p> <ul> <li> <code>ShapeTuple</code>          \u2013            <pre><code>A named tuple including ``min``, ``opt``, and ``max`` members for the shapes\ncorresponding to the input.\n</code></pre> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"Retrieves the shapes registered for a given input name.\n\n    Returns:\n        ShapeTuple:\n                A named tuple including ``min``, ``opt``, and ``max`` members for the shapes\n                corresponding to the input.\n    \"\"\"\n    if key not in self:\n        LOGGER.error(f\"Binding: {key} does not have shapes set in this profile\")\n    return super().__getitem__(key)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTProfile.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Representation.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __repr__(self):\n    \"\"\"Representation.\"\"\"\n    ret = \"TensorRTProfile()\"\n    for name, (min, opt, max) in self.items():\n        ret += f\".add('{name}', min={min}, opt={opt}, max={max})\"\n    return ret\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTProfile.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>String representation.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __str__(self):\n    \"\"\"String representation.\"\"\"\n    elems = []\n    for name, (min, opt, max) in self.items():\n        elems.append(f\"{name} [min={min}, opt={opt}, max={max}]\")\n\n    sep = \",\\n \"\n    return \"{\" + sep.join(elems) + \"}\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTProfile.add","title":"add","text":"<pre><code>add(name, min, opt, max)\n</code></pre> <p>A convenience function to add shapes for a single binding.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the binding.</p> </li> <li> <code>min</code>               (<code>Tuple[int]</code>)           \u2013            <p>The minimum shape that the profile will support.</p> </li> <li> <code>opt</code>               (<code>Tuple[int]</code>)           \u2013            <p>The shape for which TensorRT will optimize the engine.</p> </li> <li> <code>max</code>               (<code>Tuple[int]</code>)           \u2013            <p>The maximum shape that the profile will support.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Profile</code>          \u2013            <p>self, which allows this function to be easily chained to add multiple bindings, e.g., TensorRTProfile().add(...).add(...)</p> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def add(self, name, min, opt, max):\n    \"\"\"A convenience function to add shapes for a single binding.\n\n    Args:\n        name (str): The name of the binding.\n        min (Tuple[int]): The minimum shape that the profile will support.\n        opt (Tuple[int]): The shape for which TensorRT will optimize the engine.\n        max (Tuple[int]): The maximum shape that the profile will support.\n\n    Returns:\n        Profile:\n            self, which allows this function to be easily chained to add multiple bindings,\n            e.g., TensorRTProfile().add(...).add(...)\n    \"\"\"\n    self[name] = ShapeTuple(min, opt, max)\n    return self\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTProfile.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(profile_dict)\n</code></pre> <p>Create a TensorRTProfile from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>profile_dict</code>               (<code>Dict[str, Dict[str, Tuple[int, ...]]]</code>)           \u2013            <p>A dictionary mapping binding names to a dictionary containing <code>min</code>, <code>opt</code>, and <code>max</code> keys.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TensorRTProfile</code>          \u2013            <p>A TensorRTProfile object.</p> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, profile_dict: Dict[str, Dict[str, Tuple[int, ...]]]):\n    \"\"\"Create a TensorRTProfile from a dictionary.\n\n    Args:\n        profile_dict (Dict[str, Dict[str, Tuple[int, ...]]]):\n            A dictionary mapping binding names to a dictionary containing ``min``, ``opt``, and\n            ``max`` keys.\n\n    Returns:\n        TensorRTProfile:\n            A TensorRTProfile object.\n    \"\"\"\n    return cls({name: ShapeTuple(**shapes) for name, shapes in profile_dict.items()})\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorRTProfile.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Serialize to a dictionary.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Dict[str, Tuple[int, ...]]]</code>           \u2013            <p>Dict[str, Dict[str, Tuple[int, ...]]]: A dictionary mapping binding names to a dictionary containing <code>min</code>, <code>opt</code>, and <code>max</code> keys.</p> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Dict[str, Tuple[int, ...]]]:\n    \"\"\"Serialize to a dictionary.\n\n    Returns:\n        Dict[str, Dict[str, Tuple[int, ...]]]:\n            A dictionary mapping binding names to a dictionary containing ``min``, ``opt``, and\n            ``max`` keys.\n    \"\"\"\n    return {name: vars(shapes) for name, shapes in self.items()}\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TensorType","title":"TensorType","text":"<p>               Bases: <code>Enum</code></p> <p>All model formats supported by Model Navigator 'optimize' function.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchConfig","title":"TorchConfig  <code>dataclass</code>","text":"<pre><code>TorchConfig(custom_args=None, device=None, autocast=True, autocast_dtype=DEVICE, inference_mode=True)\n</code></pre> <p>               Bases: <code>CustomConfigForFormat</code></p> <p>Torch custom config used for torch runner.</p> <p>Parameters:</p> <ul> <li> <code>autocast</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enable Automatic Mixed Precision in runner (default: False).</p> </li> <li> <code>autocast_dtype</code>               (<code>AutocastType</code>, default:                   <code>DEVICE</code> )           \u2013            <p>dtype used for autocast</p> </li> <li> <code>inference_mode</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enable inference mode in runner (default: True).</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchConfig.format","title":"format  <code>property</code>","text":"<pre><code>format\n</code></pre> <p>Returns Format.TORCH.</p> <p>Returns:</p> <ul> <li> <code>Format</code>           \u2013            <p>Format.TORCH</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Post initialization to handle correctly enums.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Post initialization to handle correctly enums.\"\"\"\n    self.autocast_dtype: AutocastType = AutocastType(self.autocast_dtype)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def defaults(self) -&gt; None:\n    \"\"\"Update parameters to defaults.\"\"\"\n    self.autocast = True\n    self.autocast_dtype = AutocastType.DEVICE\n    self.inference_mode = True\n    self.custom_args = None\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate TorchConfig from a dictionary.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TorchConfig\":\n    \"\"\"Instantiate TorchConfig from a dictionary.\"\"\"\n    return cls(\n        autocast=config_dict.get(\"autocast\", True),\n        autocast_dtype=AutocastType(config_dict.get(\"autocast_dtype\", AutocastType.DEVICE)),\n        inference_mode=config_dict.get(\"inference_mode\", True),\n        custom_args=config_dict.get(\"custom_args\"),\n    )\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Name of the config.\"\"\"\n    return \"Torch\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchExportConfig","title":"TorchExportConfig  <code>dataclass</code>","text":"<pre><code>TorchExportConfig(custom_args=dict(), device=None, autocast=True, autocast_dtype=DEVICE, inference_mode=True)\n</code></pre> <p>               Bases: <code>CustomConfigForFormat</code></p> <p>Torch export custom config used for torch.export.export.</p> <p>Parameters:</p> <ul> <li> <code>autocast</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enable Automatic Mixed Precision in runner (default: False).</p> </li> <li> <code>autocast_dtype</code>               (<code>AutocastType</code>, default:                   <code>DEVICE</code> )           \u2013            <p>dtype used for autocast</p> </li> <li> <code>inference_mode</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enable inference mode in runner (default: True).</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchExportConfig.format","title":"format  <code>property</code>","text":"<pre><code>format\n</code></pre> <p>Returns Format.</p> <p>Returns:</p> <ul> <li> <code>Format</code>           \u2013            <p>Format.TORCH_EXPORTEDPROGRAM</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchExportConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Post initialization to handle correctly enums.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Post initialization to handle correctly enums.\"\"\"\n    self.autocast_dtype: AutocastType = AutocastType(self.autocast_dtype)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchExportConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def defaults(self) -&gt; None:\n    \"\"\"Update parameters to defaults.\"\"\"\n    self.autocast = True\n    self.autocast_dtype = AutocastType.DEVICE\n    self.inference_mode = True\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchExportConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchExportConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n    \"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\n    return cls(**config_dict)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchExportConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Name of the config.\"\"\"\n    return \"TorchExport\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchExportConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchExportConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchExportConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchScriptConfig","title":"TorchScriptConfig  <code>dataclass</code>","text":"<pre><code>TorchScriptConfig(\n    custom_args=dict(),\n    device=None,\n    jit_type=(SCRIPT, TRACE),\n    strict=True,\n    autocast=True,\n    autocast_dtype=DEVICE,\n    inference_mode=True,\n)\n</code></pre> <p>               Bases: <code>CustomConfigForFormat</code></p> <p>Torch custom config used for TorchScript export.</p> <p>Parameters:</p> <ul> <li> <code>jit_type</code>               (<code>Union[Union[str, JitType], Tuple[Union[str, JitType], ...]]</code>, default:                   <code>(SCRIPT, TRACE)</code> )           \u2013            <p>Type of TorchScript export.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enable or Disable strict flag for tracer used in TorchScript export (default: True).</p> </li> <li> <code>autocast</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enable Automatic Mixed Precision in runner (default: False).</p> </li> <li> <code>autocast_dtype</code>               (<code>AutocastType</code>, default:                   <code>DEVICE</code> )           \u2013            <p>dtype used for autocast</p> </li> <li> <code>inference_mode</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enable inference mode in runner (default: True).</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchScriptConfig.format","title":"format  <code>property</code>","text":"<pre><code>format\n</code></pre> <p>Returns Format.TORCHSCRIPT.</p> <p>Returns:</p> <ul> <li> <code>Format</code>           \u2013            <p>Format.TORCHSCRIPT</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchScriptConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Parse dataclass enums.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Parse dataclass enums.\"\"\"\n    jit_type = (self.jit_type,) if not isinstance(self.jit_type, (list, tuple)) else self.jit_type\n    self.jit_type = tuple(JitType(j) for j in jit_type)\n    self.autocast_dtype: AutocastType = AutocastType(self.autocast_dtype)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchScriptConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def defaults(self) -&gt; None:\n    \"\"\"Update parameters to defaults.\"\"\"\n    self.jit_type = (JitType.SCRIPT, JitType.TRACE)\n    self.strict = True\n    self.autocast = True\n    self.autocast_dtype = AutocastType.DEVICE\n    self.inference_mode = True\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchScriptConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchScriptConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n    \"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\n    return cls(**config_dict)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchScriptConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Name of the config.\"\"\"\n    return \"TorchScript\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchScriptConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchScriptConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchScriptConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchTensorRTConfig","title":"TorchTensorRTConfig  <code>dataclass</code>","text":"<pre><code>TorchTensorRTConfig(\n    custom_args=dict(),\n    device=None,\n    trt_profiles=None,\n    trt_profile=None,\n    precision=DEFAULT_TENSORRT_PRECISION,\n    precision_mode=DEFAULT_TENSORRT_PRECISION_MODE,\n    max_workspace_size=DEFAULT_MAX_WORKSPACE_SIZE_TORCHTRT,\n    conversion_fallback=False,\n    pickle_protocol=DEFAULT_PICKLE_PROTOCOL_TORCHTRT,\n)\n</code></pre> <p>               Bases: <code>CustomConfigForTensorRT</code></p> <p>Torch custom config used for TensorRT TorchScript conversion.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchTensorRTConfig.format","title":"format  <code>property</code>","text":"<pre><code>format\n</code></pre> <p>Returns Format.TORCH_TRT.</p> <p>Returns:</p> <ul> <li> <code>Format</code>           \u2013            <p>Format.TORCH_TRT</p> </li> </ul>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchTensorRTConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize common TensorRT parameters and validate configuration.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize common TensorRT parameters and validate configuration.\"\"\"\n    precision = (self.precision,) if not isinstance(self.precision, (list, tuple)) else self.precision\n    self.precision = tuple(TensorRTPrecision(p) for p in precision)\n    self.precision_mode = TensorRTPrecisionMode(self.precision_mode)\n\n    # TODO: Remove before 1.0.0 release\n    if self.trt_profile is not None and self.trt_profiles is not None:\n        raise ModelNavigatorConfigurationError(\"Only one of trt_profile and trt_profiles can be set.\")\n    elif self.trt_profile:\n        warnings.warn(\n            \"trt_profile will be deprecated in future releases. Use trt_profiles instead.\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        self.trt_profiles = [self.trt_profile]\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchTensorRTConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def defaults(self) -&gt; None:\n    \"\"\"Update parameters to defaults.\"\"\"\n    self.precision = tuple(TensorRTPrecision(p) for p in DEFAULT_TENSORRT_PRECISION)\n    self.precision_mode = TensorRTPrecisionMode(DEFAULT_TENSORRT_PRECISION_MODE)\n    self.max_workspace_size = DEFAULT_MAX_WORKSPACE_SIZE\n    self.trt_profiles = None\n    self.trt_profile = None\n    self.conversion_fallback = False\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchTensorRTConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to filter</p> </li> <li> <code>filter_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n    \"\"\"Filter fields in dictionary.\n\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n\n    Returns:\n        Filtered dictionary\n    \"\"\"\n    filtered_data = {key: value for key, value in data.items() if key not in filter_fields}\n    return filtered_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchTensorRTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate TorchTensorRTConfig from a dictionary.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TorchTensorRTConfig\":\n    \"\"\"Instantiate TorchTensorRTConfig from a dictionary.\"\"\"\n    if config_dict.get(\"trt_profiles\") is not None:\n        # if config_dict.get(\"trt_profiles\") is not None and not isinstance(config_dict[\"trt_profile\"], TensorRTProfile):\n        parsed_trt_profiles = []\n        for trt_profile in config_dict.get(\"trt_profiles\"):\n            if not isinstance(trt_profile, TensorRTProfile):\n                trt_profile = TensorRTProfile.from_dict(trt_profile)\n            parsed_trt_profiles.append(trt_profile)\n        config_dict[\"trt_profiles\"] = parsed_trt_profiles\n    return cls(**config_dict)\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchTensorRTConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Name of the config.\"\"\"\n    return \"TorchTensorRT\"\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchTensorRTConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict</code>)           \u2013            <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n    \"\"\"Parse values in provided data.\n\n    Args:\n        data: Dictionary with data to parse\n\n    Returns:\n        Parsed dictionary\n    \"\"\"\n    parsed_data = {}\n    for key, value in data.items():\n        parsed_data[key] = DataObject.parse_value(value)\n\n    return parsed_data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchTensorRTConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>           \u2013            <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n    \"\"\"Parse value to jsonable format.\n\n    Args:\n        value (Any): Value to be parsed.\n\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\n    if isinstance(value, DataObject):\n        value = value.to_dict(parse=True)\n    elif hasattr(value, \"to_json\"):\n        value = value.to_json()\n    elif isinstance(value, (Mapping, Profile)):\n        value = DataObject._from_dict(value)\n    elif isinstance(value, list) or isinstance(value, tuple):\n        value = DataObject._from_list(value)\n    elif isinstance(value, Enum):\n        value = value.value\n    elif isinstance(value, pathlib.Path):\n        value = str(value)\n    elif isinstance(value, ShapeTuple):\n        value = vars(value)\n    elif is_torch_available() and isinstance(value, torch.dtype):\n        value = str(value)\n\n    return value\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.TorchTensorRTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>filter_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of fields to filter out. Defaults to None.</p> </li> <li> <code>parse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n    \"\"\"Serialize to a dictionary.\n\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\n    if filter_fields:\n        filtered_data = DataObject.filter_data(\n            data=self.__dict__,\n            filter_fields=filter_fields,\n        )\n    else:\n        filtered_data = self.__dict__\n\n    if parse:\n        data = DataObject.parse_data(filtered_data)\n    else:\n        data = filtered_data\n\n    return data\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.configuration.map_custom_configs","title":"map_custom_configs","text":"<pre><code>map_custom_configs(custom_configs)\n</code></pre> <p>Map custom configs from list to dictionary.</p> <p>Parameters:</p> <ul> <li> <code>custom_configs</code>               (<code>Optional[Sequence[CustomConfig]]</code>)           \u2013            <p>List of custom configs passed to API method</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>           \u2013            <p>Mapped configs to dictionary</p> </li> </ul> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def map_custom_configs(custom_configs: Optional[Sequence[CustomConfig]]) -&gt; Dict:\n    \"\"\"Map custom configs from list to dictionary.\n\n    Args:\n        custom_configs: List of custom configs passed to API method\n\n    Returns:\n        Mapped configs to dictionary\n    \"\"\"\n    if not custom_configs:\n        return {}\n\n    return {config.name(): config for config in custom_configs}\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.MaxThroughputAndMinLatencyStrategy","title":"model_navigator.MaxThroughputAndMinLatencyStrategy","text":"<p>               Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the highest throughput and the lowest latency.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.MaxThroughputAndMinLatencyStrategy.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return name of strategy.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __str__(self):\n    \"\"\"Return name of strategy.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.MaxThroughputStrategy","title":"model_navigator.MaxThroughputStrategy","text":"<p>               Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the highest throughput.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.MaxThroughputStrategy.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return name of strategy.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __str__(self):\n    \"\"\"Return name of strategy.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"models_optimize/optimize/api/config/#model_navigator.MinLatencyStrategy","title":"model_navigator.MinLatencyStrategy","text":"<p>               Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the lowest latency.</p>"},{"location":"models_optimize/optimize/api/config/#model_navigator.MinLatencyStrategy.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return name of strategy.</p> Source code in <code>model_navigator/configuration/__init__.py</code> <pre><code>def __str__(self):\n    \"\"\"Return name of strategy.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"models_optimize/optimize/api/onnx/","title":"ONNX","text":""},{"location":"models_optimize/optimize/api/onnx/#model_navigator.onnx","title":"model_navigator.onnx","text":"<p>ONNX optimize API.</p>"},{"location":"models_optimize/optimize/api/onnx/#model_navigator.onnx.optimize","title":"optimize","text":"<pre><code>optimize(\n    model,\n    dataloader,\n    sample_count=DEFAULT_SAMPLE_COUNT,\n    batching=True,\n    target_formats=None,\n    target_device=CUDA,\n    runners=None,\n    optimization_profile=None,\n    workspace=None,\n    verbose=False,\n    debug=False,\n    verify_func=None,\n    custom_configs=None,\n)\n</code></pre> <p>Entrypoint for ONNX optimize.</p> <p>Perform conversion, correctness testing, profiling and model verification.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Union[Path, str]</code>)           \u2013            <p>ONNX model path or string</p> </li> <li> <code>dataloader</code>               (<code>SizedDataLoader</code>)           \u2013            <p>Sized iterable with data that will be feed to the model</p> </li> <li> <code>sample_count</code>               (<code>Optional[int]</code>, default:                   <code>DEFAULT_SAMPLE_COUNT</code> )           \u2013            <p>Limits how many samples will be used from dataloader</p> </li> <li> <code>batching</code>               (<code>Optional[bool]</code>, default:                   <code>True</code> )           \u2013            <p>Enable or disable batching on first (index 0) dimension of the model</p> </li> <li> <code>target_formats</code>               (<code>Optional[Tuple[Union[str, Format], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Target model formats for optimize process</p> </li> <li> <code>target_device</code>               (<code>Optional[DeviceKind]</code>, default:                   <code>CUDA</code> )           \u2013            <p>Target device for optimize process, default is CUDA</p> </li> <li> <code>runners</code>               (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Use only runners provided as parameter</p> </li> <li> <code>optimization_profile</code>               (<code>Optional[OptimizationProfile]</code>, default:                   <code>None</code> )           \u2013            <p>Optimization profile for conversion and profiling</p> </li> <li> <code>workspace</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Workspace where packages will be extracted</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging</p> </li> <li> <code>debug</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable debug logging from commands</p> </li> <li> <code>verify_func</code>               (<code>Optional[VerifyFunction]</code>, default:                   <code>None</code> )           \u2013            <p>Function for additional model verification</p> </li> <li> <code>custom_configs</code>               (<code>Optional[Sequence[CustomConfig]]</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of CustomConfigs used to control produced artifacts</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>           \u2013            <p>Package descriptor representing created package.</p> </li> </ul> Source code in <code>model_navigator/onnx/__init__.py</code> <pre><code>def optimize(\n    model: Union[pathlib.Path, str],\n    dataloader: SizedDataLoader,\n    sample_count: Optional[int] = DEFAULT_SAMPLE_COUNT,\n    batching: Optional[bool] = True,\n    target_formats: Optional[Tuple[Union[str, Format], ...]] = None,\n    target_device: Optional[DeviceKind] = DeviceKind.CUDA,\n    runners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\n    optimization_profile: Optional[OptimizationProfile] = None,\n    workspace: Optional[pathlib.Path] = None,\n    verbose: bool = False,\n    debug: bool = False,\n    verify_func: Optional[VerifyFunction] = None,\n    custom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n    \"\"\"Entrypoint for ONNX optimize.\n\n    Perform conversion, correctness testing, profiling and model verification.\n\n    Args:\n        model: ONNX model path or string\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as parameter\n        optimization_profile: Optimization profile for conversion and profiling\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\n    if isinstance(model, str):\n        model = pathlib.Path(model)\n\n    if target_formats is None:\n        target_formats = DEFAULT_ONNX_TARGET_FORMATS\n\n    if runners is None:\n        runners = default_runners(device_kind=target_device)\n    else:\n        runners = filter_runners(runners, device_kind=target_device)\n\n    if optimization_profile is None:\n        optimization_profile = OptimizationProfile()\n\n    target_formats_enums = enums.parse(target_formats, Format)\n    runner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\n\n    if Format.ONNX not in target_formats_enums:\n        target_formats_enums = (Format.ONNX,) + target_formats_enums\n\n    config = CommonConfig(\n        Framework.ONNX,\n        model=model,\n        dataloader=dataloader,\n        target_formats=target_formats_enums,\n        target_device=target_device,\n        sample_count=sample_count,\n        batch_dim=0 if batching else None,\n        runner_names=runner_names,\n        optimization_profile=optimization_profile,\n        verbose=verbose,\n        debug=debug,\n        verify_func=verify_func,\n        custom_configs=map_custom_configs(custom_configs=custom_configs),\n    )\n\n    models_config = ModelConfigBuilder.generate_model_config(\n        framework=Framework.ONNX,\n        target_formats=target_formats_enums,\n        custom_configs=custom_configs,\n    )\n\n    builders = [\n        preprocessing_builder,\n        find_device_max_batch_size_builder,\n        tensorrt_conversion_builder,\n        correctness_builder,\n        performance_builder,\n    ]\n\n    if verify_func:\n        builders.append(verify_builder)\n\n    package = optimize_pipeline(\n        model=model,\n        workspace=workspace,\n        builders=builders,\n        config=config,\n        models_config=models_config,\n    )\n\n    return package\n</code></pre>"},{"location":"models_optimize/optimize/api/python/","title":"Python","text":""},{"location":"models_optimize/optimize/api/python/#model_navigator.python","title":"model_navigator.python","text":"<p>Python optimize API.</p>"},{"location":"models_optimize/optimize/api/python/#model_navigator.python.optimize","title":"optimize","text":"<pre><code>optimize(\n    model,\n    dataloader,\n    sample_count=DEFAULT_SAMPLE_COUNT,\n    batching=True,\n    target_device=CPU,\n    runners=None,\n    optimization_profile=None,\n    workspace=None,\n    verbose=False,\n    debug=False,\n    verify_func=None,\n    custom_configs=None,\n)\n</code></pre> <p>Entrypoint for Python model optimize.</p> <p>Perform correctness testing, profiling and model verification.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Callable</code>)           \u2013            <p>Model inference function</p> </li> <li> <code>dataloader</code>               (<code>SizedDataLoader</code>)           \u2013            <p>Sized iterable with data that will be feed to the model</p> </li> <li> <code>sample_count</code>               (<code>int</code>, default:                   <code>DEFAULT_SAMPLE_COUNT</code> )           \u2013            <p>Limits how many samples will be used from dataloader</p> </li> <li> <code>batching</code>               (<code>Optional[bool]</code>, default:                   <code>True</code> )           \u2013            <p>Enable or disable batching on first (index 0) dimension of the model</p> </li> <li> <code>target_device</code>               (<code>Optional[DeviceKind]</code>, default:                   <code>CPU</code> )           \u2013            <p>Target device for optimize process, default is CPU</p> </li> <li> <code>runners</code>               (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Use only runners provided as parameter</p> </li> <li> <code>optimization_profile</code>               (<code>Optional[OptimizationProfile]</code>, default:                   <code>None</code> )           \u2013            <p>Optimization profile for conversion and profiling</p> </li> <li> <code>workspace</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Workspace where packages will be extracted</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging</p> </li> <li> <code>debug</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable debug logging from commands</p> </li> <li> <code>verify_func</code>               (<code>Optional[VerifyFunction]</code>, default:                   <code>None</code> )           \u2013            <p>Function for additional model verification</p> </li> <li> <code>custom_configs</code>               (<code>Optional[Sequence[CustomConfig]]</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of CustomConfigs used to control produced artifacts</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>           \u2013            <p>Package descriptor representing created package.</p> </li> </ul> Source code in <code>model_navigator/python/__init__.py</code> <pre><code>def optimize(\n    model: Callable,\n    dataloader: SizedDataLoader,\n    sample_count: int = DEFAULT_SAMPLE_COUNT,\n    batching: Optional[bool] = True,\n    target_device: Optional[DeviceKind] = DeviceKind.CPU,\n    runners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\n    optimization_profile: Optional[OptimizationProfile] = None,\n    workspace: Optional[pathlib.Path] = None,\n    verbose: bool = False,\n    debug: bool = False,\n    verify_func: Optional[VerifyFunction] = None,\n    custom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n    \"\"\"Entrypoint for Python model optimize.\n\n    Perform correctness testing, profiling and model verification.\n\n    Args:\n        model: Model inference function\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        target_device: Target device for optimize process, default is CPU\n        runners: Use only runners provided as parameter\n        optimization_profile: Optimization profile for conversion and profiling\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\n    if isinstance(model, str):\n        model = pathlib.Path(model)\n\n    if runners is None:\n        runners = default_runners(device_kind=target_device)\n\n    if optimization_profile is None:\n        optimization_profile = OptimizationProfile()\n\n    runner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\n\n    target_formats = DEFAULT_NONE_FRAMEWORK_TARGET_FORMATS\n\n    config = CommonConfig(\n        Framework.NONE,\n        model=model,\n        dataloader=dataloader,\n        target_formats=target_formats,\n        target_device=target_device,\n        sample_count=sample_count,\n        batch_dim=0 if batching else None,\n        runner_names=runner_names,\n        optimization_profile=optimization_profile,\n        verbose=verbose,\n        debug=debug,\n        verify_func=verify_func,\n        custom_configs=map_custom_configs(custom_configs=custom_configs),\n    )\n\n    models_config = ModelConfigBuilder.generate_model_config(\n        framework=Framework.NONE,\n        target_formats=target_formats,\n        custom_configs=[],\n    )\n\n    builders = [\n        preprocessing_builder,\n        correctness_builder,\n        performance_builder,\n        verify_builder,\n    ]\n\n    package = optimize_pipeline(\n        model=model,\n        workspace=workspace,\n        builders=builders,\n        config=config,\n        models_config=models_config,\n    )\n\n    return package\n</code></pre>"},{"location":"models_optimize/optimize/api/tensorflow/","title":"TensorFlow 2","text":""},{"location":"models_optimize/optimize/api/tensorflow/#model_navigator.tensorflow","title":"model_navigator.tensorflow","text":"<p>TensorFlow optimize API.</p>"},{"location":"models_optimize/optimize/api/tensorflow/#model_navigator.tensorflow.optimize","title":"optimize","text":"<pre><code>optimize(\n    model,\n    dataloader,\n    sample_count=DEFAULT_SAMPLE_COUNT,\n    batching=True,\n    input_names=None,\n    output_names=None,\n    target_formats=None,\n    target_device=CUDA,\n    runners=None,\n    optimization_profile=None,\n    workspace=None,\n    verbose=False,\n    debug=False,\n    verify_func=None,\n    custom_configs=None,\n)\n</code></pre> <p>Entrypoint for TensorFlow2 optimize.</p> <p>Perform export, conversion, correctness testing, profiling and model verification.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Model</code>)           \u2013            <p>TensorFlow2 model object</p> </li> <li> <code>dataloader</code>               (<code>SizedDataLoader</code>)           \u2013            <p>Sized iterable with data that will be feed to the model</p> </li> <li> <code>sample_count</code>               (<code>int</code>, default:                   <code>DEFAULT_SAMPLE_COUNT</code> )           \u2013            <p>Limits how many samples will be used from dataloader</p> </li> <li> <code>batching</code>               (<code>Optional[bool]</code>, default:                   <code>True</code> )           \u2013            <p>Enable or disable batching on first (index 0) dimension of the model</p> </li> <li> <code>input_names</code>               (<code>Optional[Tuple[str, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Model input names</p> </li> <li> <code>output_names</code>               (<code>Optional[Tuple[str, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Model output names</p> </li> <li> <code>target_formats</code>               (<code>Optional[Tuple[Union[str, Format], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Target model formats for optimize process</p> </li> <li> <code>target_device</code>               (<code>Optional[DeviceKind]</code>, default:                   <code>CUDA</code> )           \u2013            <p>Target device for optimize process, default is CUDA</p> </li> <li> <code>runners</code>               (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Use only runners provided as parameter</p> </li> <li> <code>optimization_profile</code>               (<code>Optional[OptimizationProfile]</code>, default:                   <code>None</code> )           \u2013            <p>Optimization profile for conversion and profiling</p> </li> <li> <code>workspace</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Workspace where packages will be extracted</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging</p> </li> <li> <code>debug</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable debug logging from commands</p> </li> <li> <code>verify_func</code>               (<code>Optional[VerifyFunction]</code>, default:                   <code>None</code> )           \u2013            <p>Function for additional model verification</p> </li> <li> <code>custom_configs</code>               (<code>Optional[Sequence[CustomConfig]]</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of CustomConfigs used to control produced artifacts</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>           \u2013            <p>Package descriptor representing created package.</p> </li> </ul> Source code in <code>model_navigator/tensorflow/__init__.py</code> <pre><code>def optimize(\n    model: tensorflow.keras.Model,\n    dataloader: SizedDataLoader,\n    sample_count: int = DEFAULT_SAMPLE_COUNT,\n    batching: Optional[bool] = True,\n    input_names: Optional[Tuple[str, ...]] = None,\n    output_names: Optional[Tuple[str, ...]] = None,\n    target_formats: Optional[Tuple[Union[str, Format], ...]] = None,\n    target_device: Optional[DeviceKind] = DeviceKind.CUDA,\n    runners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\n    optimization_profile: Optional[OptimizationProfile] = None,\n    workspace: Optional[pathlib.Path] = None,\n    verbose: bool = False,\n    debug: bool = False,\n    verify_func: Optional[VerifyFunction] = None,\n    custom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n    \"\"\"Entrypoint for TensorFlow2 optimize.\n\n    Perform export, conversion, correctness testing, profiling and model verification.\n\n    Args:\n        model: TensorFlow2 model object\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        input_names: Model input names\n        output_names: Model output names\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as parameter\n        optimization_profile: Optimization profile for conversion and profiling\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\n    if target_device == DeviceKind.CPU and any(\n        device.device_type == \"GPU\" for device in tensorflow.config.get_visible_devices()\n    ):\n        raise ModelNavigatorConfigurationError(\n            \"\\n\"\n            \"    'target_device == nav.DeviceKind.CPU' is not supported for TensorFlow2 when GPU is available.\\n\"\n            \"    To optimize model for CPU, disable GPU with: \"\n            \"'tf.config.set_visible_devices([], 'GPU')' directly after importing TensorFlow.\\n\"\n        )\n\n    if target_formats is None:\n        target_formats = DEFAULT_TENSORFLOW_TARGET_FORMATS\n\n    if runners is None:\n        runners = default_runners(device_kind=target_device)\n    else:\n        runners = filter_runners(runners, device_kind=target_device)\n\n    target_formats_enums = enums.parse(target_formats, Format)\n    runner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\n\n    if optimization_profile is None:\n        optimization_profile = OptimizationProfile()\n\n    if Format.TENSORFLOW not in target_formats_enums:\n        target_formats_enums = (Format.TENSORFLOW,) + target_formats_enums\n\n    config = CommonConfig(\n        Framework.TENSORFLOW,\n        model=model,\n        dataloader=dataloader,\n        target_formats=target_formats_enums,\n        target_device=target_device,\n        sample_count=sample_count,\n        _input_names=input_names,\n        _output_names=output_names,\n        batch_dim=0 if batching else None,\n        runner_names=runner_names,\n        optimization_profile=optimization_profile,\n        verbose=verbose,\n        debug=debug,\n        verify_func=verify_func,\n        custom_configs=map_custom_configs(custom_configs=custom_configs),\n    )\n\n    models_config = ModelConfigBuilder.generate_model_config(\n        framework=Framework.TENSORFLOW,\n        target_formats=target_formats_enums,\n        custom_configs=custom_configs,\n    )\n\n    builders = [\n        preprocessing_builder,\n        tensorflow_export_builder,\n        find_device_max_batch_size_builder,\n        tensorflow_conversion_builder,\n        tensorflow_tensorrt_conversion_builder,\n        tensorrt_conversion_builder,\n        correctness_builder,\n        performance_builder,\n    ]\n    if verify_func:\n        builders.append(verify_builder)\n\n    package = optimize_pipeline(\n        model=model,\n        workspace=workspace,\n        builders=builders,\n        config=config,\n        models_config=models_config,\n    )\n\n    return package\n</code></pre>"},{"location":"models_optimize/optimize/api/tensorrt/","title":"TensorRT","text":""},{"location":"models_optimize/optimize/api/tensorrt/#model_navigator.tensorrt","title":"model_navigator.tensorrt","text":"<p>TensorRT optimize API.</p>"},{"location":"models_optimize/optimize/api/tensorrt/#model_navigator.tensorrt.optimize","title":"optimize","text":"<pre><code>optimize(\n    model,\n    dataloader,\n    sample_count=DEFAULT_SAMPLE_COUNT,\n    batching=True,\n    runners=None,\n    optimization_profile=None,\n    workspace=None,\n    verbose=False,\n    debug=False,\n    verify_func=None,\n)\n</code></pre> <p>Function executes correctness test, performance profiling and optional verification on provided TensorRT model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Union[Path, str]</code>)           \u2013            <p>TensorRT model path or string</p> </li> <li> <code>dataloader</code>               (<code>SizedDataLoader</code>)           \u2013            <p>Sized iterable with data that will be feed to the model</p> </li> <li> <code>sample_count</code>               (<code>Optional[int]</code>, default:                   <code>DEFAULT_SAMPLE_COUNT</code> )           \u2013            <p>Limits how many samples will be used from dataloader</p> </li> <li> <code>batching</code>               (<code>Optional[bool]</code>, default:                   <code>True</code> )           \u2013            <p>Enable or disable batching on first (index 0) dimension of the model</p> </li> <li> <code>runners</code>               (<code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code>, default:                   <code>None</code> )           \u2013            <p>Use only runners provided as parameter</p> </li> <li> <code>optimization_profile</code>               (<code>Optional[OptimizationProfile]</code>, default:                   <code>None</code> )           \u2013            <p>Optimization profile for conversion and profiling</p> </li> <li> <code>workspace</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Workspace where packages will be extracted</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging</p> </li> <li> <code>debug</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable debug logging from commands</p> </li> <li> <code>verify_func</code>               (<code>Optional[VerifyFunction]</code>, default:                   <code>None</code> )           \u2013            <p>Function for additional model verification</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>           \u2013            <p>Package descriptor representing created package.</p> </li> </ul> Source code in <code>model_navigator/tensorrt/__init__.py</code> <pre><code>def optimize(\n    model: Union[Path, str],\n    dataloader: SizedDataLoader,\n    sample_count: Optional[int] = DEFAULT_SAMPLE_COUNT,\n    batching: Optional[bool] = True,\n    runners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\n    optimization_profile: Optional[OptimizationProfile] = None,\n    workspace: Optional[Path] = None,\n    verbose: bool = False,\n    debug: bool = False,\n    verify_func: Optional[VerifyFunction] = None,\n) -&gt; Package:\n    \"\"\"Function executes correctness test, performance profiling and optional verification on provided TensorRT model.\n\n    Args:\n        model: TensorRT model path or string\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        runners: Use only runners provided as parameter\n        optimization_profile: Optimization profile for conversion and profiling\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\n    if isinstance(model, str):\n        model = Path(model)\n    target_formats = DEFAULT_TENSORRT_TARGET_FORMATS\n    target_device = DeviceKind.CUDA\n\n    if runners is None:\n        runners = default_runners(device_kind=target_device)\n    else:\n        runners = filter_runners(runners, device_kind=target_device)\n\n    if optimization_profile is None:\n        optimization_profile = OptimizationProfile()\n\n    runner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\n\n    config = CommonConfig(\n        Framework.TENSORRT,\n        model=model,\n        dataloader=dataloader,\n        target_formats=target_formats,\n        target_device=target_device,\n        sample_count=sample_count,\n        batch_dim=0 if batching else None,\n        runner_names=runner_names,\n        optimization_profile=optimization_profile,\n        verbose=verbose,\n        debug=debug,\n        verify_func=verify_func,\n    )\n\n    models_config = ModelConfigBuilder.generate_model_config(\n        framework=Framework.TENSORRT,\n        target_formats=target_formats,\n        custom_configs=None,\n    )\n\n    builders = [\n        preprocessing_builder,\n        correctness_builder,\n        performance_builder,\n    ]\n    if verify_func:\n        builders.append(verify_builder)\n\n    package = optimize_pipeline(\n        model=model,\n        workspace=workspace,\n        builders=builders,\n        config=config,\n        models_config=models_config,\n    )\n\n    return package\n</code></pre>"},{"location":"models_optimize/optimize/api/torch/","title":"PyTorch","text":""},{"location":"models_optimize/optimize/api/torch/#model_navigator.torch","title":"model_navigator.torch","text":"<p>Torch optimize API.</p>"},{"location":"models_optimize/optimize/api/torch/#model_navigator.torch.optimize","title":"optimize","text":"<pre><code>optimize(\n    model,\n    dataloader,\n    sample_count=DEFAULT_SAMPLE_COUNT,\n    batching=True,\n    input_names=None,\n    output_names=None,\n    target_formats=None,\n    target_device=CUDA,\n    runners=None,\n    optimization_profile=None,\n    workspace=None,\n    verbose=False,\n    debug=False,\n    verify_func=None,\n    custom_configs=None,\n    model_precision=None,\n)\n</code></pre> <p>Entrypoint for Torch optimize.</p> <p>Perform export, conversion, correctness testing, profiling and model verification.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>PyTorch model object</p> </li> <li> <code>dataloader</code>               (<code>SizedDataLoader</code>)           \u2013            <p>Sized iterable with data that will be feed to the model</p> </li> <li> <code>sample_count</code>               (<code>Optional[int]</code>, default:                   <code>DEFAULT_SAMPLE_COUNT</code> )           \u2013            <p>Limits how many samples will be used from dataloader</p> </li> <li> <code>batching</code>               (<code>Optional[bool]</code>, default:                   <code>True</code> )           \u2013            <p>Enable or disable batching on first (index 0) dimension of the model</p> </li> <li> <code>input_names</code>               (<code>Optional[Tuple[str, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Model input names</p> </li> <li> <code>output_names</code>               (<code>Optional[Tuple[str, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Model output names</p> </li> <li> <code>target_formats</code>               (<code>Optional[Tuple[Union[str, Format], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Target model formats for optimize process</p> </li> <li> <code>target_device</code>               (<code>Optional[DeviceKind]</code>, default:                   <code>CUDA</code> )           \u2013            <p>Target device for optimize process, default is CUDA</p> </li> <li> <code>runners</code>               (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Use only runners provided as parameter</p> </li> <li> <code>optimization_profile</code>               (<code>Optional[OptimizationProfile]</code>, default:                   <code>None</code> )           \u2013            <p>Optimization profile for conversion and profiling</p> </li> <li> <code>workspace</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Workspace where packages will be extracted</p> </li> <li> <code>verbose</code>               (<code>Optional[bool]</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging</p> </li> <li> <code>debug</code>               (<code>Optional[bool]</code>, default:                   <code>False</code> )           \u2013            <p>Enable debug logging from commands</p> </li> <li> <code>verify_func</code>               (<code>Optional[VerifyFunction]</code>, default:                   <code>None</code> )           \u2013            <p>Function for additional model verification</p> </li> <li> <code>custom_configs</code>               (<code>Optional[Sequence[CustomConfig]]</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of CustomConfigs used to control produced artifacts</p> </li> <li> <code>model_precision</code>               (<code>Optional[PrecisionType]</code>, default:                   <code>None</code> )           \u2013            <p>Source model precision.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>           \u2013            <p>Package descriptor representing created package.</p> </li> </ul> Source code in <code>model_navigator/torch/__init__.py</code> <pre><code>def optimize(\n    model: torch.nn.Module,\n    dataloader: SizedDataLoader,\n    sample_count: Optional[int] = DEFAULT_SAMPLE_COUNT,\n    batching: Optional[bool] = True,\n    input_names: Optional[Tuple[str, ...]] = None,\n    output_names: Optional[Tuple[str, ...]] = None,\n    target_formats: Optional[Tuple[Union[str, Format], ...]] = None,\n    target_device: Optional[DeviceKind] = DeviceKind.CUDA,\n    runners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\n    optimization_profile: Optional[OptimizationProfile] = None,\n    workspace: Optional[pathlib.Path] = None,\n    verbose: Optional[bool] = False,\n    debug: Optional[bool] = False,\n    verify_func: Optional[VerifyFunction] = None,\n    custom_configs: Optional[Sequence[CustomConfig]] = None,\n    model_precision: Optional[PrecisionType] = None,\n) -&gt; Package:\n    \"\"\"Entrypoint for Torch optimize.\n\n    Perform export, conversion, correctness testing, profiling and model verification.\n\n    Args:\n        model: PyTorch model object\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        input_names: Model input names\n        output_names: Model output names\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as parameter\n        optimization_profile: Optimization profile for conversion and profiling\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n        model_precision: Source model precision.\n\n\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\n    if target_formats is None:\n        target_formats = DEFAULT_TORCH_TARGET_FORMATS\n        LOGGER.info(f\"Using default target formats: {[tf.name for tf in target_formats]}\")\n\n    if runners is None:\n        runners = default_runners(device_kind=target_device)\n    else:\n        runners = filter_runners(runners, device_kind=target_device)\n\n    target_formats = enums.parse(target_formats, Format)\n    runner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\n\n    if optimization_profile is None:\n        optimization_profile = OptimizationProfile()\n\n    if Format.TORCH not in target_formats:\n        target_formats = (Format.TORCH,) + target_formats\n\n    config = CommonConfig(\n        framework=Framework.TORCH,\n        model=model,\n        dataloader=dataloader,\n        target_formats=target_formats,\n        sample_count=sample_count,\n        _input_names=input_names,\n        _output_names=output_names,\n        target_device=target_device,\n        batch_dim=0 if batching else None,\n        runner_names=runner_names,\n        optimization_profile=optimization_profile,\n        verbose=verbose,\n        debug=debug,\n        verify_func=verify_func,\n        custom_configs=map_custom_configs(custom_configs=custom_configs),\n        model_precision=model_precision,\n    )\n\n    models_config = ModelConfigBuilder.generate_model_config(\n        framework=Framework.TORCH,\n        target_formats=target_formats,\n        custom_configs=custom_configs,\n    )\n\n    builders = [\n        preprocessing_builder,\n        torch_export_builder,\n        find_device_max_batch_size_builder,\n        torch_export_onnx_builder,\n        torch_exportedprogram_builder,\n        torch_conversion_builder,\n        torch_tensorrt_conversion_builder,\n        tensorrt_conversion_builder,\n        correctness_builder,\n        performance_builder,\n    ]\n    if verify_func:\n        builders.append(verify_builder)\n\n    package = optimize_pipeline(\n        model=model,\n        workspace=workspace,\n        builders=builders,\n        config=config,\n        models_config=models_config,\n    )\n\n    return package\n</code></pre>"},{"location":"models_optimize/package/package/","title":"Navigator Package","text":""},{"location":"models_optimize/package/package/#navigator-package","title":"Navigator Package","text":"<p>The model graph and/or checkpoint is not enough to perform a successful deployment of the model. When you are deploying model for inference you need to be aware of model inputs and outputs definition, maximal batch size that can be used for inference and other.</p> <p>On that purpose, we have created a <code>Navigator Package</code> - an artifact containing the serialized model, model metadata and optimization details.</p> <p>The <code>Navigator Package</code> is a recommended way of sharing the optimized model for deployment on PyTriton or Triton Inference Server sections or re-running the <code>optimize</code> method on different hardware.</p>"},{"location":"models_optimize/package/package/#save","title":"Save","text":"<p>The package created during models optimize can be saved in form of Zip file using the API method:</p> <pre><code>import model_navigator as nav\n\nnav.package.save(\n    package=package,\n    path=\"/path/to/package.nav\"\n)\n</code></pre> <p>The <code>save</code> method collect the generated models from workspace selecting:</p> <ul> <li>base formats - first available serialization formats exporting model from source</li> <li>max throughput format - the model that achieved the highest throughput during profiling</li> <li>min latency format - the model that achieved the minimal latency during profiling</li> </ul> <p>Additionally, the package contains:</p> <ul> <li>status file with optimization details</li> <li>logs from optimize execution</li> <li>reproduction script per each model format</li> <li>input and output data samples in form on numpy files</li> </ul> <p>Read more in save method API specification.</p>"},{"location":"models_optimize/package/package/#load","title":"Load","text":"<p>The packages saved to file can be loaded for further processing:</p> <pre><code>import model_navigator as nav\n\npackage = nav.package.load(\n    path=\"/path/to/package.nav\"\n)\n</code></pre> <p>Once the package is loaded, you can obtain desired information or use it to <code>optimize</code> or <code>profile</code> the package. Read more in load method API specification.</p>"},{"location":"models_optimize/package/package/#optimize","title":"Optimize","text":"<p>The loaded package object can be used to re-run the optimize process. In comparison to the framework dedicated API, the package optimize process starts from the serialized models inside the package and reproduces the available optimization paths. This step can be used to reproduce the process without access to sources on different hardware.</p> <p>The optimization from the package can be run using:</p> <pre><code>import model_navigator as nav\n\noptimized_package = nav.package.optimize(\n    package=package\n)\n</code></pre> <p>At the end of the process, the new optimized models are generated. Please be aware, the workspace is overridden in this step. Read more in optimize method API specification.</p>"},{"location":"models_optimize/package/package/#profile","title":"Profile","text":"<p>The optimize process uses a single sample from dataloader for profiling. The process is focusing on selecting the best model format, and this requires an unequivocal sample for performance comparison.</p> <p>In some cases, you may want to profile the models on different dataset. For that purpose, the Triton Model Navigator exposes the API for profiling all samples in the dataset for each model:</p> <pre><code>import torch\nimport model_navigator as nav\n\nprofiling_results = nav.package.profile(\n    package=package,\n    dataloader=[torch.randn(1, 3, 256, 256), torch.randn(1, 3, 512, 512)],\n)\n</code></pre> <p>The results contain profiling information per each model and sample. You can use it to perform desired analysis based on the results. Read more in profile method API specification.</p>"},{"location":"models_optimize/package/api/package/","title":"Package","text":""},{"location":"models_optimize/package/api/package/#model_navigator.package.package","title":"model_navigator.package.package","text":"<p>Package module - structure to snapshot optimization result.</p>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package","title":"Package","text":"<pre><code>Package(status, workspace, model=None)\n</code></pre> <p>Class for storing pipeline execution status.</p> <p>Initialize object.</p> <p>Parameters:</p> <ul> <li> <code>status</code>               (<code>Status</code>)           \u2013            <p>A navigator execution status</p> </li> <li> <code>workspace</code>               (<code>Workspace</code>)           \u2013            <p>Workspace for package files</p> </li> <li> <code>model</code>               (<code>Optional[object]</code>, default:                   <code>None</code> )           \u2013            <p>An optional model</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def __init__(self, status: Status, workspace: Workspace, model: Optional[object] = None):\n    \"\"\"Initialize object.\n\n    Args:\n        status: A navigator execution status\n        workspace: Workspace for package files\n        model: An optional model\n    \"\"\"\n    self.status = status\n    self.workspace = workspace\n    self._model = model\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package.config","title":"config  <code>property</code>","text":"<pre><code>config\n</code></pre> <p>Generate configuration from package.</p> <p>Returns:</p> <ul> <li> <code>CommonConfig</code>           \u2013            <p>The configuration object</p> </li> </ul>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package.framework","title":"framework  <code>property</code>","text":"<pre><code>framework\n</code></pre> <p>Framework for which package was created.</p> <p>Returns:</p> <ul> <li> <code>Framework</code>           \u2013            <p>Framework object for package</p> </li> </ul>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package.model","title":"model  <code>property</code>","text":"<pre><code>model\n</code></pre> <p>Return source model.</p> <p>Returns:</p> <ul> <li> <code>object</code>           \u2013            <p>Source model.</p> </li> </ul>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package._create_status_file","title":"_create_status_file","text":"<pre><code>_create_status_file()\n</code></pre> <p>Create a status.yaml file for package.</p> Source code in <code>model_navigator/package/package.py</code> <pre><code>def _create_status_file(self) -&gt; None:\n    \"\"\"Create a status.yaml file for package.\"\"\"\n    path = self.workspace.path / self.status_filename\n    data = self._status_serializable_dict()\n    with path.open(\"w\") as f:\n        yaml.safe_dump(data, f, sort_keys=False)\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package._delete_status_file","title":"_delete_status_file","text":"<pre><code>_delete_status_file()\n</code></pre> <p>Delete the status.yaml file from package.</p> Source code in <code>model_navigator/package/package.py</code> <pre><code>def _delete_status_file(self):\n    \"\"\"Delete the status.yaml file from package.\"\"\"\n    path = self.workspace.path / self.status_filename\n    if path.exists():\n        path.unlink()\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package._get_custom_configs","title":"_get_custom_configs","text":"<pre><code>_get_custom_configs(custom_configs)\n</code></pre> <p>Build custom configs from config data.</p> <p>Parameters:</p> <ul> <li> <code>custom_configs</code>               (<code>Dict[str, Union[Dict, CustomConfigForFormat]]</code>)           \u2013            <p>Dictionary with custom configs data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>           \u2013            <p>List with mapped objects</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def _get_custom_configs(self, custom_configs: Dict[str, Union[Dict, CustomConfigForFormat]]) -&gt; Dict:\n    \"\"\"Build custom configs from config data.\n\n    Args:\n        custom_configs: Dictionary with custom configs data\n\n    Returns:\n        List with mapped objects\n    \"\"\"\n    custom_configs_mapped = {}\n    for class_name, obj in custom_configs.items():\n        if isinstance(obj, dict):\n            custom_config_class = CUSTOM_CONFIGS_MAPPING[class_name]\n            obj = custom_config_class.from_dict(obj)  # pytype: disable=not-instantiable\n\n        custom_configs_mapped[class_name] = obj\n\n    return custom_configs_mapped\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package._get_runner","title":"_get_runner","text":"<pre><code>_get_runner(model_key, runner_name, device, return_type, inplace=False, runner_config=None)\n</code></pre> <p>Load runner.</p> <p>Parameters:</p> <ul> <li> <code>model_key</code>               (<code>str</code>)           \u2013            <p>Unique key of the model.</p> </li> <li> <code>runner_name</code>               (<code>str</code>)           \u2013            <p>Name of the runner.</p> </li> <li> <code>return_type</code>               (<code>TensorType</code>)           \u2013            <p>Type of the runner output.</p> </li> <li> <code>device</code>               (<code>str</code>)           \u2013            <p>Device on which the model has been executed</p> </li> <li> <code>inplace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Indicate if runner is in inplace mode.</p> </li> <li> <code>runner_config</code>               (<code>Optional[RunnerConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Runner configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NavigatorRunner</code>           \u2013            <p>NavigatorRunner object</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def _get_runner(\n    self,\n    model_key: str,\n    runner_name: str,\n    device: str,\n    return_type: TensorType,\n    inplace: bool = False,\n    runner_config: Optional[RunnerConfig] = None,\n) -&gt; NavigatorRunner:\n    \"\"\"Load runner.\n\n    Args:\n        model_key: Unique key of the model.\n        runner_name: Name of the runner.\n        return_type: Type of the runner output.\n        device: Device on which the model has been executed\n        inplace: Indicate if runner is in inplace mode.\n        runner_config: Runner configuration.\n\n    Raises:\n        ModelNavigatorNotFoundError when no runner found for provided constraints.\n\n    Returns:\n        NavigatorRunner object\n    \"\"\"\n    try:\n        model_config = self.status.models_status[model_key].model_config\n    except KeyError:\n        raise ModelNavigatorNotFoundError(f\"Model {model_key} not found.\") from None\n\n    if is_source_format(model_config.format):\n        model = self._model\n    else:\n        model = self.workspace.path / model_config.path\n\n    if runner_config is None:\n        runner_config = {}\n\n    device_kind = get_device_kind_from_device_string(device)\n    LOGGER.info(f\"Creating model `{model_key}` on runner `{runner_name}` and device `{device}`\")\n    # TODO: implement better handling for redundant device argument in _get_runner and runner_config\n    runner_config_dict = runner_config.to_dict(parse=True) if runner_config else {}\n    runner_config_dict[\"device\"] = device\n\n    return get_runner(runner_name, device_kind)(\n        model=model,\n        input_metadata=self.status.input_metadata,\n        output_metadata=self.status.output_metadata,\n        return_type=return_type,\n        # device=device, # TODO: remove redundant device argument and use runner_config\n        inplace=inplace,\n        **runner_config_dict,\n    )  # pytype: disable=not-instantiable\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package._status_serializable_dict","title":"_status_serializable_dict","text":"<pre><code>_status_serializable_dict()\n</code></pre> <p>Convert status to serializable dict.</p> Source code in <code>model_navigator/package/package.py</code> <pre><code>def _status_serializable_dict(self) -&gt; Dict:\n    \"\"\"Convert status to serializable dict.\"\"\"\n    config = DataObject.filter_data(\n        data=self.status.config,\n        filter_fields=[\n            \"model\",\n            \"dataloader\",\n            \"verify_func\",\n            \"workspace\",\n        ],\n    )\n    config = DataObject.parse_data(config)\n    status = copy.copy(self.status)\n    status.config = config\n    data = status.to_dict(parse=True)\n    return data\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package.get_best_model_status","title":"get_best_model_status","text":"<pre><code>get_best_model_status(strategies=None, include_source=True)\n</code></pre> <p>Returns ModelStatus of best model for given strategy.</p> <p>Parameters:</p> <ul> <li> <code>strategies</code>               (<code>Optional[List[RuntimeSearchStrategy]]</code>, default:                   <code>None</code> )           \u2013            <p>List of strategies for finding the best model. Strategies are selected in provided order. When         first fails, next strategy from the list is used. When no strategies have been provided it         defaults to [<code>MaxThroughputAndMinLatencyStrategy</code>, <code>MinLatencyStrategy</code>]</p> </li> <li> <code>include_source</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag if Python based model has to be included in analysis</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModelStatus</code>           \u2013            <p>ModelStatus of best model for given strategy or None.</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def get_best_model_status(\n    self,\n    strategies: Optional[List[RuntimeSearchStrategy]] = None,\n    include_source: bool = True,\n) -&gt; ModelStatus:\n    \"\"\"Returns ModelStatus of best model for given strategy.\n\n    Args:\n        strategies: List of strategies for finding the best model. Strategies are selected in provided order. When\n                    first fails, next strategy from the list is used. When no strategies have been provided it\n                    defaults to [`MaxThroughputAndMinLatencyStrategy`, `MinLatencyStrategy`]\n        include_source: Flag if Python based model has to be included in analysis\n\n    Returns:\n        ModelStatus of best model for given strategy or None.\n    \"\"\"\n    runtime_result = self.get_best_runtime(strategies=strategies, include_source=include_source)\n    return runtime_result.model_status\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package.get_best_runtime","title":"get_best_runtime","text":"<pre><code>get_best_runtime(strategies=None, include_source=True, inplace=False)\n</code></pre> <p>Returns best runtime for given strategy.</p> <p>Parameters:</p> <ul> <li> <code>strategies</code>               (<code>Optional[List[RuntimeSearchStrategy]]</code>, default:                   <code>None</code> )           \u2013            <p>List of strategies for finding the best model. Strategies are selected in provided order. When         first fails, next strategy from the list is used. When no strategies have been provided it         defaults to [<code>MaxThroughputAndMinLatencyStrategy</code>, <code>MinLatencyStrategy</code>]</p> </li> <li> <code>include_source</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag if Python based model has to be included in analysis</p> </li> <li> <code>inplace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>should only inplace supported runners be included in analysis</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Best runtime for given strategy.</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def get_best_runtime(\n    self,\n    strategies: Optional[List[RuntimeSearchStrategy]] = None,\n    include_source: bool = True,\n    inplace: bool = False,\n):\n    \"\"\"Returns best runtime for given strategy.\n\n    Args:\n        strategies: List of strategies for finding the best model. Strategies are selected in provided order. When\n                    first fails, next strategy from the list is used. When no strategies have been provided it\n                    defaults to [`MaxThroughputAndMinLatencyStrategy`, `MinLatencyStrategy`]\n        include_source: Flag if Python based model has to be included in analysis\n        inplace: should only inplace supported runners be included in analysis\n\n    Returns:\n        Best runtime for given strategy.\n\n    Raises:\n        ModelNavigatorRuntimeAnalyzerError when no matching results found.\n    \"\"\"\n    if strategies is None:\n        strategies = DEFAULT_RUNTIME_STRATEGIES\n\n    formats = None\n    if not include_source:\n        formats = [fmt.value for fmt in SERIALIZED_FORMATS]\n\n    runners = None\n    if inplace:\n        runners = [name for name, runner in runner_registry.items() if runner.is_inplace]\n\n    runtime_result = None\n    for strategy in strategies:\n        try:\n            runtime_result = RuntimeAnalyzer.get_runtime(\n                self.status.models_status,\n                strategy=strategy,\n                formats=formats,\n                runners=runners,\n            )\n            break\n        except ModelNavigatorRuntimeAnalyzerError:\n            LOGGER.debug(f\"No model found with strategy: {strategy}\")\n\n    if runtime_result is None:\n        raise ModelNavigatorRuntimeAnalyzerError(\"No matching results found.\")\n\n    return runtime_result\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package.get_model_path","title":"get_model_path","text":"<pre><code>get_model_path(model_key)\n</code></pre> <p>Return path of the model.</p> <p>Parameters:</p> <ul> <li> <code>model_key</code>               (<code>str</code>)           \u2013            <p>Unique key of the model.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ModelNavigatorNotFoundError</code>             \u2013            <p>When model not found.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>Path</code> )          \u2013            <p>model path</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def get_model_path(self, model_key: str) -&gt; pathlib.Path:\n    \"\"\"Return path of the model.\n\n    Args:\n        model_key (str): Unique key of the model.\n\n    Raises:\n        ModelNavigatorNotFoundError: When model not found.\n\n    Returns:\n        Path: model path\n    \"\"\"\n    try:\n        model_config = self.status.models_status[model_key].model_config\n    except KeyError:\n        raise ModelNavigatorNotFoundError(f\"Model {model_key} not found.\") from None\n    return self.workspace.path / model_config.path\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package.get_runner","title":"get_runner","text":"<pre><code>get_runner(strategies=None, include_source=True, return_type=NUMPY, device='cuda', inplace=False)\n</code></pre> <p>Get the runner according to the strategy.</p> <p>Parameters:</p> <ul> <li> <code>strategies</code>               (<code>Optional[List[RuntimeSearchStrategy]]</code>, default:                   <code>None</code> )           \u2013            <p>List of strategies for finding the best model. Strategies are selected in provided order. When         first fails, next strategy from the list is used. When no strategies have been provided it         defaults to [<code>MaxThroughputAndMinLatencyStrategy</code>, <code>MinLatencyStrategy</code>]</p> </li> <li> <code>include_source</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag if Python based model has to be included in analysis</p> </li> <li> <code>return_type</code>               (<code>TensorType</code>, default:                   <code>NUMPY</code> )           \u2013            <p>The type of the output tensor. Defaults to <code>TensorType.NUMPY</code>. If the return_type supports CUDA tensors (e.g. TensorType.TORCH) and the input tensors are on CUDA, there will be no additional data transfer between CPU and GPU.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda'</code> )           \u2013            <p>Device where model is going to be executed. Defaults to <code>\"cuda\"</code>.</p> </li> <li> <code>inplace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Indicate that runner is in inplace mode.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NavigatorRunner</code>           \u2013            <p>The optimal runner for the optimized model.</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def get_runner(\n    self,\n    strategies: Optional[List[RuntimeSearchStrategy]] = None,\n    include_source: bool = True,\n    return_type: TensorType = TensorType.NUMPY,\n    device: str = \"cuda\",\n    inplace: bool = False,\n) -&gt; NavigatorRunner:\n    \"\"\"Get the runner according to the strategy.\n\n    Args:\n        strategies: List of strategies for finding the best model. Strategies are selected in provided order. When\n                    first fails, next strategy from the list is used. When no strategies have been provided it\n                    defaults to [`MaxThroughputAndMinLatencyStrategy`, `MinLatencyStrategy`]\n        include_source: Flag if Python based model has to be included in analysis\n        return_type: The type of the output tensor. Defaults to `TensorType.NUMPY`.\n            If the return_type supports CUDA tensors (e.g. TensorType.TORCH) and the input tensors are on CUDA,\n            there will be no additional data transfer between CPU and GPU.\n        device: Device where model is going to be executed. Defaults to `\"cuda\"`.\n        inplace: Indicate that runner is in inplace mode.\n\n    Returns:\n        The optimal runner for the optimized model.\n    \"\"\"\n    runtime_result = self.get_best_runtime(strategies=strategies, include_source=include_source, inplace=inplace)\n    model_config = runtime_result.model_status.model_config\n\n    runner_config = None\n    if hasattr(runtime_result.model_status.model_config, \"runner_config\"):\n        runner_config = runtime_result.model_status.model_config.runner_config  # pytype: disable=attribute-error\n\n    runner_status = runtime_result.runner_status\n\n    if not is_source_format(model_config.format) and not (self.workspace.path / model_config.path).exists():\n        raise ModelNavigatorNotFoundError(\n            f\"The best runner expects {model_config.format.value!r} \"\n            \"model but it is not available in the loaded package.\"\n        )\n\n    if is_source_format(model_config.format) and self._model is None:\n        raise ModelNavigatorMissingSourceModelError(\n            \"The best runner uses the source model but it is not available in the loaded package. \"\n            \"Please load the source model with `package.load_source_model(model)` \"\n            \"or exclude source model from optimal runner search \"\n            \"with `package.get_runner(include_source=False)`.\"\n        )\n\n    return self._get_runner(\n        model_config.key,\n        runner_status.runner_name,\n        return_type=return_type,\n        device=device,\n        inplace=inplace,\n        runner_config=runner_config,\n    )\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package.is_empty","title":"is_empty","text":"<pre><code>is_empty()\n</code></pre> <p>Validate if package is empty - no models were produced.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if empty package, False otherwise.</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Validate if package is empty - no models were produced.\n\n    Returns:\n        True if empty package, False otherwise.\n    \"\"\"\n    for model_status in self.status.models_status.values():\n        if not is_source_format(model_status.model_config.format):\n            for runner_status in model_status.runners_status.values():\n                if (\n                    runner_status.status.get(Correctness.__name__) == CommandStatus.OK\n                    and runner_status.status.get(Performance.__name__) != CommandStatus.FAIL\n                    and (self.workspace.path / model_status.model_config.path.parent).exists()\n                ):\n                    return False\n    return True\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package.load_source_model","title":"load_source_model","text":"<pre><code>load_source_model(model)\n</code></pre> <p>Load model defined in Python code.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>object</code>)           \u2013            <p>A model object</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def load_source_model(self, model: object) -&gt; None:\n    \"\"\"Load model defined in Python code.\n\n    Args:\n        model: A model object\n    \"\"\"\n    if self._model is not None:\n        LOGGER.warning(\"Overriding existing source model.\")\n    self._model = model\n</code></pre>"},{"location":"models_optimize/package/api/package/#model_navigator.package.package.Package.save_status_file","title":"save_status_file","text":"<pre><code>save_status_file()\n</code></pre> <p>Save the status.yaml.</p> Source code in <code>model_navigator/package/package.py</code> <pre><code>def save_status_file(self) -&gt; None:\n    \"\"\"Save the status.yaml.\"\"\"\n    self._delete_status_file()\n    self._create_status_file()\n</code></pre>"},{"location":"models_optimize/package/api/package_load/","title":"Load","text":""},{"location":"models_optimize/package/api/package_load/#model_navigator.package.load","title":"model_navigator.package.load","text":"<pre><code>load(path, workspace=None)\n</code></pre> <p>Load package from provided path.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>The location of package to load</p> </li> <li> <code>workspace</code>               (<code>Optional[Union[str, Path]]</code>, default:                   <code>None</code> )           \u2013            <p>Workspace where packages will be extracted</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>           \u2013            <p>Package.</p> </li> </ul> Source code in <code>model_navigator/package/__init__.py</code> <pre><code>def load(\n    path: Union[str, pathlib.Path],\n    workspace: Optional[Union[str, pathlib.Path]] = None,\n) -&gt; Package:\n    \"\"\"Load package from provided path.\n\n    Args:\n        path: The location of package to load\n        workspace: Workspace where packages will be extracted\n\n    Returns:\n        Package.\n    \"\"\"\n    LOGGER.info(f\"Loading package from {path} to {workspace}.\")\n    workspace = Workspace(workspace)\n    workspace.initialize()\n\n    loader = PackageLoader()\n    package = loader.from_file(path=path, workspace=workspace)\n    LOGGER.info(f\"Package loaded and unpacked {workspace}.\")\n\n    return package\n</code></pre>"},{"location":"models_optimize/package/api/package_optimize/","title":"Optimize","text":""},{"location":"models_optimize/package/api/package_optimize/#model_navigator.package.optimize","title":"model_navigator.package.optimize","text":"<pre><code>optimize(\n    package,\n    target_formats=None,\n    target_device=CUDA,\n    runners=None,\n    optimization_profile=None,\n    verbose=False,\n    debug=False,\n    verify_func=None,\n    custom_configs=None,\n    defaults=True,\n    fail_on_empty=True,\n)\n</code></pre> <p>Generate target formats and run correctness and profiling tests for available runners.</p> <p>Parameters:</p> <ul> <li> <code>package</code>               (<code>Package</code>)           \u2013            <p>Package to optimize.</p> </li> <li> <code>target_formats</code>               (<code>Optional[Tuple[Union[str, Format], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Formats to generate and profile. Defaults to target formats from the package.</p> </li> <li> <code>target_device</code>               (<code>Optional[DeviceKind]</code>, default:                   <code>CUDA</code> )           \u2013            <p>Target device for optimize process, default is CUDA</p> </li> <li> <code>runners</code>               (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Runners to run correctness tests and profiling on. Defaults to runners from the package.</p> </li> <li> <code>optimization_profile</code>               (<code>Optional[OptimizationProfile]</code>, default:                   <code>None</code> )           \u2013            <p>Optimization profile used for conversion and profiling.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True enable verbose logging. Defaults to False.</p> </li> <li> <code>debug</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True print debugging logs. Defaults to False.</p> </li> <li> <code>verify_func</code>               (<code>Optional[VerifyFunction]</code>, default:                   <code>None</code> )           \u2013            <p>Function used for verifying generated models. Defaults to None.</p> </li> <li> <code>custom_configs</code>               (<code>Optional[List[CustomConfig]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom formats configuration. Defaults to None.</p> </li> <li> <code>defaults</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>reset configuration of custom configs to defaults</p> </li> <li> <code>fail_on_empty</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Fail optimization when empty (no model or base exported model) package provided</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>           \u2013            <p>Optimized package</p> </li> </ul> Source code in <code>model_navigator/package/__init__.py</code> <pre><code>def optimize(\n    package: Package,\n    target_formats: Optional[Tuple[Union[str, Format], ...]] = None,\n    target_device: Optional[DeviceKind] = DeviceKind.CUDA,\n    runners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\n    optimization_profile: Optional[OptimizationProfile] = None,\n    verbose: bool = False,\n    debug: bool = False,\n    verify_func: Optional[VerifyFunction] = None,\n    custom_configs: Optional[List[CustomConfig]] = None,\n    defaults: bool = True,\n    fail_on_empty: bool = True,\n) -&gt; Package:\n    \"\"\"Generate target formats and run correctness and profiling tests for available runners.\n\n    Args:\n        package: Package to optimize.\n        target_formats: Formats to generate and profile. Defaults to target formats from the package.\n        target_device: Target device for optimize process, default is CUDA\n        runners: Runners to run correctness tests and profiling on. Defaults to runners from the package.\n        optimization_profile: Optimization profile used for conversion and profiling.\n        verbose: If True enable verbose logging. Defaults to False.\n        debug: If True print debugging logs. Defaults to False.\n        verify_func: Function used for verifying generated models. Defaults to None.\n        custom_configs: Custom formats configuration. Defaults to None.\n        defaults: reset configuration of custom configs to defaults\n        fail_on_empty: Fail optimization when empty (no model or base exported model) package provided\n\n    Returns:\n        Optimized package\n    \"\"\"\n    if fail_on_empty and package.is_empty() and package.model is None:\n        raise ModelNavigatorEmptyPackageError(\n            \"Package is empty and source model is not loaded. Unable to run optimize.\"\n        )\n    config = package.config\n\n    is_source_available = package.model is not None\n    if target_formats is None:\n        target_formats = get_target_formats(framework=package.framework, is_source_available=is_source_available)\n\n    if runners is None:\n        runners = default_runners(device_kind=target_device)\n\n    if optimization_profile is None:\n        optimization_profile = OptimizationProfile()\n\n    _update_config(\n        config=config,\n        is_source_available=is_source_available,\n        target_formats=target_formats,\n        runners=runners,\n        optimization_profile=optimization_profile,\n        verbose=verbose,\n        debug=debug,\n        verify_func=verify_func,\n        custom_configs=custom_configs,\n        defaults=defaults,\n        target_device=target_device,\n    )\n\n    builders = _get_builders(\n        framework=package.framework,\n    )\n\n    models_config = _get_model_configs(\n        config=config,\n        custom_configs=list(config.custom_configs.values()),\n    )\n\n    optimized_package = optimize_pipeline(\n        package=package,\n        workspace=package.workspace.path,\n        builders=builders,\n        config=config,\n        models_config=models_config,\n    )\n\n    return optimized_package\n</code></pre>"},{"location":"models_optimize/package/api/package_profile/","title":"Profile","text":""},{"location":"models_optimize/package/api/package_profile/#model_navigator.package.profile","title":"model_navigator.package.profile","text":"<pre><code>profile(\n    package,\n    dataloader=None,\n    target_formats=None,\n    target_device=CUDA,\n    runners=None,\n    max_batch_size=None,\n    batch_sizes=None,\n    window_size=DEFAULT_WINDOW_SIZE,\n    stability_percentage=DEFAULT_STABILITY_PERCENTAGE,\n    stabilization_windows=DEFAULT_STABILIZATION_WINDOWS,\n    min_trials=DEFAULT_MIN_TRIALS,\n    max_trials=DEFAULT_MAX_TRIALS,\n    throughput_cutoff_threshold=DEFAULT_THROUGHPUT_CUTOFF_THRESHOLD,\n    throughput_backoff_limit=DEFAULT_THROUGHPUT_BACKOFF_LIMIT,\n    verbose=False,\n)\n</code></pre> <p>Profile provided package.</p> <p>When <code>dataloader</code> is provided, use all samples obtained from dataloader per-each batch size to perform profiling. The profiling result return the min, max and average results per batch size from all samples.</p> <p>When no <code>dataloader</code> provided, the profiling sample from package is used.</p> <p>Parameters:</p> <ul> <li> <code>package</code>               (<code>Package</code>)           \u2013            <p>Package to profile.</p> </li> <li> <code>dataloader</code>               (<code>Optional[SizedDataLoader]</code>, default:                   <code>None</code> )           \u2013            <p>Sized iterable with data that will be feed to the model</p> </li> <li> <code>target_formats</code>               (<code>Optional[Tuple[Union[str, Format], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Formats to profile. Defaults to target formats from the package.</p> </li> <li> <code>target_device</code>               (<code>Optional[DeviceKind]</code>, default:                   <code>CUDA</code> )           \u2013            <p>Target device to run profiling on.</p> </li> <li> <code>runners</code>               (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Runners to run profiling on. Defaults to runners from the package.</p> </li> <li> <code>max_batch_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Maximal batch size used for profiling. Default: None</p> </li> <li> <code>batch_sizes</code>               (<code>Optional[List[int]]</code>, default:                   <code>None</code> )           \u2013            <p>List of batch sizes to profile. Default: None</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>DEFAULT_WINDOW_SIZE</code> )           \u2013            <p>Number of inference queries performed in measurement window</p> </li> <li> <code>stability_percentage</code>               (<code>float</code>, default:                   <code>DEFAULT_STABILITY_PERCENTAGE</code> )           \u2013            <p>Allowed percentage of variation from the mean in three consecutive windows.</p> </li> <li> <code>stabilization_windows</code>               (<code>int</code>, default:                   <code>DEFAULT_STABILIZATION_WINDOWS</code> )           \u2013            <p>Number consecutive windows selected for stabilization.</p> </li> <li> <code>min_trials</code>               (<code>int</code>, default:                   <code>DEFAULT_MIN_TRIALS</code> )           \u2013            <p>Minimal number of window trials.</p> </li> <li> <code>max_trials</code>               (<code>int</code>, default:                   <code>DEFAULT_MAX_TRIALS</code> )           \u2013            <p>Maximum number of window trials.</p> </li> <li> <code>throughput_cutoff_threshold</code>               (<code>float</code>, default:                   <code>DEFAULT_THROUGHPUT_CUTOFF_THRESHOLD</code> )           \u2013            <p>Minimum throughput increase to continue profiling.</p> </li> <li> <code>throughput_backoff_limit</code>               (<code>int</code>, default:                   <code>DEFAULT_THROUGHPUT_BACKOFF_LIMIT</code> )           \u2013            <p>Back-off limit to run multiple more profiling steps to avoid stop at local minimum                       when throughput saturate based on <code>throughput_cutoff_threshold</code>.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True enable verbose logging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ProfilingResults</code>           \u2013            <p>Profiling results</p> </li> </ul> Source code in <code>model_navigator/package/__init__.py</code> <pre><code>def profile(\n    package: Package,\n    dataloader: Optional[SizedDataLoader] = None,\n    target_formats: Optional[Tuple[Union[str, Format], ...]] = None,\n    target_device: Optional[DeviceKind] = DeviceKind.CUDA,\n    runners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\n    max_batch_size: Optional[int] = None,\n    batch_sizes: Optional[List[int]] = None,\n    window_size: int = DEFAULT_WINDOW_SIZE,\n    stability_percentage: float = DEFAULT_STABILITY_PERCENTAGE,\n    stabilization_windows: int = DEFAULT_STABILIZATION_WINDOWS,\n    min_trials: int = DEFAULT_MIN_TRIALS,\n    max_trials: int = DEFAULT_MAX_TRIALS,\n    throughput_cutoff_threshold: float = DEFAULT_THROUGHPUT_CUTOFF_THRESHOLD,\n    throughput_backoff_limit: int = DEFAULT_THROUGHPUT_BACKOFF_LIMIT,\n    verbose: bool = False,\n) -&gt; ProfilingResults:\n    \"\"\"Profile provided package.\n\n    When `dataloader` is provided, use all samples obtained from dataloader per-each batch size to perform profiling.\n    The profiling result return the min, max and average results per batch size from all samples.\n\n    When no `dataloader` provided, the profiling sample from package is used.\n\n    Args:\n        package: Package to profile.\n        dataloader: Sized iterable with data that will be feed to the model\n        target_formats: Formats to profile. Defaults to target formats from the package.\n        target_device: Target device to run profiling on.\n        runners: Runners to run profiling on. Defaults to runners from the package.\n        max_batch_size: Maximal batch size used for profiling. Default: None\n        batch_sizes: List of batch sizes to profile. Default: None\n        window_size: Number of inference queries performed in measurement window\n        stability_percentage: Allowed percentage of variation from the mean in three consecutive windows.\n        stabilization_windows: Number consecutive windows selected for stabilization.\n        min_trials: Minimal number of window trials.\n        max_trials: Maximum number of window trials.\n        throughput_cutoff_threshold: Minimum throughput increase to continue profiling.\n        throughput_backoff_limit: Back-off limit to run multiple more profiling steps to avoid stop at local minimum\n                                  when throughput saturate based on `throughput_cutoff_threshold`.\n        verbose: If True enable verbose logging. Defaults to False.\n\n    Returns:\n        Profiling results\n    \"\"\"\n    if package.is_empty() and package.model is None:\n        raise ModelNavigatorEmptyPackageError(\n            \"Package is empty and source model is not loaded. Unable to run optimize.\"\n        )\n\n    config = package.config\n    is_source_available = package.model is not None\n\n    if target_formats is None:\n        target_formats = get_target_formats(framework=package.framework, is_source_available=is_source_available)\n\n    if runners is None:\n        runners = default_runners(device_kind=target_device)\n\n    if dataloader is None:\n        dataloader = []\n\n    optimization_profile = OptimizationProfile(\n        max_batch_size=max_batch_size,\n        batch_sizes=batch_sizes,\n        window_size=window_size,\n        stability_percentage=stability_percentage,\n        stabilization_windows=stabilization_windows,\n        min_trials=min_trials,\n        max_trials=max_trials,\n        throughput_cutoff_threshold=throughput_cutoff_threshold,\n        throughput_backoff_limit=throughput_backoff_limit,\n    )\n\n    _update_config(\n        config=config,\n        dataloader=dataloader,\n        is_source_available=is_source_available,\n        target_formats=target_formats,\n        runners=runners,\n        optimization_profile=optimization_profile,\n        verbose=verbose,\n        target_device=target_device,\n    )\n\n    builders = [\n        preprocessing_builder,\n        profiling_builder,\n    ]\n\n    model_configs = _get_model_configs(\n        config=config,\n        custom_configs=[],\n    )\n    profiling_results = profile_pipeline(\n        package=package,\n        config=config,\n        builders=builders,\n        models_config=model_configs,\n    )\n\n    return profiling_results\n</code></pre>"},{"location":"models_optimize/package/api/package_profile/#model_navigator.package.ProfilingResults","title":"model_navigator.package.ProfilingResults  <code>dataclass</code>","text":"<pre><code>ProfilingResults(models, samples_data)\n</code></pre> <p>Profiling results for models.</p> <p>Parameters:</p> <ul> <li> <code>models</code>               (<code>Dict[str, RunnerResults]</code>)           \u2013            <p>Mapping of models and their runner results</p> </li> </ul>"},{"location":"models_optimize/package/api/package_profile/#model_navigator.package.ProfilingResults.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Return results in form of dictionary.</p> Source code in <code>model_navigator/package/profiling_results.py</code> <pre><code>def to_dict(self):\n    \"\"\"Return results in form of dictionary.\"\"\"\n    return dataclass2dict(self)\n</code></pre>"},{"location":"models_optimize/package/api/package_profile/#model_navigator.package.ProfilingResults.to_file","title":"to_file","text":"<pre><code>to_file(path)\n</code></pre> <p>Save results to file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>A path to yaml files</p> </li> </ul> Source code in <code>model_navigator/package/profiling_results.py</code> <pre><code>def to_file(self, path: Union[str, pathlib.Path]):\n    \"\"\"Save results to file.\n\n    Args:\n        path: A path to yaml files\n    \"\"\"\n    path = pathlib.Path(path)\n    data = self.to_dict()\n    with path.open(\"w\") as f:\n        yaml.safe_dump(data, f, sort_keys=False)\n</code></pre>"},{"location":"models_optimize/package/api/package_profile/#model_navigator.package.profiling_results.RunnerResults","title":"model_navigator.package.profiling_results.RunnerResults  <code>dataclass</code>","text":"<pre><code>RunnerResults(runners)\n</code></pre> <p>Result for runners.</p> <p>Parameters:</p> <ul> <li> <code>runners</code>               (<code>Dict[str, RunnerProfilingResults]</code>)           \u2013            <p>Mapping of runner and their profiling results</p> </li> </ul>"},{"location":"models_optimize/package/api/package_profile/#model_navigator.package.profiling_results.RunnerProfilingResults","title":"model_navigator.package.profiling_results.RunnerProfilingResults  <code>dataclass</code>","text":"<pre><code>RunnerProfilingResults(status, detailed)\n</code></pre> <p>Profiling results for runner.</p> <p>Parameters:</p> <ul> <li> <code>status</code>               (<code>CommandStatus</code>)           \u2013            <p>Status of profiling execution</p> </li> <li> <code>detailed</code>               (<code>Dict[int, List[ProfilingResult]]</code>)           \u2013            <p>Result mapping - per sample id</p> </li> </ul>"},{"location":"models_optimize/package/api/package_profile/#model_navigator.package.profiling_results.ProfilingResult","title":"model_navigator.package.profiling_results.ProfilingResult  <code>dataclass</code>","text":"<pre><code>ProfilingResult(\n    batch_size,\n    avg_latency,\n    std_latency,\n    p50_latency,\n    p90_latency,\n    p95_latency,\n    p99_latency,\n    throughput,\n    avg_gpu_clock,\n    request_count,\n)\n</code></pre> <p>Result for single profiling for sample.</p> <p>Parameters:</p> <ul> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Size of batch used for profiling</p> </li> <li> <code>avg_latency</code>               (<code>float</code>)           \u2013            <p>Average latency of profiling</p> </li> <li> <code>std_latency</code>               (<code>float</code>)           \u2013            <p>Standard deviation of profiled latency</p> </li> <li> <code>p50_latency</code>               (<code>float</code>)           \u2013            <p>50th percentile of measured latency</p> </li> <li> <code>p90_latency</code>               (<code>float</code>)           \u2013            <p>90th percentile of measured latency</p> </li> <li> <code>p95_latency</code>               (<code>float</code>)           \u2013            <p>95th percentile of measured latency</p> </li> <li> <code>p99_latency</code>               (<code>float</code>)           \u2013            <p>99th percentile of measured latency</p> </li> <li> <code>throughput</code>               (<code>float</code>)           \u2013            <p>Inferences per second</p> </li> <li> <code>request_count</code>               (<code>int</code>)           \u2013            <p>Number of inference requests</p> </li> </ul>"},{"location":"models_optimize/package/api/package_save/","title":"Save","text":""},{"location":"models_optimize/package/api/package_save/#model_navigator.package.save","title":"model_navigator.package.save","text":"<pre><code>save(package, path, override=False, save_data=True)\n</code></pre> <p>Save export results into the .nav package at given path.</p> <p>Parameters:</p> <ul> <li> <code>package</code>               (<code>Package</code>)           \u2013            <p>A package object to prepare the package</p> </li> <li> <code>path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>A path to file where the package has to be saved</p> </li> <li> <code>override</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>flag to override existing package in provided path</p> </li> <li> <code>save_data</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>disable saving samples from the dataloader</p> </li> </ul> Source code in <code>model_navigator/package/__init__.py</code> <pre><code>def save(\n    package: Package,\n    path: Union[str, pathlib.Path],\n    override: bool = False,\n    save_data: bool = True,\n) -&gt; None:\n    \"\"\"Save export results into the .nav package at given path.\n\n    Args:\n        package: A package object to prepare the package\n        path: A path to file where the package has to be saved\n        override: flag to override existing package in provided path\n        save_data: disable saving samples from the dataloader\n    \"\"\"\n    builder = PackageBuilder()\n    builder.save(\n        package=package,\n        path=path,\n        override=override,\n        save_data=save_data,\n    )\n</code></pre>"},{"location":"pipelines_optimize/optimize/optimize/","title":"Optimize Pipelines","text":""},{"location":"pipelines_optimize/optimize/optimize/#optimize-pipelines","title":"Optimize Pipelines","text":"<p>The majority of Generative AI models consist of multiple DL pipelines orchestrated through Python code. The models like Stable Diffusion or HuggingFace pipelines represent how the modern, complex models are developed and deployed directly from Python.</p>"},{"location":"pipelines_optimize/optimize/optimize/#overview-of-inplace-optimize","title":"Overview of Inplace Optimize","text":"<p>The Inplace Optimize feature of the Triton Model Navigator offers a PyTorch-specific solution that seamlessly substitutes <code>nn.Module</code> objects in your code with enhanced and optimised models.</p> <p>The Triton Model Navigator Inplace Optimize provides a smooth way of optimizing the model to TensorRT or Torch-TensorRT under single coherent API directly in your Python source code.</p> <p>This process is centered around the <code>nav.Module</code> wrapper, which is used to decorate your pipeline models. It initiates the optimization across the entire pipeline when paired with the appropriate dataloader.</p> <p>The Triton Model Navigator diligently audits the decorated modules, gathering essential metadata about the inputs and outputs. It then commences the optimization process, akin to that used for individual model optimization. Ultimately, it replaces the original model with its optimized version directly within the codebase.</p> <p>The concept is built around the callable and dataloader:</p> <ul> <li><code>callable</code> - a Python object, function or callable with 1 or more wrapped models to optimize.</li> <li><code>dataloader</code> - a method or class generating input data. The data is utilized to perform export and conversion, as well   as determine the maximum and minimum shapes of the model inputs and create output samples that are used during   the optimization process.</li> </ul>"},{"location":"pipelines_optimize/optimize/optimize/#optimizing-stable-diffusion-pipeline","title":"Optimizing Stable Diffusion pipeline","text":"<p>The below code presents Stable Diffusion pipeline optimization. First, initialize pipeline and wrap the model components with <code>nav.Module</code>:</p> <pre><code>import model_navigator as nav\nfrom transformers.modeling_outputs import BaseModelOutputWithPooling\nfrom diffusers import DPMSolverMultistepScheduler, StableDiffusionPipeline\n\n\ndef get_pipeline():\n    # Initialize Stable Diffusion pipeline and wrap modules for optimization\n    pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\")\n    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe = pipe.to(\"cuda\")\n\n    pipe.text_encoder = nav.Module(\n        pipe.text_encoder,\n        name=\"clip\",\n        output_mapping=lambda output: BaseModelOutputWithPooling(**output), # Mapping to convert output data to HuggingFace class\n    )\n    pipe.unet = nav.Module(\n        pipe.unet,\n        name=\"unet\",\n    )\n    pipe.vae.decoder = nav.Module(\n        pipe.vae.decoder,\n        name=\"vae\",\n    )\n\n    return pipe\n</code></pre> <p>Prepare a simple dataloader:</p> <pre><code>def get_dataloader():\n    # Please mind, the first element in tuple need to be a batch size\n    return [(1, \"a photo of an astronaut riding a horse on mars\")]\n</code></pre> <p>Execute model optimization:</p> <pre><code>pipe = get_pipeline()\ndataloader = get_dataloader()\n\nnav.optimize(pipe, dataloader)\n</code></pre> <p>Review all possible options in the optimize API.</p>"},{"location":"pipelines_optimize/optimize/optimize/#optimizing-a-quantized-module","title":"Optimizing a quantized module","text":"<p>If you don not specify any configuration for <code>nav.optimize</code> function, Module Navigator will try to optimize with some sensible defaults. Still, you can add some arguments to give hints for the optimization process like <code>batching</code> or <code>precision</code>. One usefull scenario may be related to pipelines with different modules' precisions due to quantization. In such a case: - load the whole pipeline in <code>fp16</code> - quantize the most time consuming module e.g. unet into <code>int8</code> precision using for example NVIDIA TensorRT Model Optimizer library - let Model Navigator optimize the whole pipeline</p> <p>The sample code snippet below shows how to achieve that. Notice that for clarity we assumed we have a given function to quantize <code>quantize_int_8_bits</code> and the quantized module takes an extra parameter specifying its precision i.e. <code>precision=\"int8\"</code>.</p> <p>Model Navigator will try by default export <code>text_encoder</code> and <code>vae.decoder</code> into <code>fp32</code> and <code>fp16</code> TensorRT precisions but the <code>unet</code> will be exported into <code>int8</code>.</p> <pre><code>pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\",\n                                                        torch_dtype=torch.float16,\n                                                        variant=\"fp16\").to(\"cuda\")\n\npipe.unet = quantize_int_8_bits(pipe.unet)\n\npipe.text_encoder = nav.Module(\n    pipe.text_encoder,\n    name=\"clip\",\n    output_mapping=lambda output: BaseModelOutputWithPooling(**output), # Mapping to convert output data to HuggingFace class\n)\npipe.unet = nav.Module(\n    pipe.unet,\n    name=\"unet\",\n    precision=\"int8\",\n)\npipe.vae.decoder = nav.Module(\n    pipe.vae.decoder,\n    name=\"vae\",\n)\n</code></pre> <p>If you would like to have more granular control over the configuration you can either create it for the whole pipeline i.e. <pre><code>nav.optimize(model, dataloader, config=config)\n</code></pre> or specify it for each module.</p>"},{"location":"pipelines_optimize/optimize/optimize/#optimizing-resnet18-model","title":"Optimizing ResNet18 model","text":"<p>The Inplace Optimize can be easily used to optimize a single <code>nn.Module</code>. The below example shows how to optimize a ResNet18 model from TorchHub.</p> <p>First, initialize model from TorchHub: <pre><code>import torch\n\nresnet18 = torch.hub.load(\"pytorch/vision:v0.10.0\", \"resnet18\", pretrained=True).to(\"cuda\").eval()\n</code></pre></p> <p>Next, define a simple dataloader: <pre><code>dataloader = [(1, torch.rand(1, 3, 224, 224, device=\"cuda\")) for _ in range(150)]\n</code></pre></p> <p>Finally, wrap the model and run optimize: <pre><code>import model_navigator as nav\n\nresnet18 = nav.Module(resnet18, name=\"resnet18\")\nnav.optimize(resnet18, dataloader)\n</code></pre></p>"},{"location":"pipelines_optimize/optimize/optimize/#loading-optimized-modules","title":"Loading optimized modules","text":"<p>Once the pipeline or model has been optimized, you can load explicit the most performant version of the modules executing:</p> <pre><code>nav.load_optimized()\n</code></pre> <p>After executing this method, when the optimized version of module exists, it will be used in your pipeline execution directly in Python.</p>"},{"location":"pipelines_optimize/optimize/optimize/#deploying-optimized-pipeline-or-model","title":"Deploying optimized pipeline or model","text":"<p>Once optimization is done, you can use the pipeline for deployment directly from Python. The example how to serve Stable Diffusion pipeline through PyTriton can be found here.</p>"},{"location":"pipelines_optimize/optimize/optimize/#per-module-configuration","title":"Per module configuration","text":"<p><code>nav.optimize</code> sets its configuration to all pipeline modules that do not have the configuration already specified. So, if you need a different configuration for a given module, just set the <code>module.optimize_config</code> property.</p> <pre><code>pipe = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(\"nvidia/parakeet-ctc-0.6b\")\n\npipe.encoder = nav.Module(pipe.encoder, name=\"encoder\")\npipe.encoder.optimize_config = nav.OptimizeConfig(\n    target_formats=(\n        nav.Format.TENSORRT,\n    ),\n    runners=(\n        \"TensorRT\",\n    )\n)\n\npipe.decoder = nav.Module(pipe.decoder, name=\"decoder\")\npipe.decoder.optimize_config = nav.OptimizeConfig(\n    target_formats=(\n        nav.Format.TENSORRT,\n        nav.Format.ONNX,\n    ),\n    runners=(\n        \"TensorRT\",\n        \"OnnxCUDA\", # try also other runner\n    )\n)\n\nnav.optimize(pipe, dataloader)\n</code></pre>"},{"location":"pipelines_optimize/optimize/optimize/#model-custom-forwarding-function","title":"Model custom forwarding function","text":"<p>Model Navigator expects that the models are using <code>__call__</code> function to propagate the data through the model as it binds to this method. If the model is not using <code>__call__</code> function, you can wrap the model with <code>nav.Module</code> and set the <code>forward_func_name</code> argument to the function that is actually used, that will allow Model Navigator to collect data for optimization.</p> <p>The problem is not visible at first glance, but it can present itself when running <code>nav.optimize</code> with following error:</p> <pre><code>FileNotFoundError Traceback (most recent call last)\n...\n1 nav.optimize(model.encode, dataloader, config)\n...\nFileNotFoundError: [Errno 2] No such file or directory: '/home/usr/.cache/model_navigator/transformer'\n</code></pre> <p>That may mean Model Navigator was not called properly and did not save any input/output data for the optimization.</p> <p>In below example, we want to use <code>encode</code> function as it contains \"complicated\" preprocessing of input data. But we see that <code>forward</code> function is used instead of <code>__call__</code> which will cause the error in the optimization. That is why we instruct <code>nav.Module</code> to use non-standard function.</p> <pre><code>class SentenceTransformer(nn.Module):\n\n    def forward(self, x):\n        return x\n\n    def encode(self, x):\n        x1 = self.preprocessing(x)\n\n        x2 = self.forward(x1) # instead of self(1)\n\n        return x2\n\n    def preprocessing(self, x):\n        return x + 1\n\n# run optimization in the parent process only\n# wrapping the module for optimization, with non-standard forward function\npipe = nav.Module(SentenceTransformer(), name=\"transformer\", forward_func=\"forward\")\n\n# we want to use the encode function as it contains preprocessing step and maybe other important steps\nnav.optimize(pipe.encode, dataloader, config)\n</code></pre>"},{"location":"pipelines_optimize/optimize/optimize/#error-isolation-when-running-python-script","title":"Error isolation when running Python script","text":"<p>Important: Please review below section to prevent unexpected issues when running <code>optimize</code>.</p> <p>For better error isolation, some conversions and exports are run in separate child processes using multiprocessing in the <code>spawn</code> mode. This means that everything in a global scope will be run in a child process. You can encounter unexpected issue when the optimization code is place in Python script and executed as: <pre><code>python optimize.py\n</code></pre> To prevent nested optimization, you have to either put the optimize code in: <pre><code>if __name__ == \"__main__\":\n    # optimization goes here\n</code></pre> or <pre><code>import multiprocessing as mp\nif mp.current_process().name == \"MainProcess\":\n    # optimization goes here\n</code></pre></p> <p>If none of the above works for you, you can run all optimization in a single process at the cost of error isolation by setting the following environment variable: <pre><code>NAVIGATOR_USE_MULTIPROCESSING=False\n</code></pre></p>"},{"location":"pipelines_optimize/optimize/optimize/#detailed-optimization-report","title":"Detailed optimization report","text":"<p>Model navigator tries to hide the conversion and export complexity under the hood so that you can quickly start optimizing a model. For advanced use cases where you would like to have better insights into the process, you can set the environment variable: <pre><code>NAVIGATOR_CONSOLE_OUTPUT=detailed\n</code></pre></p> <p>After each optimization of a model or module (in inplace mode) Model Navigator will print an optimization status table with each detailed step.</p> <p>Another option is to take a look at the log files in the <code>workspace/module/navigator.log</code> where you can find detailed logs and summary table of the whole process.</p> <p>If you would like to see the logs on the console during the optimization, set the following property:</p> <pre><code>NAVIGATOR_CONSOLE_OUTPUT=logs\n</code></pre>"},{"location":"pipelines_optimize/optimize/optimize/#pytorch-optimization","title":"PyTorch Optimization","text":""},{"location":"pipelines_optimize/optimize/optimize/#torch-compile-configuration","title":"Torch Compile Configuration","text":""},{"location":"pipelines_optimize/optimize/optimize/#cache-configuration","title":"Cache Configuration","text":"<p>Model Navigator supports caching for torch.compile to improve subsequent compilation times through environment variables:</p> <pre><code># TORCHINDUCTOR_CACHE_DIR is set - overriden - to the navigator workspace directory by default\n\n# Enable additional caching features\nexport TORCHINDUCTOR_FX_GRAPH_CACHE=1   # Cache FX graph transformations\nexport TORCHINDUCTOR_AUTOGRAD_CACHE=1   # Cache autograd optimizations\n</code></pre> <p>These settings can significantly reduce compilation time for repeated runs with the same model configuration.</p>"},{"location":"pipelines_optimize/optimize/api/config/","title":"Config","text":""},{"location":"pipelines_optimize/optimize/api/config/#classes-enums-and-types-used-to-configure-inplace-optimize","title":"Classes, enums and types used to configure Inplace Optimize","text":""},{"location":"pipelines_optimize/optimize/api/config/#model_navigator.InplaceConfig","title":"model_navigator.InplaceConfig","text":"<pre><code>InplaceConfig()\n</code></pre> <p>Inplace Optimize configuration.</p> <p>Initialize InplaceConfig.</p> Source code in <code>model_navigator/inplace/config.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize InplaceConfig.\"\"\"\n    self._cache_dir: pathlib.Path = inplace_cache_dir()\n    self._min_num_samples: int = DEFAULT_MIN_NUM_SAMPLES\n    self._max_num_samples_stored: int = DEFAULT_MAX_NUM_SAMPLES_STORED\n    self.strategies: List[RuntimeSearchStrategy] = [MaxThroughputAndMinLatencyStrategy(), MinLatencyStrategy()]\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/config/#model_navigator.InplaceConfig.cache_dir","title":"cache_dir  <code>property</code> <code>writable</code>","text":"<pre><code>cache_dir\n</code></pre> <p>Get the cache directory.</p>"},{"location":"pipelines_optimize/optimize/api/config/#model_navigator.InplaceConfig.max_num_samples_stored","title":"max_num_samples_stored  <code>property</code> <code>writable</code>","text":"<pre><code>max_num_samples_stored\n</code></pre> <p>Get the minimum number of samples to collect before optimizing.</p>"},{"location":"pipelines_optimize/optimize/api/config/#model_navigator.InplaceConfig.min_num_samples","title":"min_num_samples  <code>property</code> <code>writable</code>","text":"<pre><code>min_num_samples\n</code></pre> <p>Get the minimum number of samples to collect before optimizing.</p>"},{"location":"pipelines_optimize/optimize/api/module/","title":"Module","text":""},{"location":"pipelines_optimize/optimize/api/module/#inplace-optimize-module","title":"Inplace Optimize Module","text":""},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module","title":"model_navigator.Module","text":"<pre><code>Module(\n    module,\n    name=None,\n    input_mapping=None,\n    output_mapping=None,\n    timer=None,\n    forward_func=None,\n    batching=None,\n    precision=\"fp32\",\n    model_path=None,\n)\n</code></pre> <p>               Bases: <code>ObjectProxy</code></p> <p>Inplace Optimize module wrapper.</p> <p>This class wraps a torch module and provides inplace optimization functionality. Depending on the configuration set in config, the module will be optimized, recorded, or passed through.</p> <p>This wrapper can be used in place of a torch module, and will behave identically to the original module.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>torch module to wrap.</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>module name.</p> </li> <li> <code>input_mapping</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>function to map module inputs to the expected input.</p> </li> <li> <code>output_mapping</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>function to map module outputs to the expected output.</p> </li> <li> <code>forward_func</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>forwarding function name used by the module, if None, the module call is used.</p> </li> <li> <code>batching</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>enable or disable batching on first (index 0) dimension of the model</p> </li> <li> <code>precision</code>               (<code>PrecisionType</code>, default:                   <code>'fp32'</code> )           \u2013            <p>precision of the module</p> </li> <li> <code>model_path</code>               (<code>Optional[Union[str, Path]]</code>, default:                   <code>None</code> )           \u2013            <p>optional path to ONNX or TensorRT model file, if provided the model will be loaded from the file instead of converting</p> </li> </ul> Note <p>batching if specified takes precedence over corresponding values in the configuration specified in nav.profile.</p> Example <p>import torch import model_navigator as nav model = torch.nn.Linear(10, 10) model = nav.Module(model)</p> <p>Initialize Module.</p> Source code in <code>model_navigator/inplace/wrapper.py</code> <pre><code>def __init__(\n    self,\n    module: \"torch.nn.Module\",\n    name: Optional[str] = None,\n    input_mapping: Optional[Callable] = None,\n    output_mapping: Optional[Callable] = None,\n    timer: Optional[Timer] = None,\n    forward_func: Optional[str] = None,\n    batching: Optional[bool] = None,\n    precision: PrecisionType = \"fp32\",\n    model_path: Optional[Union[str, pathlib.Path]] = None,\n) -&gt; None:\n    \"\"\"Initialize Module.\"\"\"\n    super().__init__(module)\n    if not isinstance(module, torch.nn.Module):\n        raise ModelNavigatorUserInputError(\"Only torch modules are supported.\")\n\n    self._name = name or get_object_name(module)\n    self._input_mapping = input_mapping or (lambda x: x)\n    self._output_mapping = output_mapping or (lambda x: x)\n    self._optimize_config = None\n\n    if timer:\n        self.add_timer(timer=timer)\n    else:\n        self._module_timer = None\n\n    current_forward = None\n    if forward_func:\n        try:\n            current_forward = getattr(module, forward_func)\n        except AttributeError as e:\n            raise ModelNavigatorUserInputError(f\"Forward method must exist, got {forward_func}.\") from e\n        setattr(module, forward_func, lambda *args, **kwargs: Module.__call__(self, *args, **kwargs))\n\n    self.batching = batching\n    self.precision = precision\n\n    if isinstance(model_path, str):\n        self.model_path = pathlib.Path(model_path)\n    else:\n        self.model_path = model_path\n\n    if self.model_path is not None and self.model_path.suffix not in [\n        \".onnx\",\n        \".plan\",\n    ]:  # pytype: disable=attribute-error\n        raise ModelNavigatorUserInputError(\n            f\"model_path must be either ONNX or TensorRT model file with .onnx or .plan extension, got {self.model_path}.\"\n        )\n\n    self._device = get_module_device(module) or torch.device(\"cpu\")\n    self._wrapper = RecordingModule(\n        module,\n        # OptimizeConfig(),\n        self._name,\n        self._input_mapping,\n        self._output_mapping,\n        forward=current_forward,\n    )\n    module_registry.register(self._name, self)\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.is_optimized","title":"is_optimized  <code>property</code>","text":"<pre><code>is_optimized\n</code></pre> <p>Check if the module is optimized.</p>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.is_ready_for_optimization","title":"is_ready_for_optimization  <code>property</code>","text":"<pre><code>is_ready_for_optimization\n</code></pre> <p>Check if the module is ready for optimization.</p>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Module name.</p>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.optimize_config","title":"optimize_config  <code>property</code> <code>writable</code>","text":"<pre><code>optimize_config\n</code></pre> <p>Module optimize config.</p>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.wrapper","title":"wrapper  <code>property</code>","text":"<pre><code>wrapper\n</code></pre> <p>Return the wrapper module.</p>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Call the wrapped module.</p> <p>This method overrides the call method of the wrapped module. If the module is already optimized it is replaced with the optimized one.</p> Source code in <code>model_navigator/inplace/wrapper.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; Any:\n    \"\"\"Call the wrapped module.\n\n    This method overrides the __call__ method of the wrapped module.\n    If the module is already optimized it is replaced with the optimized one.\n    \"\"\"\n    if self._module_timer and self._module_timer.enabled:\n        with self._module_timer:\n            output = self._wrapper(*args, **kwargs)\n            if isinstance(self, torch.nn.Module) and torch.cuda.is_available():\n                torch.cuda.synchronize()\n    else:\n        output = self._wrapper(*args, **kwargs)\n\n    return output\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module._override_config_with_module_tags","title":"_override_config_with_module_tags","text":"<pre><code>_override_config_with_module_tags(config)\n</code></pre> <p>Overrides given configuration.</p> <p>Overridden parameters:     batching     precision     model_path     model_precision</p> <p>Note: - batching is overridden if specified during model initialization - precision is applied only if TensortRT custom configuration have not been already specified.</p> Source code in <code>model_navigator/inplace/wrapper.py</code> <pre><code>def _override_config_with_module_tags(self, config: OptimizeConfig):\n    \"\"\"Overrides given configuration.\n\n      Overridden parameters:\n        batching\n        precision\n        model_path\n        model_precision\n\n    Note:\n    - batching is overridden if specified during model initialization\n    - precision is applied only if TensortRT custom configuration have not been already specified.\n    \"\"\"\n    if self.batching is not None:\n        config.batching = self.batching\n\n    if self.precision:\n        config.model_precision = self.precision\n\n    config.custom_configs = config.custom_configs or []\n    trt_config_provided = False\n    for cc in config.custom_configs:\n        if isinstance(cc, CustomConfigForTensorRT):\n            trt_config_provided = True\n            break\n    if not trt_config_provided:\n        precision = (\"fp32\", \"fp16\") if self.precision == \"fp32\" else self.precision\n        new_trt_config = TensorRTConfig(precision=precision, precision_mode=TensorRTPrecisionMode.HIERARCHY)\n        config.custom_configs = list(config.custom_configs) + [new_trt_config]\n\n    if self.model_path:\n        if self.model_path.suffix == \".onnx\":\n            config_class = OnnxConfig\n        elif self.model_path.suffix == \".plan\":\n            config_class = TensorRTConfig\n\n        config_provided = False\n        for cc in config.custom_configs:\n            if isinstance(cc, config_class):\n                cc.model_path = self.model_path\n                config_provided = True\n                break\n        if not config_provided:\n            config.custom_configs = list(config.custom_configs) + [config_class(model_path=self.model_path)]\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.add_timer","title":"add_timer","text":"<pre><code>add_timer(timer)\n</code></pre> <p>Add timer to module.</p> Source code in <code>model_navigator/inplace/wrapper.py</code> <pre><code>def add_timer(self, timer: Timer) -&gt; None:\n    \"\"\"Add timer to module.\"\"\"\n    self._module_timer = timer.register_module(self._name)\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.load_eager","title":"load_eager","text":"<pre><code>load_eager(device=None)\n</code></pre> <p>Load eager module.</p> Source code in <code>model_navigator/inplace/wrapper.py</code> <pre><code>@deactivate_wrapper\ndef load_eager(self, device: Optional[str] = None) -&gt; None:\n    \"\"\"Load eager module.\"\"\"\n    self._wrapper = EagerModule(\n        module=self._wrapper.module,\n        name=self._name,\n        input_mapping=self._input_mapping,\n        output_mapping=self._output_mapping,\n        optimize_config=self._optimize_config,\n        device=device or self._device,\n        forward=self._wrapper.forward_call,\n    )\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.load_optimized","title":"load_optimized","text":"<pre><code>load_optimized(strategies=None, device='cuda', activate_runners=True)\n</code></pre> <p>Load optimized module.</p> <p>Parameters:</p> <ul> <li> <code>strategies</code>               (<code>Optional[List[RuntimeSearchStrategy]]</code>, default:                   <code>None</code> )           \u2013            <p>List of strategies for finding the best model. Strategies are selected in provided order. When         first fails, next strategy from the list is used. When no strategies have been provided it         defaults to [<code>MaxThroughputAndMinLatencyStrategy</code>, <code>MinLatencyStrategy</code>]</p> </li> <li> <code>device</code>               (<code>Union[str, device]</code>, default:                   <code>'cuda'</code> )           \u2013            <p>Device on which optimized modules would be loaded. Defaults to \"cuda\".</p> </li> <li> <code>activate_runners</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Activate models - load on device. Defaults to True.</p> </li> </ul> Source code in <code>model_navigator/inplace/wrapper.py</code> <pre><code>@deactivate_wrapper\ndef load_optimized(\n    self,\n    strategies: Optional[List[RuntimeSearchStrategy]] = None,\n    device: Union[str, \"torch.device\"] = \"cuda\",\n    activate_runners: bool = True,\n) -&gt; None:\n    \"\"\"Load optimized module.\n\n    Args:\n        strategies: List of strategies for finding the best model. Strategies are selected in provided order. When\n                    first fails, next strategy from the list is used. When no strategies have been provided it\n                    defaults to [`MaxThroughputAndMinLatencyStrategy`, `MinLatencyStrategy`]\n        device: Device on which optimized modules would be loaded. Defaults to \"cuda\".\n        activate_runners: Activate models - load on device. Defaults to True.\n    \"\"\"\n    self._wrapper = OptimizedModule(\n        module=self._wrapper.module,\n        optimize_config=self._optimize_config,\n        name=self._name,\n        input_mapping=self._input_mapping,\n        output_mapping=self._output_mapping,\n        strategies=strategies,\n        activate_runners=activate_runners,\n        device=str(device),\n        forward=self._wrapper.forward_call,\n    )\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.load_recorded","title":"load_recorded","text":"<pre><code>load_recorded()\n</code></pre> <p>Load recorded module.</p> Source code in <code>model_navigator/inplace/wrapper.py</code> <pre><code>@deactivate_wrapper\ndef load_recorded(self) -&gt; None:\n    \"\"\"Load recorded module.\"\"\"\n    self._wrapper = RecordingModule(\n        module=self._wrapper.module,\n        name=self._name,\n        input_mapping=self._input_mapping,\n        output_mapping=self._output_mapping,\n        optimize_config=self._optimize_config,\n        forward=self._wrapper.forward_call,\n    )\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.optimize","title":"optimize","text":"<pre><code>optimize()\n</code></pre> <p>Optimize the module.</p> Source code in <code>model_navigator/inplace/wrapper.py</code> <pre><code>def optimize(self) -&gt; None:\n    \"\"\"Optimize the module.\"\"\"\n    assert isinstance(self.wrapper, RecordingModule), f\"Module {self.name} must be in recording mode to optimize.\"\n    assert not self.is_optimized, f\"Module {self.name} is already optimized.\"\n    assert hasattr(self.wrapper, \"optimize\"), f\"Module {self.name} does not have an optimize method.\"\n\n    self._wrapper.optimize()\n    self.load_optimized(activate_runners=False)\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/module/#model_navigator.Module.triton_model_store","title":"triton_model_store","text":"<pre><code>triton_model_store(\n    model_repository_path,\n    strategies=None,\n    model_name=None,\n    model_version=1,\n    response_cache=False,\n    warmup=False,\n    package_idx=-1,\n)\n</code></pre> <p>Store the optimized module in the Triton model store.</p> <p>Parameters:</p> <ul> <li> <code>model_repository_path</code>               (<code>Path</code>)           \u2013            <p>Path to store the optimized module.</p> </li> <li> <code>strategies</code>               (<code>Optional[List[RuntimeSearchStrategy]]</code>, default:                   <code>None</code> )           \u2013            <p>List of strategies for finding the best model.     Strategies are selected in provided order. When first fails, next strategy from the list is used.     When no strategies have been provided it defaults to [<code>MaxThroughputAndMinLatencyStrategy</code>, <code>MinLatencyStrategy</code>]</p> </li> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name of the module to use in the Triton model store, by default the module name is used.</p> </li> <li> <code>model_version</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Version of model that is deployed</p> </li> <li> <code>response_cache(bool)</code>           \u2013            <p>Enable response cache for model</p> </li> <li> <code>warmup</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable warmup for min and max batch size</p> </li> <li> <code>package_idx</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Index of package - pipeline execution status - to use for storing in Triton model store. Default is -1, which means the last package.</p> </li> </ul> Source code in <code>model_navigator/inplace/wrapper.py</code> <pre><code>def triton_model_store(\n    self,\n    model_repository_path: pathlib.Path,\n    strategies: Optional[List[RuntimeSearchStrategy]] = None,\n    model_name: Optional[str] = None,\n    model_version: int = 1,\n    response_cache: bool = False,\n    warmup: bool = False,\n    package_idx: int = -1,\n):\n    \"\"\"Store the optimized module in the Triton model store.\n\n    Args:\n        model_repository_path (pathlib.Path): Path to store the optimized module.\n        strategies (Optional[List[RuntimeSearchStrategy]]): List of strategies for finding the best model.\n                Strategies are selected in provided order. When first fails, next strategy from the list is used.\n                When no strategies have been provided it defaults to [`MaxThroughputAndMinLatencyStrategy`, `MinLatencyStrategy`]\n        model_name (Optional[str]): Name of the module to use in the Triton model store, by default the module name is used.\n        model_version (int): Version of model that is deployed\n        response_cache(bool): Enable response cache for model\n        warmup (bool): Enable warmup for min and max batch size\n        package_idx (int): Index of package - pipeline execution status - to use for storing in Triton model store. Default is -1, which means the last package.\n    \"\"\"\n    if not isinstance(self._wrapper, OptimizedModule):\n        raise ModelNavigatorUserInputError(\n            f\"Module {self.name} must be optimized to store in Triton model store. Did you load_optimized()?\"\n        )\n\n    if len(self._wrapper.packages) == 0:\n        raise ModelNavigatorUserInputError(\n            f\"Module {self.name} must have packages to store in Triton model store. Did you optimize the module?\"\n        )\n\n    try:\n        package = self._wrapper.packages[package_idx]\n    except IndexError as e:\n        raise ModelNavigatorUserInputError(\n            f\"Incorrect package index {package_idx=} for module {self.name}. There are only {len(self._wrapper.packages)} packages.\"\n        ) from e\n\n    model_name = model_name or self.name\n\n    model_repository.add_model_from_package(\n        model_repository_path,\n        model_name,\n        package,\n        strategies=strategies,\n        model_version=model_version,\n        response_cache=response_cache,\n        warmup=warmup,\n    )\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/optimize/","title":"Optimize","text":""},{"location":"pipelines_optimize/optimize/api/optimize/#inplace-optimize","title":"Inplace Optimize","text":""},{"location":"pipelines_optimize/optimize/api/optimize/#model_navigator.optimize","title":"model_navigator.optimize","text":"<pre><code>optimize(func, dataloader, config=None)\n</code></pre> <p>Optimize registered modules executed in scope of callable.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable</code>)           \u2013            <p>Callable in scope of which optimize is executed.</p> </li> <li> <code>dataloader</code>               (<code>Sequence[Tuple[int, Any]]</code>)           \u2013            <p>List of tuples with batch size and input.</p> </li> <li> <code>config</code>               (<code>Optional[OptimizeConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Optimize config.</p> </li> </ul> Source code in <code>model_navigator/inplace/__init__.py</code> <pre><code>def optimize(\n    func: Callable,\n    dataloader: Sequence[Tuple[int, Any]],\n    config: Optional[OptimizeConfig] = None,\n) -&gt; InplaceOptimizeStatus:\n    \"\"\"Optimize registered modules executed in scope of callable.\n\n    Args:\n        func:  Callable in scope of which optimize is executed.\n        dataloader: List of tuples with batch size and input.\n        config: Optimize config.\n    \"\"\"\n    try:\n        with ctx.global_context.temporary() as tmp_ctx:\n            tmp_ctx.set(ctx.INPLACE_OPTIMIZE_STRATEGIES_CONTEXT_KEY, inplace_config.strategies)\n            tmp_ctx.set(ctx.INPLACE_OPTIMIZE_KEY, True)\n            if config is None:\n                config = OptimizeConfig()\n\n            for m in module_registry.values():\n                # set main config if user did not provide one for a module\n                if m.optimize_config is None:\n                    m.optimize_config = config\n                m.load_recorded()\n\n            for input_ in dataloader:\n                batch_size, sample = input_  # unpack batch_size and sample\n                tmp_ctx.set(ctx.INPLACE_OPTIMIZE_BATCH_CONTEXT_KEY, batch_size)\n                if not isinstance(sample, (list, tuple)):\n                    sample = (sample,)\n                if not isinstance(sample[-1], dict):\n                    sample = (*sample, {})\n                *args, kwargs = sample\n                func(*args, **kwargs)\n\n            module_registry.optimize()\n            return _build_optimize_status()\n    except Exception as e:\n        raise e\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/optimize/#model_navigator.OptimizeConfig","title":"model_navigator.OptimizeConfig  <code>dataclass</code>","text":"<pre><code>OptimizeConfig(\n    sample_count=DEFAULT_SAMPLE_COUNT,\n    batching=True,\n    input_names=None,\n    output_names=None,\n    target_formats=None,\n    target_device=CUDA,\n    runners=None,\n    optimization_profile=None,\n    workspace=None,\n    verbose=False,\n    debug=False,\n    verify_func=None,\n    custom_configs=None,\n    model_precision=None,\n)\n</code></pre> <p>Configuration for inplace Optimize.</p> <p>Parameters:</p> <ul> <li> <code>sample_count</code>               (<code>int</code>, default:                   <code>DEFAULT_SAMPLE_COUNT</code> )           \u2013            <p>Limits how many samples will be used from dataloader</p> </li> <li> <code>batching</code>               (<code>Optional[bool]</code>, default:                   <code>True</code> )           \u2013            <p>Enable or disable batching on first (index 0) dimension of the model</p> </li> <li> <code>input_names</code>               (<code>Optional[Tuple[str, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Model input names</p> </li> <li> <code>output_names</code>               (<code>Optional[Tuple[str, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Model output names</p> </li> <li> <code>target_formats</code>               (<code>Optional[Tuple[Union[str, Format], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Target model formats for optimize process</p> </li> <li> <code>target_device</code>               (<code>Optional[DeviceKind]</code>, default:                   <code>CUDA</code> )           \u2013            <p>Target device for optimize process, default is CUDA</p> </li> <li> <code>runners</code>               (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Use only runners provided as parameter</p> </li> <li> <code>optimization_profile</code>               (<code>Optional[OptimizationProfile]</code>, default:                   <code>None</code> )           \u2013            <p>Optimization profile for conversion and profiling</p> </li> <li> <code>workspace</code>               (<code>Optional[Path]</code>, default:                   <code>None</code> )           \u2013            <p>Workspace where packages will be extracted</p> </li> <li> <code>verbose</code>               (<code>Optional[bool]</code>, default:                   <code>False</code> )           \u2013            <p>Enable verbose logging</p> </li> <li> <code>debug</code>               (<code>Optional[bool]</code>, default:                   <code>False</code> )           \u2013            <p>Enable debug logging from commands</p> </li> <li> <code>verify_func</code>               (<code>Optional[VerifyFunction]</code>, default:                   <code>None</code> )           \u2013            <p>Function for additional model verification</p> </li> <li> <code>custom_configs</code>               (<code>Optional[Sequence[CustomConfig]]</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of CustomConfigs used to control produced artifacts</p> </li> <li> <code>model_precision</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Source model precision</p> </li> </ul>"},{"location":"pipelines_optimize/optimize/api/optimize/#model_navigator.OptimizeConfig.clone","title":"clone","text":"<pre><code>clone()\n</code></pre> <p>Clone the current OptimizeConfig using deepcopy.</p> Source code in <code>model_navigator/inplace/config.py</code> <pre><code>def clone(self) -&gt; \"OptimizeConfig\":\n    \"\"\"Clone the current OptimizeConfig using deepcopy.\"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/optimize/#model_navigator.OptimizeConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert OptimizeConfig to dictionary.</p> Source code in <code>model_navigator/inplace/config.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert OptimizeConfig to dictionary.\"\"\"\n    config_dict = {}\n    for field in dataclasses.fields(self):\n        value = getattr(self, field.name)\n        config_dict[field.name] = value\n    return config_dict\n</code></pre>"},{"location":"pipelines_optimize/optimize/api/optimize/#model_navigator.load_optimized","title":"model_navigator.load_optimized","text":"<pre><code>load_optimized(device='cuda')\n</code></pre> <p>Load optimized modules.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>Union[str, device]</code>, default:                   <code>'cuda'</code> )           \u2013            <p>Device on which optimized models are loaded.</p> </li> </ul> Source code in <code>model_navigator/inplace/__init__.py</code> <pre><code>def load_optimized(device: Union[str, \"torch.device\"] = \"cuda\"):  # noqa: F821\n    # pytype: enable=name-error\n    \"\"\"Load optimized modules.\n\n    Args:\n        device: Device on which optimized models are loaded.\n    \"\"\"\n    for m in module_registry.values():\n        m.load_optimized(device=device)\n</code></pre>"},{"location":"pipelines_optimize/profile/profile/","title":"Profile model or callable","text":""},{"location":"pipelines_optimize/profile/profile/#profile-model-or-callable","title":"Profile model or callable","text":"<p>The Triton Model Navigator enhances models and pipelines and provides a uniform method for profiling any Python function, callable, or model. At present, our support is limited strictly to static batch profiling scenarios. Profiling is conducted for each sample provided in the dataloader. This allows for the use of various batch sizes in the dataloader, enabling testing of different batch size distributions. As an example, we will use a simple function that simply sleeps for 50ms:</p> <pre><code>import time\n\n\ndef custom_fn(input_):\n    # wait 50ms\n    time.sleep(0.05)\n    return input_\n</code></pre> <p>Let's provide a dataloader we will use for profiling:</p> <pre><code># Tuple of batch size and data sample\ndataloader = [(1, [\"This is example input\"])]\n</code></pre> <p>Finally, run the profiling of the function with prepared dataloader:</p> <pre><code>nav.profile(custom_fn, dataloader)\n</code></pre> <p>Review all possible options in the profile API.</p>"},{"location":"pipelines_optimize/profile/api/profile/","title":"Profile","text":""},{"location":"pipelines_optimize/profile/api/profile/#profile-api","title":"Profile API","text":""},{"location":"pipelines_optimize/profile/api/profile/#model_navigator.profile","title":"model_navigator.profile","text":"<pre><code>profile(\n    func,\n    dataloader,\n    target_formats=None,\n    runners=None,\n    window_size=DEFAULT_WINDOW_SIZE,\n    stability_percentage=DEFAULT_STABILITY_PERCENTAGE,\n    stabilization_windows=DEFAULT_STABILIZATION_WINDOWS,\n    min_trials=DEFAULT_MIN_TRIALS,\n    max_trials=DEFAULT_MAX_TRIALS,\n    throughput_cutoff_threshold=DEFAULT_THROUGHPUT_CUTOFF_THRESHOLD,\n    throughput_backoff_limit=DEFAULT_THROUGHPUT_BACKOFF_LIMIT,\n    device=\"cuda\",\n    initialize=True,\n    verbose=False,\n)\n</code></pre> <p>Profile <code>func</code> in scope of which registered modules are executed.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable</code>)           \u2013            <p>Callable to profile.</p> </li> <li> <code>dataloader</code>               (<code>Sequence[Tuple[int, Any]]</code>)           \u2013            <p>List of tuples with batch size and input.</p> </li> <li> <code>target_formats</code>               (<code>Optional[Tuple[Union[str, Format], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Target model formats for optimize process</p> </li> <li> <code>runners</code>               (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Use only runners provided as parameter</p> </li> <li> <code>min_trials</code>               (<code>int</code>, default:                   <code>DEFAULT_MIN_TRIALS</code> )           \u2013            <p>Minimum number of trials.</p> </li> <li> <code>max_trials</code>               (<code>int</code>, default:                   <code>DEFAULT_MAX_TRIALS</code> )           \u2013            <p>Maximum number of trials.</p> </li> <li> <code>stabilization_windows</code>               (<code>int</code>, default:                   <code>DEFAULT_STABILIZATION_WINDOWS</code> )           \u2013            <p>Number of stabilization windows.</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>DEFAULT_WINDOW_SIZE</code> )           \u2013            <p>Number of inference queries performed in measurement window</p> </li> <li> <code>stability_percentage</code>               (<code>float</code>, default:                   <code>DEFAULT_STABILITY_PERCENTAGE</code> )           \u2013            <p>Allowed percentage of variation from the mean in three consecutive windows.</p> </li> <li> <code>throughput_cutoff_threshold</code>               (<code>Optional[float]</code>, default:                   <code>DEFAULT_THROUGHPUT_CUTOFF_THRESHOLD</code> )           \u2013            <p>Minimum throughput increase to continue profiling. If None is provided,                          profiling run through whole dataloader</p> </li> <li> <code>throughput_backoff_limit</code>               (<code>int</code>, default:                   <code>DEFAULT_THROUGHPUT_BACKOFF_LIMIT</code> )           \u2013            <p>Back-off limit to run multiple more profiling steps to avoid stop at local minimum                       when throughput saturate based on <code>throughput_cutoff_threshold</code>.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda'</code> )           \u2013            <p>Default device used for loading unoptimized model.</p> </li> <li> <code>initialize</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to initialize pipeline on device before profiling.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Provide verbose logging</p> </li> </ul> Source code in <code>model_navigator/inplace/__init__.py</code> <pre><code>def profile(\n    func: Callable,\n    dataloader: Sequence[Tuple[int, Any]],\n    target_formats: Optional[Tuple[Union[str, Format], ...]] = None,\n    runners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\n    window_size: int = DEFAULT_WINDOW_SIZE,\n    stability_percentage: float = DEFAULT_STABILITY_PERCENTAGE,\n    stabilization_windows: int = DEFAULT_STABILIZATION_WINDOWS,\n    min_trials: int = DEFAULT_MIN_TRIALS,\n    max_trials: int = DEFAULT_MAX_TRIALS,\n    throughput_cutoff_threshold: Optional[float] = DEFAULT_THROUGHPUT_CUTOFF_THRESHOLD,\n    throughput_backoff_limit: int = DEFAULT_THROUGHPUT_BACKOFF_LIMIT,\n    device: str = \"cuda\",\n    initialize: bool = True,\n    verbose: bool = False,\n) -&gt; InplaceProfileStatus:\n    \"\"\"Profile `func` in scope of which registered modules are executed.\n\n    Args:\n        func:  Callable to profile.\n        dataloader: List of tuples with batch size and input.\n        target_formats: Target model formats for optimize process\n        runners: Use only runners provided as parameter\n        min_trials: Minimum number of trials.\n        max_trials: Maximum number of trials.\n        stabilization_windows: Number of stabilization windows.\n        window_size: Number of inference queries performed in measurement window\n        stability_percentage: Allowed percentage of variation from the mean in three consecutive windows.\n        throughput_cutoff_threshold: Minimum throughput increase to continue profiling. If None is provided,\n                                     profiling run through whole dataloader\n        throughput_backoff_limit: Back-off limit to run multiple more profiling steps to avoid stop at local minimum\n                                  when throughput saturate based on `throughput_cutoff_threshold`.\n        device: Default device used for loading unoptimized model.\n        initialize: Whether to initialize pipeline on device before profiling.\n        verbose: Provide verbose logging\n    \"\"\"\n    log_file = inplace_config.cache_dir / \"profiling.log\"\n    reconfigure_logging_to_file(log_file)\n\n    if target_formats is None:\n        target_formats = DEFAULT_TORCH_TARGET_FORMATS_FOR_PROFILING\n    if runners is None:\n        runners = list(runner_registry.values())\n\n    event_emitter = profile_event_emitter()\n    event_emitter.emit(ProfileEvent.PROFILING_STARTED)\n\n    validate_device_string(device)\n    modelkeys_runners = _get_modelkeys_runners(target_formats, runners)\n\n    default_modelkeys_runners = [(\"python\", \"eager\")]\n    optimized_modules_count = len([m.is_optimized for m in module_registry.values()])\n    if optimized_modules_count &gt; 1:\n        default_modelkeys_runners += [(\"navigator\", \"optimized\")]\n\n    modelkeys_runners = default_modelkeys_runners + list(modelkeys_runners)\n    LOGGER.info(f\"Profiling runners: {modelkeys_runners}\")\n\n    profiling_results = ProfilingResults()\n    for model_key, runner_name in modelkeys_runners:\n        runtime_name = f\"{model_key} on {runner_name}\"\n        event_emitter.emit(ProfileEvent.RUNTIME_PROFILING_STARTED, name=runtime_name)\n        try:\n            _initialize_modules(\n                func=func,\n                model_key=model_key,\n                runner_name=runner_name,\n                device=device,\n                initialize=initialize,\n                verbose=verbose,\n            )\n            try:\n                runner_profiling_results = RunnerProfilingResults(status=CommandStatus.OK)\n                for sample_id, result in _profile_runner(\n                    runner_name=runner_name,\n                    func=func,\n                    dataloader=dataloader,\n                    min_trials=min_trials,\n                    max_trials=max_trials,\n                    stabilization_windows=stabilization_windows,\n                    window_size=window_size,\n                    stability_percentage=stability_percentage,\n                    throughput_cutoff_threshold=throughput_cutoff_threshold,\n                    throughput_backoff_limit=throughput_backoff_limit,\n                ):\n                    runner_profiling_results.detailed[sample_id] = result\n\n                results_str = []\n                for result in runner_profiling_results.detailed.values():\n                    results_str.append(\n                        f\"\"\"Batch: {result.batch_size:6}, \"\"\"\n                        f\"\"\"Throughput: {result.throughput:10.2f} [infer/sec], \"\"\"\n                        f\"\"\"Avg Latency: {result.avg_latency:10.2f} [ms]\"\"\"\n                    )\n\n                results_str = \"\\n\".join(results_str)\n                LOGGER.info(f\"Collected results: \\n{results_str}\")\n                time.sleep(0.1)  # FIXME: WAR to avoid overlapping messages\n\n                for result in runner_profiling_results.detailed.values():\n                    event_emitter.emit(ProfileEvent.RUNTIME_PROFILING_RESULT, result=result)\n\n                event_emitter.emit(ProfileEvent.RUNTIME_PROFILING_FINISHED)\n            except Exception as e:\n                LOGGER.error(f\"Profiling failed for model_key {model_key} and runner {runner_name}.\")\n                LOGGER.error(str(e))\n                if verbose:\n                    LOGGER.error(f\"Traceback: {traceback.format_exc()}\")\n\n                runner_profiling_results = RunnerProfilingResults(status=CommandStatus.FAIL)\n                event_emitter.emit(ProfileEvent.RUNTIME_PROFILING_ERROR)\n        except Exception as e:\n            LOGGER.error(f\"Loading model failed for model_key {model_key} and runner {runner_name}.\")\n            LOGGER.error(str(e))\n            if verbose:\n                LOGGER.error(f\"Traceback: {traceback.format_exc()}\")\n\n            runner_profiling_results = RunnerProfilingResults(status=CommandStatus.FAIL)\n            event_emitter.emit(ProfileEvent.RUNTIME_PROFILING_ERROR)\n\n        if model_key not in profiling_results.models:\n            profiling_results.models[model_key] = RunnerResults()\n            profiling_results.models[model_key].runners[runner_name] = RunnerProfilingResults()\n        elif runner_name not in profiling_results.models[model_key].runners:\n            profiling_results.models[model_key].runners[runner_name] = RunnerProfilingResults()\n        profiling_results.models[model_key].runners[runner_name] = runner_profiling_results\n\n    event_emitter.emit(ProfileEvent.PROFILING_FINISHED)\n\n    status = _build_profile_status(profiling_results)\n\n    return status\n</code></pre>"}]}